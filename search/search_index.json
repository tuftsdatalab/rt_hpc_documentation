{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Tufts TTS Research Technology","text":"<p>Research Technology (RT) works closely with Tufts academic and research communities to define their technology needs and provide innovative IT solutions that support the University\u2019s goal of academic and research excellence. RT offers a wide array of services in support of teaching, learning and research.</p>"},{"location":"#tts-research-technology-services","title":"TTS Research Technology Services","text":"<p>Infrastructure &amp; Tools:</p> <ul> <li>Tufts Data Labs</li> <li>High-Performance Research Computing</li> <li>Research Storage</li> <li>Research Data Management</li> <li>Specialized Hardware</li> <li>Commercial and Open-source Software Application</li> </ul> <p>Consultation Services</p> <ul> <li>Provide walk-in and advanced consultations for scientific computing, bioinformatics, data management, data engineering, statistics, computational methods, Geographic Information Systems (GIS), and visualization</li> <li>Emerging Technologies / Grant Proposals</li> <li>Project Management</li> <li>University Goals and Initiatives</li> <li>Strategic Planning</li> </ul> <p>Educational Services</p> <ul> <li>Provide educational support for scientific computing, bioinformatics, data engineering, statistics, computational methods, Geographic Information Systems (GIS), and visualization</li> <li>Provide in-class sessions and workshops</li> <li>Assist with course design</li> <li>Develop &amp; maintain online tutorials &amp; instructional materials</li> <li>Coordinate outreach</li> </ul>"},{"location":"RStore/Description/","title":"What is RStore and How Should I Use it?","text":"<p>What is RStore?<p>Rstore (Research Data Storage) is a networked drive for researchers and faculty to store research-related data and computational assets. Users typically begin with 50 GB of storage and can be escalated up to several TB as needed.  </p> <p>The storage space can be accessed by all members of the lab/group following the approval of the PI(s) or other listed approvers. </p> </p>"},{"location":"RStore/Description/#how-to-create-a-new-storage","title":"How to Create a NEW storage","text":"Please do the following to submit an official RStore Drive Request:  <p> a. Go to https://it.tufts.edu/research-technology/  b. Click on \"Research Storage Request\"  c. Login with your Tufts credentials  d. Fill out blank fields  e. Select \"Research Storage Request Related to Cluster, RStore or Galaxy\"  f. Select \"Rstore Drive (R drive, Desktop Storage)\"  g. Select \"New Storage\"  h. Click [ \u2192 ]  i. Fill out remaining information  j. Click on Submit </p>"},{"location":"RStore/Description/#how-to-map-a-drive-once-it-has-been-created","title":"How to map a drive once it has been created:","text":"To map the drive(s) on a Windows computer: <p> 1. Open Computer by clicking the Start button, and then clicking Computer  2. Click the Tools menu, and then click Map Network Drive  3. In the Drive list, click a drive letter (R)  4. In the Folder box, type the path of the drive  5. Click Finish  6. If your computer is not owned by Tufts, your credentials should be:      - Username: tufts\\Tufts_Username      - Password: Tufts_Password      </p> To map the drive(s) on a Mac: <p> 1. Click on Finder then press Command+K  2. Enter the path to the network drive you want to map  3. Click Connect </p>"},{"location":"RStore/Requests/","title":"Requests","text":""},{"location":"RStore/Requests/#additional-r-store-requests","title":"Additional R Store Requests","text":"<p>For all of the following requests please go to tufts.qualtrics.com/jfe/form/SV_5bUmpFT0IXeyEfj and login with your Tufts Credentials. </p> <p></p> <p>Once at the screen above, fill out the blank fields and select \"Research Storage Request Related to Cluster, RStore or Galaxy\" and click [ \u2192 ]. </p> <p>Proceed to the next set of directions below according to your request.  </p>"},{"location":"RStore/Requests/#rstore-drive-increase","title":"Rstore Drive INCREASE","text":"To submit an official RStore Drive Increase Request. <p> a. Select \"Rstore Drive (R drive, Desktop Storage)\"   b. Select \"Increase\"   c. For FULL PATH of STORAGE input: \\rstore.tufts.edu\\RstoreDriveName   d. Click [ \u2192 ]   e. Fill out remaining information   f. Click [ \u2192 ] to Submit  </p>"},{"location":"RStore/Requests/#rstore-drive-new-increment","title":"Rstore Drive NEW INCREMENT","text":"To submit an official RStore Drive Increment Request. <p> a. Select \"Rstore Drive (R drive, Desktop Storage)\"   b. Select \"New Storage or Increment\"   c. Click [ \u2192 ]   d. Fill out remaining information letting us know you need a new storage increment   e. Click [ \u2192 ] to Submit  </p>"},{"location":"RStore/Requests/#rstore-drive-grant-access","title":"Rstore Drive Grant Access","text":"To request access for yourself or other to Rstore Drive(s): <p> a. Select \"Rstore Drive (R drive, Desktop Storage)\"   b. Select \"Request Access\"   c. For FULL PATH of STORAGE input: \\rstore.tufts.edu\\RstoreDriveName   d. Click [ \u2192 ]   e. Fill out remaining information   f. Click on [ \u2192 ] to Submit  </p>"},{"location":"about/about/","title":"About Tufts Research Technology Bioinformatics","text":"<p>Research Technology Bioinformatics provides consultations to Tufts students, faculty and researchers.  In addition we maintain bioinformatics tools on the Tufts HPC Cluster and the Tufts Galaxy Platform.  We also lead in-class sessions, partner on grants, and develop workshops.</p>"},{"location":"about/about/#contributers","title":"Contributers","text":""},{"location":"about/about/#coming-soon","title":"Coming Soon","text":""},{"location":"about/about/#acknowledgemets","title":"Acknowledgemets","text":""},{"location":"hpc_tools/all_hpc_tools/","title":"Available HPC Tools","text":""},{"location":"hpc_tools/all_hpc_tools/#available-tufts-hpc-tools","title":"Available Tufts HPC Tools","text":"<p>There are a number of modules available on the Tufts HPC. Here we link out to the NIH's documentation for these tools, which will provide descriptions and how to create a batch script to use that tool. </p> <p>Note about NIH HPC Documentation</p> <p>Note that this will not perfectly line up with how to use this tool on the HPC as we have different reservation, node, and partition names. However, it is a good start to get you going on how to use the tool in a batch script. </p> <p>For more information on how to create a batch script to work with the Tufts Cluster, check out the following tutorial: How to Create a Batch Script</p> Link To Documentation Versions ABySS https://hpc.nih.gov/apps/abyss.html ABySS/1.5.2 afni https://hpc.nih.gov/apps/afni.html afni/2014-01-28(default) amber https://hpc.nih.gov/apps/AMBER.html amber/12(default), amber/12.gpu, amber/14, amber/22-cuda, amber/22-mpi, amber/22-omp, amber/22-serial anaconda https://hpc.nih.gov/apps/python.html anaconda/2, anaconda/3, anaconda/2020.02, anaconda/2021.05, anaconda/2021.11, anaconda/bio35 ancestrymap https://hpc.nih.gov/apps/AncestryMap.html ancestrymap/6210 ant https://hpc.nih.gov/apps/ANTs.html ant/1.8.2 aspera https://hpc.nih.gov/docs/transfer.html aspera/3.5.4 ATLAS https://hpc.nih.gov/development/ATLAS.html ATLAS/3.9.78 autoconf https://hpc.nih.gov/development/autotools.html autoconf/2.69 bcftools https://hpc.nih.gov/apps/samtools.html bcftools/1.2, bcftools/1.12 bcl2fastq https://hpc.nih.gov/apps/bcl2fastq.html bcl2fastq/2.19 beagle https://hpc.nih.gov/apps/Beagle.html beagle/1.0-cuda, beagle/3.1.2 BEAST https://hpc.nih.gov/apps/BEAST.html BEAST/1.10.1(default), BEAST/1.10.4, BEAST/1.8.4 bedtools https://hpc.nih.gov/apps/bedtools.html bedtools/2.17.0, bedtools/2.19.1, bedtools/2.26.0(default) binutils https://hpc.nih.gov/apps/python.html binutils/2.29.1 blacs https://hpc.nih.gov/docs/svis.html blacs/1.1 BLAS https://hpc.nih.gov/apps/Blast.html BLAS/1.0, BLAS/3.5.0, OpenBLAS/0.2.19 blast https://hpc.nih.gov/apps/Blast.html blast/2.2.24, blast/2.2.31, blast/2.3.0, blast/2.8.1, blast-plus/2.11.0, ncbi-magicblast/1.5.0 blat https://hpc.nih.gov/apps/blat.html blat/20140708(default) bowtie https://hpc.nih.gov/apps/bowtie.html bowtie/0.12.7(default), bowtie/1.0.1, bowtie/1.1.2, bowtie/2.1.0, bowtie2/2.2.3(default) bowtie2 https://hpc.nih.gov/apps/bowtie2.html bowtie2/2.2.3(default) bwa https://hpc.nih.gov/apps/bwa.html bwa/0.7.17, bwa/0.7.9a caffe https://hpc.nih.gov/development/caffe.html caffe/20161026, caffe/20170201, caffe/20170227, caffe_unet/16.04.10.7, matcaffe/1.0 cellranger https://hpc.nih.gov/apps/cellranger.html cellranger/2.1.1, cellranger/3.0.2, cellranger-atac/1.2.0 chimera64 https://hpc.nih.gov/apps/Chimera.html chimera64/1.6.2, chimera64-osmesa/1.4.2577 chimera64-osmesa https://hpc.nih.gov/apps/Chimera.html chimera64-osmesa/1.4.2577 CirSeq https://hpc.nih.gov/apps/VIRTUS.html CirSeq/3 comsol https://hpc.nih.gov/apps/Comsol.html comsol/4.3b1, comsol/4.4, comsol/5.0, comsol/5.1, comsol/5.2(default), comsol/5.2a, comsol/5.3, comsol/5.3a, comsol/5.4, comsol/5.5, comsol/5.6, comsol/6.0 cuda https://hpc.nih.gov/docs/deep_learning.html beagle/1.0-cuda, cuda/10.0, cuda/10.2, cuda/11.0, cuda/4.2.9, cuda/5.0.35, cuda/6.5.14, cuda/7.5.18, cuda/8.0.44, cuda/9.0, amber/22-cuda, cuda/11.6, cuda/11.7 cudnn https://hpc.nih.gov/development/cuDNN.html cudnn/5.1, cudnn/7.1, cudnn/8.0.4-11.0 cufflinks https://hpc.nih.gov/apps/cufflinks.html cufflinks/0.8.3, cufflinks/2.0.0, cufflinks/2.0.2(default), cufflinks/2.1.1, cufflinks/2.2.1 Cytoscape https://hpc.nih.gov/apps/Cytoscape.html Cytoscape/2.8.3 eclipse https://hpc.nih.gov/apps/solar.html eclipse/1.4.1, eclipse/1.4.1-x86_64, eclipse/4.6.2-cpp, eclipse/4.6.2-java eigen https://hpc.nih.gov/apps/eigensoft.html eigen/3.2.2(default), eigensoft/3.0, eigensoft/4.2, eigen/3.4.0 eigensoft https://hpc.nih.gov/apps/eigensoft.html eigensoft/3.0, eigensoft/4.2 espresso https://hpc.nih.gov/apps/express.html espresso/3.1.1, espresso/3.3.1 exonerate https://hpc.nih.gov/apps/exonerate.html exonerate/2.2.0(default) fastqc https://hpc.nih.gov/apps/fastqc.html fastqc/0.11.5, fastqc/0.11.8, fastqc/0.11.9 fastx https://hpc.nih.gov/apps/fastxtoolkit.html fastx/0.0.13 ffmpeg https://hpc.nih.gov/apps/python.html ffmpeg/3.2.2 fftw https://hpc.nih.gov/apps/modules.html fftw/3.3.2(default), fftw/3.3.2-double, fftw/3.3.3, fftw/3.3.3-double, fftw/3.3.10 freebayes https://hpc.nih.gov/apps/freebayes.html freebayes/1.1.0 freesurfer https://hpc.nih.gov/apps/freesurfer.html freesurfer/5.3.0(default) fsl https://hpc.nih.gov/apps/fsl.html fsl/5.0.6(default), fsl/5.0.9, fsl/6.0.5 gamess https://hpc.nih.gov/apps/GAMESS.html gamess/2013-03-01 GATK https://hpc.nih.gov/apps/GATK.html GATK/3.1-1(default), GATK/3.7, GATK/4.2.6.1 gaussian https://hpc.nih.gov/apps/Gaussian.html gaussian/g09, gaussian/g16avx, gaussian/g16avx2, gaussian/g16sse4_2 gcc https://hpc.nih.gov/development/compilers.html gcc/4.7.0, gcc/4.9.2, gcc/5.3.0, gcc/7.3.0, gcc/9.3.0, gcc/8.4.0, netcdf/4.0.0-gcc, intel-oneapi-mkl/2021.1.1-gcc-9.3.0, lhapdf5/5.9.1-gcc-4.8.5, intel-oneapi-tbb/2021.1.1-gcc-9.3.0, log4cpp/1.1.3-gcc-4.8.5, perl/5.34.0_gcc7.3.0, pythia6/6.4.28-gcc-4.8.5 gcta https://hpc.nih.gov/apps/GCTA.html gcta/1.25.2 glibc https://hpc.nih.gov/apps/csd.html glibc/2.14 glog https://hpc.nih.gov/apps/python.html glog/0.3.4, glog/0.4.0 gnu-parallel https://hpc.nih.gov/apps/parallel.html gnu-parallel/20150122 alphafold https://hpc.nih.gov/apps/alphafold2.html alphafold/2.1.1, alphafold/2.2.0 annovar https://hpc.nih.gov/apps/ANNOVAR.html annovar/202106 aria2 https://hpc.nih.gov/apps/aria2.html aria2/1.35.0 automake https://hpc.nih.gov/development/autotools.html automake/1.16.3 biobakery https://hpc.nih.gov/apps/biobakery_workflows.html biobakery/workflows blast-plus https://hpc.nih.gov/apps/Blast.html blast-plus/2.11.0 blender https://hpc.nih.gov/apps/blender.html blender/3.4.1 cactus https://hpc.nih.gov/apps/cactus.html cactus/gpu cellprofiler https://hpc.nih.gov/apps/cellprofiler.html cellprofiler/3.1.9, cellprofiler/4.0.6, cellprofiler/4.1.3, cellprofiler/4.2.1 cellranger-atac https://hpc.nih.gov/apps/cellranger-atac.html cellranger-atac/1.2.0 chimera https://hpc.nih.gov/apps/Chimera.html chimera64/1.6.2, chimera64-osmesa/1.4.2577, chimera/1.15 condaenv https://hpc.nih.gov/apps/python.html condaenv/bracken ctffind https://hpc.nih.gov/apps/ctffind.html ctffind/4.1.8 dssp https://hpc.nih.gov/apps/DSSP.html dssp/3.1.4 emacs https://hpc.nih.gov/apps/Editors.html emacs/26.3, emacs/27.1_ccgpu, emacs/X26.3 eman2 https://hpc.nih.gov/apps/EMAN2.html eman2/2.91 fastjar https://hpc.nih.gov/apps/R.html fastjar/0.98 fasttree https://hpc.nih.gov/apps/FastTree.html fasttree/2.1.10 Fiji https://hpc.nih.gov/apps/Fiji.html Fiji/1.53t, Fiji/ImageJ flash https://hpc.nih.gov/systems/hardware.html flash/1.2.11 fontconfig https://hpc.nih.gov/apps/python.html fontconfig/2.13.94 freetype https://hpc.nih.gov/apps/python.html freetype/2.11.0 hail https://hpc.nih.gov/apps/Hail.html hail/202106 gstreamer https://hpc.nih.gov/apps/python.html gstreamer/0.10 imagej https://hpc.nih.gov/apps/Fiji.html imagej/1.52a, imagej/1.5.2 hdf5 https://hpc.nih.gov/apps/ont-fast5-api.html hdf5/1.10.4, hdf5/1.12.1 hmmer https://hpc.nih.gov/apps/hmmer.html hmmer/3.3, hmmer/3.1b2 htslib https://hpc.nih.gov/apps/samtools.html htslib/1.9, htslib/1.2.1 libuuid https://hpc.nih.gov/apps/python.html libuuid/1.0.3 gem https://hpc.nih.gov/apps/gem.html gem/1.4.3, imagemagick/7.1.0 harfbuzz https://hpc.nih.gov/apps/python.html harfbuzz/2.9.1, harfbuzz/5.1.0 intel https://hpc.nih.gov/systems/hardware.html intel/mpi_2021.5.0, intel/compilers_2022.0.1, intel-oneapi-mkl/2021.1.1-gcc-9.3.0, intel-oneapi-tbb/2021.1.1-gcc-9.3.0, intel/2013_sp1, intel/tbb433 lftp https://hpc.nih.gov/docs/transfer.html lftp/4.8.1 fribidi https://hpc.nih.gov/apps/python.html fribidi/1.0.5 lastz https://hpc.nih.gov/apps/LASTZ.html lastz/1.03 netcdf https://hpc.nih.gov/apps/python.html netcdf/4.0.0-gcc, netcdf/4.0.0-pgi, netcdf/4.0.0, netcdf/c-4.8.1, netcdf/fortran-4.5.2, netcdf/fortran-4.5.3, netcdf/4.7.4 java https://hpc.nih.gov/development/java.html eclipse/4.6.2-java, java/11.0.2, java/15.0.2, java/1.5.0_64bit, java/1.6.0_64bit, java/1.5.0, java/1.4.2, java/1.6.0, java/1.6.0_25, java/1.6.0_25_64bit, java/1.8.0_60, java/1.7.0_51, structure-java/2.3.4 lapack https://hpc.nih.gov/development/LAPACK.html lapack/3.9.0, lapack/3.4.0, lapack/3.2.1, lapack/3.5.0, scalapack/1.8.0 gromacs https://hpc.nih.gov/apps/Gromacs.html gromacs/4.6.5, gromacs/4.6.5d, gromacs/5.0.1, gromacs/4.5.5(default), gromacs/4.6.1, gromacs/4.6.5-gls, gromacs/4.6.1-slurm, gromacs/4.6.7, gromacs/4.6.7-gpu, gromacs/5.1.0, gromacs/5.1.4-gpu, gromacs-plumed-libmatheval/2014-06-17(default), gromacs/4.6.5+plumed, gromacs/5.1.4 matlab https://hpc.nih.gov/apps/Matlab.html matlab/2020a, matlab/2019a, matlab/2019b, matlab/2020b, matlab/2021a, matlab/2022a, matlab/2016a, matlab/2016b, matlab/2017b, matlab/2018a, matlab/2017a(default) mpfr https://hpc.nih.gov/apps/python.html mpfr/2.4.2(default), mpfr/3.1.2 julia https://hpc.nih.gov/apps/julia.html julia/1.6.0, julia/1.5.3, julia/0.3 gcam https://hpc.nih.gov/apps/gautomatch.html gcam/5.3, gcam/5.4 giflib https://hpc.nih.gov/apps/python.html giflib/5.1.4 gobject-introspection https://hpc.nih.gov/apps/python.html gobject-introspection/1.56.1 interproscan https://hpc.nih.gov/apps/interproscan.html interproscan/5.20 glib https://hpc.nih.gov/apps/python.html glibc/2.14, glib/2.56.1, glib/2.66.7 lammps https://hpc.nih.gov/apps/lammps.html lammps/20210310, lammps/27Aug13(default), lammps/10Aug15, lammps/16Mar18 libgmp https://hpc.nih.gov/apps/R.html libgmp/4.1.4 libicu https://hpc.nih.gov/apps/python.html libicu/4.2.1 imod https://hpc.nih.gov/apps/IMOD.html imod/4.11.18, imod/4.9.9, imod/4.9.12 ld_lib https://hpc.nih.gov/apps/modules.html ld_lib/2.2.21 libiconv https://hpc.nih.gov/apps/python.html libiconv/1.16 libtiff https://hpc.nih.gov/apps/python.html libtiff/4.2.0 gurobi https://hpc.nih.gov/apps/gurobi.html gurobi/9.5.0, gurobi/6.5.1, gurobi/7.5.1, gurobi/8.1.0 metal https://hpc.nih.gov/apps/metal.html metal/2010-02-08, metal/2011-03-25(default), metal/2010-08-01, metal/2009-10-10 mirdeep2 https://hpc.nih.gov/apps/mirdeep.html mirdeep2/2.0.0.5(default) openmpi https://hpc.nih.gov/development/MPI.html openmpi/1.8.6, openmpi/1.8.2(default), openmpi/2.1.2, openmpi/4.0.4, openmpi/4.0.5, openmpi/1.10.1, openmpi/1.10.2, openmpi/1.6.3-gnu, openmpi/4.1.2, openmpi/3.1.2, openmpi/4.1.1, openmpi/3.1.6, openmpi/3.1.6_slurm, openmpi/4.1.2_slurm, openmpi/4.1.4 hisat https://hpc.nih.gov/apps/hisat.html hisat/2.0.5, hisat/2.1.0 namd https://hpc.nih.gov/apps/NAMD.html namd/2014-07-10, namd/2.12-mpi, namd/2.8(default) openmm https://hpc.nih.gov/apps/charmm/older_charmm.html openmm/4.1.1(default), openmm/5.1 PEET https://hpc.nih.gov/apps/PEET.html PEET/1_13.0, PEET/1_12.0 NCAR https://hpc.nih.gov/docs/deep_learning.html NCAR/5.2.1 opencv https://hpc.nih.gov/apps/python.html opencv/3.1.0, opencv/4.5.1 IGV https://hpc.nih.gov/apps/IGV.html IGV/1.5.30 MCR https://hpc.nih.gov/apps/dynamo.html MCR/R2012b, MCR/R2012a MDAnalysis https://hpc.nih.gov/apps/mdtraj.html MDAnalysis/0.8.1 multinest https://hpc.nih.gov/policies/multinode.html multinest/201911 icc https://hpc.nih.gov/apps/R.html icc/10.1.017 jupyter https://hpc.nih.gov/apps/jupyter.html jupyter/3.0, jupyter/1.0(default) kallisto https://hpc.nih.gov/apps/kallisto.html kallisto/0.45.0 mothur https://hpc.nih.gov/apps/mothur.html mothur/1.25.1, mothur/1.29.1, mothur/1.36.1(default), mothur/1.48, mothur/1.44, mothur/1.47 graphviz https://hpc.nih.gov/apps/modules.html graphviz/2.40.1 HTSeq https://hpc.nih.gov/apps/htseq.html HTSeq/0.5.4p5(default), HTSeq/0.6.1p1 misopy https://hpc.nih.gov/apps/misopy.html misopy/0.5.2(default) mrbayes https://hpc.nih.gov/apps/mrbayes.html mrbayes/3.1.2 nccl https://hpc.nih.gov/docs/deeplearning/multinode_DL.html nccl/2.7.8-1 impute https://hpc.nih.gov/apps/IMPUTE.html impute/2.0.3 mach https://hpc.nih.gov/apps/mash.html mach/1.0.16 MACS https://hpc.nih.gov/apps/macs.html MACS/1.4.2-1 merlin https://hpc.nih.gov/apps/merlin.html merlin/1.1.2 mvapich2 https://hpc.nih.gov/development/MPI.html mvapich2/1.6-1 ont-guppy https://hpc.nih.gov/apps/guppy.html ont-guppy/3.3.0-gpu, ont-guppy/3.3.0-cpu, ont-guppy/4.5.2-cpu, ont-guppy/4.5.2-gpu, ont-guppy/5.0.11-gpu, ont-guppy/5.0.11-cpu gpaw-mpi https://hpc.nih.gov/development/MPI.html gpaw-mpi/0.4.2627 idl https://hpc.nih.gov/apps/idl.html idl/8.4 libffi https://hpc.nih.gov/apps/python.html libffi/3.2.1 maq https://hpc.nih.gov/docs/helixdrive.html maq/0.7.1 mathematica https://hpc.nih.gov/apps/mathematica.html mathematica/13.1, mathematica/13.2.1, mathematica/10.4.0, mathematica/10.4.1, mathematica/11.0.0, mathematica/11.3.0, mathematica/12.0, mathematica/13.0, mathematica/12.1(default), mathematica/12.2, mathematica/12.3 muscle https://hpc.nih.gov/apps/muscle.html muscle/3.8.31 gromacs-plumed-libmatheval https://hpc.nih.gov/apps/Gromacs.html gromacs-plumed-libmatheval/2014-06-17(default) MAnorm https://hpc.nih.gov/apps/manorm.html MAnorm/2014-04-03 multiqc https://hpc.nih.gov/apps/multiqc.html multiqc/1.7.0 paraview https://hpc.nih.gov/apps/paraview.html paraview/3.8.1, paraview/4.4 povray https://hpc.nih.gov/apps/POVRay/index.html povray/3.6.1 haploview https://hpc.nih.gov/apps/plink.html haploview/4.1 pcre https://hpc.nih.gov/apps/python.html pcre/8.38 mafft https://hpc.nih.gov/apps/mafft.html mafft/7.481 metaphlan https://hpc.nih.gov/apps/metaphlan.html metaphlan/2.6.0 nextflow https://hpc.nih.gov/apps/nextflow.html nextflow/21.10.1, nextflow/21.10.6 ORFfinder https://hpc.nih.gov/apps/ORFfinder.html ORFfinder/0.4.3 orthofinder https://hpc.nih.gov/apps/orthofinder.html orthofinder/2.5.4 libxml2 https://hpc.nih.gov/apps/python.html libxml2/2.9.10, libxml2/2.9.12 libxpm https://hpc.nih.gov/apps/python.html libxpm/3.5.12 mamba https://hpc.nih.gov/apps/python.html mamba/22.9.0-2 minimap2 https://hpc.nih.gov/apps/minimap2.html minimap2/2.15 orca https://hpc.nih.gov/apps/python.html orca/5.0.3, orca/5.0.4 structure-java https://hpc.nih.gov/development/java.html structure-java/2.3.4 salmon https://hpc.nih.gov/apps/salmon.html salmon/1.6.0, salmon/0.13.1 openssl https://hpc.nih.gov/apps/python.html openssl/1.1.1 ninja https://hpc.nih.gov/apps/python.html ninja/1.10.1 tcltk https://hpc.nih.gov/apps/R.html tcltk/08.5.11-rhel6, tcltk/8.5.11 rstudio https://hpc.nih.gov/apps/RStudio.html rstudio/0.96.331, rstudio/1.1.447, rstudio/1.1.383, rstudio/0.99.903 tophat https://hpc.nih.gov/apps/tophat.html tophat/1.0.14, tophat/2.0.10, tophat/2.0.9(default), tophat/2.0.13 picard https://hpc.nih.gov/apps/picard.html picard/1.139, picard/2.8.0 protobuf https://hpc.nih.gov/apps/python.html protobuf/3.18, protobuf/2.6 R https://hpc.nih.gov/apps/R.html abaqus/V6R2017, abaqus/V6R2018, NCAR/5.2.1, MCR/R2012b, mirPRo/1.1.4, MCR/R2012a, ORFfinder/0.4.3, sf/R_4.0.0, R/3.2.2, R/3.4.1, R/4.0.0, R/3.2.5, PyRosetta/r55194, R/3.6.3, RepeatMasker/4.0.7, STAR/2.7.0a, R/3.3.3, R/3.4, R/4.2.2, tecplot/2013R1, R/3.4_dulla, R/3.0.3, R/3.5.0, RetroSeq/20130716(default), STAR/2.6.1d, R/3.0.2, R/3.1.0, ViennaRNA/2.1.6(default), PyRosetta/r55193, R/3.0.1, R/3.3.2, R/3.4.3(default), R/4.1.1, STAR/2.3.0e, R/2.15.3, STAR/2.5.2b(default), RAxML/8.2.12, R/4.2.0 rosetta https://hpc.nih.gov/apps/rosetta.html rosetta/3.4, rosetta/3.13_mpi, rosetta/3.7, rosetta/3.13 usearch https://hpc.nih.gov/apps/usearch.html usearch/8.1.1861, usearch/10.0.240, usearch/7.0.1001(default) ncbi-magicblast https://hpc.nih.gov/apps/magicblast.html ncbi-magicblast/1.5.0 QIIME https://hpc.nih.gov/apps/QIIME.html QIIME/1.9.0(default), QIIME/1.7.0, QIIME/1.8.0, QIIME/1.5.0, QIIME/1.6.0 samtools https://hpc.nih.gov/apps/samtools.html samtools/0.1.18(default), samtools/0.1.19, samtools/1.9, samtools/1.2 plink https://hpc.nih.gov/apps/plink.html plink/1.90b5, plink2/2.0, plink/1.06, plinkseq/0.10 sra https://hpc.nih.gov/apps/sratoolkit.html sra/2.5.0, sra/2.10.8, sra/2.9.2 root https://hpc.nih.gov/apps/singularity.html root/6.22.08_pythia6, root/6.24.06, root/5.34.18, proot/5.1.0, root/6.22.08 python https://hpc.nih.gov/apps/python.html boost/1.63.0-python3, python/2.7.4, python/3.8.8, python/3.6.0, tensorflow/11-python3.5, python/2.7.6, python/3.5.0, tensorflow/11-python2.7, python/2.6.5, python/2.7.3(default) rsem https://hpc.nih.gov/apps/rsem.html rsem/1.3, rsem/1.3.3, rsem/1.2.14(default), rsem/1.3.1 singularity https://hpc.nih.gov/apps/singularity.html singularity/3.6.1, singularity/2.6.1(default), singularity/3.1.0, singularity/3.5.3, singularity/3.8.4 subread https://hpc.nih.gov/apps/subread.html subread/1.6.3, subread/1.5.1 vmd https://hpc.nih.gov/apps/VMD.html vmd/1.9.4, vmd/1.9 pytorchgpu https://hpc.nih.gov/docs/deep_learning.html pytorchgpu/1 perl https://hpc.nih.gov/apps/modules.html bioperl/1.7.6, perl/5.34.0, perl/5.34.0_gcc7.3.0, superlu/4.0, perl/5.32.1 PyRosetta https://hpc.nih.gov/apps/PyRosetta.html PyRosetta/r55194, PyRosetta/r55193 trinity https://hpc.nih.gov/apps/trinity.html trinity/10.15.15, trinity/7.17.14 plink2 https://hpc.nih.gov/apps/plink.html plink2/2.0 RepeatMasker https://hpc.nih.gov/apps/repeatmasker.html RepeatMasker/4.0.7 STAR https://hpc.nih.gov/apps/STAR.html STAR/2.7.0a, STAR/2.6.1d, STAR/2.3.0e, STAR/2.5.2b(default) rclone https://hpc.nih.gov/apps/rclone.html rclone/1.51.0, rclone/1.59.0 rsync https://hpc.nih.gov/docs/transfer.html rsync/3.2.2 pythia https://hpc.nih.gov/apps/python.html root/6.22.08_pythia6, pythia/8235, pythia/8219, pythia6/6.4.28-gcc-4.8.5, pythia6/6.4.28 trinotate https://hpc.nih.gov/apps/trinotate.html trinotate/2.0.2 stacks https://hpc.nih.gov/apps/R.html stacks/2.53, stacks/1.12(default) plinkseq https://hpc.nih.gov/apps/plinkseq.html plinkseq/0.10 spades https://hpc.nih.gov/apps/spades.html spades/3.12.0, spades/3.15.4 zlib https://hpc.nih.gov/apps/R.html zlib/1.2.8, zlib/1.2.11 phred https://hpc.nih.gov/apps/modphred.html phred/0.0 rnaseqmut https://hpc.nih.gov/apps/VIRTUS.html rnaseqmut/1.0 pandoc https://hpc.nih.gov/apps/python.html pandoc/2.14 valgrind https://hpc.nih.gov/development/debugging.html valgrind/3.16.1 pilon https://hpc.nih.gov/apps/pilon.html pilon/1.22 qiime2 https://hpc.nih.gov/apps/QIIME.html qiime2/2017.7, qiime2/2021.11, qiime2/2018.8 SAS https://hpc.nih.gov/apps/SAS.html SAS/9.3, SAS/9.4(default) solar https://hpc.nih.gov/apps/solar.html solar/4.2.7 squid https://hpc.nih.gov/docs/transfer.html squid/1.9g(default) relion https://hpc.nih.gov/apps/RELION/index.html relion/4.0 tabix https://hpc.nih.gov/apps/samtools.html tabix/20131216 proj4 https://hpc.nih.gov/apps/R.html proj4/4.4 sniffles https://hpc.nih.gov/apps/sniffles.html sniffles/1.0.10 SuiteSparse https://hpc.nih.gov/apps/python.html SuiteSparse/3.6.1 tensorflow https://hpc.nih.gov/docs/deep_learning.html tensorflow/11-python3.5, tensorflow/11-python2.7 vcftools https://hpc.nih.gov/apps/vcftools.html vcftools/0.1.13(default), vcftools/0.1.12b phenix https://hpc.nih.gov/apps/Phenix.html phenix/1.13 Qt https://hpc.nih.gov/apps/RStudio.html Qt/4.8.4 RetroSeq https://hpc.nih.gov/apps/VIRTUS.html RetroSeq/20130716(default) tinker https://hpc.nih.gov/apps/mdtraj.html tinker/7.1 trimmomatic https://hpc.nih.gov/apps/trimmomatic.html trimmomatic/0.38 tmhmm https://hpc.nih.gov/apps/trinotate.html tmhmm/2.0c vim https://hpc.nih.gov/apps/Editors.html vim/8.1 randfold https://hpc.nih.gov/apps/randfold.html randfold/2.0(default) spark https://hpc.nih.gov/apps/spark.html spark/2.0.1, spark/2.3.0(default) ViennaRNA https://hpc.nih.gov/apps/viennarna.html ViennaRNA/2.1.6(default) readline https://hpc.nih.gov/apps/python.html readline/8.1 proot https://hpc.nih.gov/docs/biowulf_tools.html proot/5.1.0 Schrodinger https://hpc.nih.gov/apps/schrodinger/index.html Schrodinger/2015-4, Schrodinger/2012 sqlite3 https://hpc.nih.gov/apps/sqlite.html sqlite3/3.33.0 trim-galore https://hpc.nih.gov/apps/trimgalore.html trim-galore/0.6.4_dev zmq https://hpc.nih.gov/apps/python.html zmq/4.8.1 RAxML https://hpc.nih.gov/apps/raxml.html RAxML/8.2.12 transdecoder https://hpc.nih.gov/apps/TransDecoder.html transdecoder/2.0"},{"location":"hpc_tools/api/","title":"API (Application Programming Interface)","text":""},{"location":"hpc_tools/api/#api-queries","title":"API Queries","text":"<p>An API, or Application Programming Interface, is a way of accessing data directly from a website. In this way we can pull data from a website without having to deal with parsing HTML content. An API request occurs between a client and a server:</p> <p> </p> Image by DATAQUEST <p>Essentiall, we (the client) reach out to the server and request data. In return we get the data and a response code telling us how the request went. Sometimes we don't get the data and the response code can give us a hint as to why:</p> <p> </p> Image by WhiteHat <p>Each website (with an available API) should have more specific documentation on these codes and how to structure your request. Here we are going to cover how to use the STRINGDB API using Python and R. </p>"},{"location":"hpc_tools/api/#api-request","title":"API Request","text":"RPython <ul> <li>We will first need to load the R packages necessary to handle API requests:</li> </ul> <pre><code>library(httr)\nlibrary(jsonlite)\n</code></pre> <ul> <li>Now we will need to take a look at the API documentation on the STRINGDB website. Typically, we have a base url that we pull from, and in this case is is:</li> </ul> <p>https://string-db.org/</p> <ul> <li>We then need to plug in the information we would like to pull by adding to the url. So to get an image of the  information we will add the following to the url:</li> </ul> <pre><code>api/json/interaction_partners?\n</code></pre> <ul> <li>To point to specific genes, say PTCH1, we will add the following:</li> </ul> <pre><code>identifiers=PTCH1\n</code></pre> <ul> <li>Now the full url will be:</li> </ul> <pre><code>https://string-db.org/api/json/interaction_partners?identifiers=PTCH1\n</code></pre> <ul> <li>We can now plug this url into the <code>GET</code> function!</li> </ul> <pre><code>res &lt;- GET(\"https://string-db.org/api/json/interaction_partners?identifiers=PTCH1\")\nres\n</code></pre> <p>output</p> <pre><code>Response [https://string-db.org/api/json/interaction_partners?identifiers=PTCH1]\n  Date: 2023-01-10 19:37\n  Status: 200\n  Content-Type: text/json; charset=utf-8\n  Size: 2.72 kB\n</code></pre> <ul> <li>Here we see that our request did come through. However, the data is in json format. We can convert this json data to tabular data with:</li> </ul> <pre><code>data = fromJSON(rawToChar(res$content))\nhead(data)\n</code></pre> <p>output</p> <pre><code>           stringId_A           stringId_B preferredName_A preferredName_B\n1 9606.ENSP00000332353 9606.ENSP00000295731           PTCH1             IHH\n2 9606.ENSP00000332353 9606.ENSP00000297261           PTCH1             SHH\n3 9606.ENSP00000332353 9606.ENSP00000266991           PTCH1             DHH\n4 9606.ENSP00000332353 9606.ENSP00000256442           PTCH1           CCNB1\n5 9606.ENSP00000332353 9606.ENSP00000249373           PTCH1             SMO\n6 9606.ENSP00000332353 9606.ENSP00000376458           PTCH1            CDON\n</code></pre> <ul> <li>Congratulations! You have pulled data using an API!</li> </ul>"},{"location":"hpc_tools/api/#references","title":"References","text":"<ol> <li>https://www.dataquest.io/blog/r-api-tutorial/</li> <li>https://www.dataquest.io/blog/python-api-tutorial/</li> <li>https://apidocs.whitehatsec.com/whs/docs/error-handling</li> </ol>"},{"location":"hpc_tools/containers/","title":"Containers","text":""},{"location":"hpc_tools/containers/#containers","title":"Containers","text":"<p>Containers are a way of sharing software across different systems. On the Tufts HPC Cluster we can use the Singularity module (now called Apptainer) to build containers on the cluster. </p>"},{"location":"hpc_tools/containers/#building-an-outside-container-on-the-cluster","title":"Building An Outside Container On The Cluster","text":"<ul> <li>To begin you will need to load the following modules:</li> </ul> <pre><code>module load singularity/3.6.1\nmodule load squashfs\n</code></pre> <ul> <li>Now, search docker hub for the tool of your choice. You will be using this image to build a singularity container or sif file. In this case we will be demonstrating how to download the biobakery workflows docker image:</li> </ul> <pre><code>singularity build bioBakery.sif docker://biobakery/workflows\n</code></pre>"},{"location":"hpc_tools/containers/#using-the-container","title":"Using The Container","text":"<ul> <li>To use this tool you will need to reference the sif file you created. So to run the humann3 command you would use the following:</li> </ul> <pre><code>singularity exec bioBakery.sif humann3 --help\n</code></pre>"},{"location":"hpc_tools/containers/#references","title":"References","text":"<ol> <li>CHPC - Research Computing and Data Support for the University - Singularity</li> </ol>"},{"location":"hpc_tools/dbgap/","title":"dbGAP Downloads","text":""},{"location":"hpc_tools/dbgap/#downloading-fastq-data-using-dbgap","title":"Downloading Fastq Data Using dbGAP","text":"<p>dbGAP is a repository of data assessing the connection between genotypes and phenotypes. Here we discuss how to access this data using the Tufts HPC.</p> <ol> <li>Obtain your dbGaP repository key by logging into dgGAP and clicking \"get dbGAP repository key\"</li> </ol> <p>dbGAP Download Guide</p> <p></p> <ol> <li>Now, navigate to the dbGAP SRA RUN Selector, login with your credentials, select the files you'd like to download, and click Accession List:</li> </ol> <p></p> <ol> <li> <p>Upload this ngc file and the accession list to the desired directory on the Tufts HPC cluster. For more information on how to login to the cluster visit: Navigate To The Cluster</p> </li> <li> <p>Now you will need to load the tools needed to download your data:</p> </li> </ol> <pre><code>module load sra/2.10.8\n</code></pre> <ol> <li>Now you will need to configure the sratoolkit:</li> </ol> <p><pre><code>vdb-config --interactive\n</code></pre> 5. Hit \"X\"</p> <ol> <li>Now set up the following batch script:</li> </ol> Using ParallelNot Using Parallel <p>dbGAP_download.sh</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=dbGap\n#SBATCH --time=07-00:00:00\n#SBATCH --partition=largemem\n#SBATCH --nodes=1\n#SBATCH -c 8\n#SBATCH --mem=110Gb\n#SBATCH --output=%j.out\n#SBATCH --error=%j.err\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=Your.Email@tufts.edu\n\nmodule load sra/2.10.8 parallel\n\n# using parallel\nparallel --jobs 4 \"fastq-dump -X 9999999999999 --ngc /path/to/projectNgcFile.ngc --split-files --gzip {}\" &lt; /path/to/accessionList.txt\n</code></pre> <p>dbGAP_download.sh</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=dbGap\n#SBATCH --time=07-00:00:00\n#SBATCH --partition=largemem\n#SBATCH --nodes=1\n#SBATCH -c 8\n#SBATCH --mem=110Gb\n#SBATCH --output=%j.out\n#SBATCH --error=%j.err\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=Your.Email@tufts.edu\n\nmodule load sra/2.10.8 # not using parallel\nfastq-dump -X 9999999999999 --ngc /path/to/projectNgcFile.ngc --gzip $(&lt;/path/to/accessionList.txt)\n</code></pre> <ol> <li>To run your script, enter the following:</li> </ol> <pre><code>sbatch dbGAP_download.sh\n</code></pre> <ol> <li>To check on the status of your job, enter the following:</li> </ol> <pre><code>squeue -u $USER\n</code></pre> <ol> <li>dbGAP repositories can contain a lot of data, so if you need your job extended reach out to tts-research@tufts.edu</li> </ol>"},{"location":"hpc_tools/dbgap/#downloading-other-dbgap-data","title":"Downloading Other dbGAP Data","text":"<ol> <li>Obtain your dbGaP repository key by logging into dgGAP and clicking \"get dbGAP repository key\"</li> </ol> <p>dbGAP Download Guide</p> <p></p> <ol> <li>Now, navigate to the dbGAP SRA RUN Selector, login with your credentials, select the files you'd like to download, and click Cart File:</li> </ol> <p></p> <ol> <li> <p>Upload this ngc file and the accession list to the desired directory on the Tufts HPC cluster. For more information on how to login to the cluster visit: Navigate To The Cluster</p> </li> <li> <p>Now you will need to load the tools needed to download your data:</p> </li> </ol> <pre><code>module load sra/2.10.8\n</code></pre> <ol> <li>Now you will need to configure the sratoolkit:</li> </ol> <p><pre><code>vdb-config --interactive\n</code></pre> 5. Hit \"X\"</p> <ol> <li>Now set up the following batch script:</li> </ol> <p>dbGAP_download.sh<pre><code>#!/bin/bash\n#SBATCH --job-name=dbGap\n#SBATCH --time=07-00:00:00\n#SBATCH --partition=largemem\n#SBATCH --nodes=1\n#SBATCH -c 8\n#SBATCH --mem=110Gb\n#SBATCH --output=%j.out\n#SBATCH --error=%j.err\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=Your.Email@tufts.edu\n\nmodule load sra/2.10.8\nprefetch -X 9999999999999 --ngc your_file.ngc cart_prj#####_###.krt\nvdb-decrypt --ngc your_file.ngc enc_file.xml\n</code></pre> </p> <p>Note</p> <p>Note that we add in the option <code>-X 9999999999999</code>. This allows for files larger than 20GB, and without this option larger files will not download.</p> <ol> <li>To run your script, enter the following:</li> </ol> <pre><code>sbatch dbGAP_download.sh\n</code></pre> <ol> <li>To check on the status of your job, enter the following:</li> </ol> <pre><code>squeue -u $USER\n</code></pre> <ol> <li>dbGAP repositories can contain a lot of data, so if you need your job extended reach out to tts-research@tufts.edu</li> </ol>"},{"location":"hpc_tools/dbgap/#references","title":"References","text":"<ol> <li>dbGAP Download Guide</li> </ol>"},{"location":"hpc_tools/github/","title":"GitHub","text":""},{"location":"hpc_tools/github/#introduction-to-github","title":"Introduction to GitHub","text":"<p>The power in GitHub lies in version control. Code is often changed and in doing so previous versions of files can easily be lost. GitHub saves changes to files so that one can go back and restore previous versions if needed. Additionally, there is a plethora of functionality to: update code collaboratively, publish static website pages, report issues with code, etc..</p>"},{"location":"hpc_tools/github/#creating-a-git-repository","title":"Creating a Git Repository","text":"<ul> <li>To create a Git Repository by:</li> </ul> <pre><code># change into your project directory\ncd /path/to/your/project\n\n# initialize the repository\ngit init\n</code></pre>"},{"location":"hpc_tools/github/#addcommit-files","title":"Add/Commit Files","text":"<ul> <li>To add files to be tracked:</li> </ul> <pre><code># add files to be tracked\ngit add main.py input.txt \n</code></pre> <ul> <li>To commit these files to your Git Repository:</li> </ul> <pre><code># commit the files to the repository, creating the first snapshot\ngit commit -m \"Initial Commit\"\n</code></pre>"},{"location":"hpc_tools/github/#configure-your-credentials","title":"Configure Your Credentials","text":"<ul> <li>To configure your credentials:</li> </ul> <pre><code># configure your user name/email\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"[email address]\"\n</code></pre>"},{"location":"hpc_tools/github/#push-to-github","title":"Push To GitHub","text":"<ul> <li>Currently, these files are not on github. To add them to Github:</li> </ul> <pre><code>## push files to github\ngit remote add origin git@github.com:user_name/my_new_repo.git\ngit push -u origin master\n</code></pre>"},{"location":"hpc_tools/github/#going-further","title":"Going Further","text":"<ul> <li>GitHub has much more functionality than what has been described here. To learn more, check out:</li> </ul> <p>GitHub Docs</p>"},{"location":"hpc_tools/github/#references","title":"References","text":"<ol> <li>GitHub Docs</li> </ol>"},{"location":"hpc_tools/nextflow/","title":"Nextflow","text":""},{"location":"hpc_tools/nextflow/#nextflow","title":"Nextflow","text":"<p>Nextflow is a type of workflow manager, designed to be portable and reproducible. Nextflow can be used on local, HPC schedulers, AWS Batch, Google Cloud Life Sciences, and Kubernetes. Nextflow can access tool dependencies through Conda, Spack, Docker, Podman, Singularity, Modules and more.</p>"},{"location":"hpc_tools/nextflow/#how-do-i-access-nextflow-on-the-cluster","title":"How Do I Access Nextflow on the Cluster?","text":"<p>Nextflow can be accessed on the Tufts HPC Cluster with the following modules:</p> <pre><code>module load gcc nextflow/21.10.1 java/15.0.2 anaconda/2021.11\n</code></pre> <p>You might also have noticed that we include <code>anaconda/2021.11</code> as well. This is because many nextflow pipelines you might import will use Anaconda for pipeline tool dependencies. </p>"},{"location":"hpc_tools/nextflow/#running-nf-core-pipelines","title":"Running nf-core Pipelines","text":"<p>Speaking of importing Nextflow pipelines, it is worth mentioning nf-core pipelines. These are community developed pipelines that are available for your use! There are a number of pipelines you can use:</p> <p>Nextflow nf-core Pipelines</p> <p>Danger</p> <p>When Nextflow is running tasks it will create many temp files within: the <code>./work</code> directory. Be very careful when running these pipelines and always be sure to clear out files in this folder to avoid running out of file storage. </p> <p>We will demonstrate how to run the RNA-seq pipeline:</p> <pre><code>nextflow run nf-core/rnaseq \\\n--input samplesheet.csv \\\n--outdir &lt;OUTDIR&gt; \\\n--genome GRCh37 \\\n-profile docker\n</code></pre> <p>Where your <code>samplesheet.csv</code> will include:</p> <p>samplesheet.csv</p> <pre><code>sample,fastq_1,fastq_2,strandedness\nCONTROL_REP1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L003_R1_001.fastq.gz,AEG588A1_S1_L003_R2_001.fastq.gz,auto\nCONTROL_REP1,AEG588A1_S1_L004_R1_001.fastq.gz,AEG588A1_S1_L004_R2_001.fastq.gz,auto\n</code></pre> <p>However, this is a multi-faceted pipeline, with many additional parameters you can tweak for your needs. To better understand those parameters, always check the <code>Usage docs</code> and <code>Parameters</code> sections of each pipeline's documentation.</p>"},{"location":"hpc_tools/nextflow/#building-your-own-nextflow-pipeline","title":"Building your own Nextflow Pipeline","text":""},{"location":"hpc_tools/nextflow/#references","title":"References","text":"<ol> <li>Nextflow Github</li> <li>RNA-Seq Nextflow</li> </ol>"},{"location":"hpc_tools/parallel/","title":"Introduction to Parallel","text":"<p>GNU Parallel is a shell tool that allows for independent jobs to be run in parallel over multiple compute resources. </p>"},{"location":"hpc_tools/parallel/#bash-loop-using-parallel","title":"Bash Loop Using Parallel","text":"<p>GNU Parallel can greatly speed up a task given that it can leverage multiple compute resources at once. Let's examine the case of the following bash loop (example taken from Yale Center For Research Computing - Parallel):</p> <pre><code>for letter in {a..f};\ndo\necho $letter\ndone\n</code></pre> <p>output</p> <pre><code>a\nb\nc\nd\ne\nf\n</code></pre> <p>To parallelize this task we can use the <code>parallel</code> module and ask for multiple CPUs per task:</p> <pre><code>salloc -c 4\nmodule load parallel\nparallel -j 4 \"echo {}\" ::: {a..f}\n</code></pre> <p>output</p> <pre><code>a\nb\nc\nd\ne\nf\n</code></pre>"},{"location":"hpc_tools/parallel/#parallel-in-a-bash-script","title":"Parallel In A Bash Script","text":"<p>Additionally, we can leverage the <code>parallel</code> module in a batch script:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=runParallel\n#SBATCH --time=01-00:00:00\n#SBATCH --nodes=1\n#SBATCH -c 8\n#SBATCH --mem=4G\n#SBATCH --output=%x.%j.out\n#SBATCH --error=%x.%j.err\n\n# load modules\nmodule load parallel\nmodule load fastqc\n\n# make an output directory\nmkdir fastqc_output\n\n# find all fastq files and run fastqc them\nls *.fastq.gz | parallel -j ${SLURM_CPUS_PER_TASK} \"fastqc {} -o fastqc_output\"\n</code></pre> <p>Here we load the <code>parallel</code> and <code>fastqc</code> modules. We then create an output directory (<code>fastqc_output</code>). In our command we list all our fastq files (<code>ls *.fastq.gz</code>), then use the parallel command to run fastqc on each file (<code>fastqc {} -o fastqc_output</code>). We reference each fastq file with the curly brackets <code>{}</code>. You'll also notice that we specify how many compute resources are available with <code>-j ${SLURM_CPUS_PER_TASK}</code>.</p>"},{"location":"hpc_tools/parallel/#references","title":"References","text":"<ol> <li>https://www.gnu.org/software/parallel/</li> <li>https://docs.ycrc.yale.edu/clusters-at-yale/guides/parallel/</li> </ol>"},{"location":"hpc_tools/python-conda/","title":"Conda Environments","text":""},{"location":"hpc_tools/python-conda/#conda-on-the-hpc","title":"Conda on the HPC","text":"<p>It is often desirable to download your own python packages into a conda environment. We will quickly go through how to create one and add packages to that conda environment.</p> <ul> <li>Login to the HPC cluster either by Command Line or the OnDemand Website. For information on how to log into the cluster check out:</li> </ul> <p>Navigate To The Cluster</p> <ul> <li>Start an interactive session on the Tufts HPC Cluster to work on a compute node. To learn more about how to set up an interactive session visit:</li> </ul> <p>Start an Interative Session</p> <ul> <li>Load relevant modules:</li> </ul> <pre><code>module load anaconda/2021.11\n</code></pre> <p>Info</p> <p>You may need to load other modules, such as <code>cuda</code> if you plan to utilize GPUs:</p> <pre><code>module load cuda/11.0\n</code></pre>"},{"location":"hpc_tools/python-conda/#create-your-conda-environment","title":"Create your conda environment","text":"<ul> <li>Now you can create your own conda env:</li> </ul> <pre><code>cd /cluster/tufts/XXXXlab/$USER/condaenv/\nconda create -p yourenvname\n</code></pre> <ul> <li>Or if you have a specific version of python you need to use, e.g. 3.8 (Recommended!):</li> </ul> <pre><code>conda create -p yourenvname python=3.8 \n</code></pre> <p>Note</p> <p>you will need to have python and pip installed inside the env to pip install packages inside the env.</p> <ul> <li>Activate the environment (needs to be executed whenever you need to use the conda env you have created)</li> </ul> <pre><code>source activate yourenvname\n</code></pre> <ul> <li>If you are using system installed conda, please DO NOT use conda activate to activate your environment Install yourpackage in the conda env</li> </ul> <pre><code>conda install yourpackage\n</code></pre> <ul> <li>Or if you have python (comes with pip) installed</li> </ul> <pre><code>pip install yourpackage\n</code></pre> <ul> <li>Or follow the instruction on package website. Check what's installed in your conda environment:</li> </ul> <pre><code>conda list\n</code></pre> <ul> <li>When you are done, deactivate the environment:</li> </ul> <pre><code>conda deactivate\n</code></pre>"},{"location":"hpc_tools/python-conda/#additional-information-for-jupyter-users-run-conda-env-as-a-kernel-in-jupyter","title":"Additional Information for Jupyter Users: Run conda env as a kernel in Jupyter","text":"<ul> <li>If you would like to use JupyterNotebook or JupyterLab from OnDemand, you can follow the instructions below and run your conda env as a kernel in Jupyter.</li> <li>Make sure with python 3.7+ and make sure you load cluster's anaconda module (this only works with py3.7+)</li> <li>Activate your conda env from terminal. Install ipykernel with:</li> </ul> <pre><code>pip install ipykernel \n</code></pre> <p>Note</p> <p>this assumes you installed python and pip in your env, otherwise, use \"--user\" flag</p> <ul> <li>Add your env to jupyter with:</li> </ul> <pre><code>python -m ipykernel install --user --name=myenvname \n</code></pre> <ul> <li>Restart Jupyter from OnDemand </li> </ul>"},{"location":"hpc_tools/python-jupyter/","title":"JupyterLab","text":"<p>Often times you like to test your python code using an interactive development environment or IDE. We offer the python IDE, JupyterLab, to do just that. Here's how to request a Jupiter lab session OnDemand:</p> <ul> <li>Go to:</li> </ul> <p>OnDemand</p> <ul> <li>Navigate to \"Interactive Apps\"</li> <li>Scroll down and click on \"JupyterLab\"</li> <li> <p>Select:</p> <ul> <li>how long of a session you would like</li> <li>the number of cores, the memory</li> <li>your python version</li> <li>and your reservation</li> </ul> </li> </ul> <p></p> <ul> <li>Click \"Launch\" </li> <li>Once your session is ready, click on the \"Connect to JupyterLab\"</li> </ul>"},{"location":"hpc_tools/python-modules/","title":"Python Interactive Session","text":""},{"location":"hpc_tools/python-modules/#python-interactive-session","title":"Python Interactive Session","text":"<ul> <li>Login to the HPC cluster either by Command Line or the OnDemand Website. For information on how to log into the cluster check out:</li> </ul> <p>Navigate To The Cluster</p> <ul> <li>From the login node, load the python module </li> </ul> <pre><code>module load python/3.8.8\n</code></pre> <ul> <li>To check out different python modules enter the command:</li> </ul> <pre><code>module av python\n</code></pre> <ul> <li>Allocate computing resources. Start an interactive session with your desired number of cores and memory, here we are using 2 cores with 4GB of memory: </li> </ul> <pre><code>srun -p interactive -n 2 --mem=4g --pty bash\n</code></pre> <ul> <li> <p>The Interactive partition has a default 4-hour time limit </p> </li> <li> <p>For more information on how to allocate resources on Tufts HPC cluster, check out:</p> </li> </ul> <p>Compute Resources</p>"},{"location":"hpc_tools/r-batch/","title":"R Batch Jobs","text":""},{"location":"hpc_tools/r-batch/#r-batch-jobs","title":"R batch jobs","text":"<p>Sometimes an R script will take to long to either run via an interactive session or RStudio. In these cases we can submit the R script as a batch job.</p> <ul> <li>Login to the HPC cluster either by Command Line or the OnDemand Website. For information on how to log into the cluster check out:</li> </ul> <p>Navigate To The Cluster</p> <ul> <li> <p>Upload your R script to the HPC cluster</p> </li> <li> <p>Go to the directory/folder which contains your R script</p> </li> <li> <p>Open your favorite text editor and write a slurm submission script similar to the following one <code>batchjob.sh</code> (name your own)</p> </li> </ul> <p>batchjob.sh</p> <pre><code>#!/bin/bash\n#SBATCH -J myRjob  #job name\n#SBATCH --time=00-00:20:00 #requested time\n#SBATCH -p batch  #running on \"batch\" partition/queue\n#SBATCH -n 2  #2 cores total\n#SBATCH --mem=2g #requesting 2GB of RAM total\n#SBATCH --output=myRjob.%j.out #saving standard output to file\n#SBATCH --error=myRjob.%j.err  #saving standard error to file\n#SBATCH --mail-type=ALL  #email optitions\n#SBATCH --mail-user=Your_Tufts_Email @tufts.edu\nmodule load R/4.0.0\nRscript --no-save your_rscript_name.R\n</code></pre> <ul> <li>Submit it with: </li> </ul> <pre><code>sbatch batchjob.sh\n</code></pre> <ul> <li>If you are submitting multiple batch jobs to run the same script on different datasets, please make sure they are saving results to different files inside of your R script.</li> </ul>"},{"location":"hpc_tools/r-interactive/","title":"R Interactive Session","text":""},{"location":"hpc_tools/r-interactive/#r-interactive-session","title":"R Interactive Session","text":"<ul> <li>Login to the HPC cluster either by Command Line or the OnDemand Website. For information on how to log into the cluster check out:</li> </ul> <p>Navigate To The Cluster</p> <ul> <li>From the login node, load R module and associated modules</li> </ul> <pre><code>module load R/4.0.0 boost/1.63.0-python3 java/1.8.0_60 gsl/2.6\n</code></pre> <ul> <li> <p>Additional modules may need to be loaded, such as <code>sf/R_4.0.0</code> </p> </li> <li> <p>Allocate computing resources. Start an interactive session with your desired number of cores and memory, here we are using 2 cores with 4GB of memory: </p> </li> </ul> <pre><code>srun -p interactive -n 2 --mem=4g --pty bash\n</code></pre> <ul> <li> <p>The Interactive partition has a default 4-hour time limit. </p> </li> <li> <p>For more information on how to allocate resources on Tufts HPC cluster, check out:</p> </li> </ul> <p>Compute Resources</p> <ul> <li>Within the interactive session, you can start R </li> </ul> <pre><code>R\n</code></pre>"},{"location":"hpc_tools/r-interactive/#installing-r-packages","title":"Installing R packages","text":"<ul> <li>In R, you can install the packages you need in your home directory with:</li> </ul> <pre><code>install.packages(\"XXX\")\n</code></pre> <ul> <li>You can also use the packages installed in HPC Tools R package repo:</li> </ul> <pre><code>LIB='/cluster/tufts/hpc/tools/R/4.0.0' .libPaths(c(\"\",LIB))\n</code></pre> <ul> <li>You can also use packages installed in BioTools R package repo:</li> </ul> <pre><code>LIB='/cluster/tufts/bio/tools/R_libs/4.0.0' .libPaths(c(\"\",LIB)) </code></pre> <ul> <li> <p>If you are having trouble installing the packages you need, please contact tts-research@tufts.edu.</p> </li> <li> <p>To exit from R command line interface:</p> </li> </ul> <p><pre><code>q()\n</code></pre> - To terminate interactive session </p> <pre><code>exit\n</code></pre>"},{"location":"hpc_tools/r-interactive/#r-package-installation-troubleshooting","title":"R Package Installation Troubleshooting","text":"<p>Suggestion 1: Try installing the R package from command line instead of RStudio OnDemand</p> <ul> <li>The RStudio OnDemand interface is not perfect and can store things like different libPaths between sessions. To be safe, it is always best to install new R package from the command line when on the Tufts HPC.</li> </ul> <p>Suggestion 2: You may need to load more modules</p> <ul> <li>On the Tufts HPC you load modules of software that might already be installed on your machine. This is why it can be easier to install R packages on your local machine rather than the Tufts HPC. If you are unsuccessful at installing an R package, try loading the following modules:</li> </ul> <pre><code>module load curl/7.47.1 gcc/7.3.0 hdf5/1.10.4 boost/1.63.0-python3 libpng/1.6.37 java/1.8.0_60 libxml2/2.9.10 libiconv/1.16 fftw/3.3.2 gsl/2.6 R/4.0.0\n</code></pre> <p>Suggestion 3: Take a look at the last few lines of the error message</p> <ul> <li>The error message will give you a clue as to what is going wrong. For example a common example:</li> </ul> <pre><code>Error in loadNamespace(j &lt;- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]) : \n  there is no package called \u2018Rcpp\u2019\nError: package or namespace load failed for \u2018ggplot2\u2019\n</code></pre> <ul> <li>Here you might need to install a dependency beforehand with either:</li> </ul> <pre><code>install.packages(\"Rcpp\")\n</code></pre> <p>or:</p> <pre><code>install.packages(\"ggplot2\",dependencies = TRUE)\n</code></pre> <p>Suggestion 4: Install the package from the Repository</p> <ul> <li>Dependant on the R module you are loading, you may be working with an older version of a package manager, like BiocManager. As such some of the packages might have dependencies that are deprecated. For example, when installing <code>APAlyzer</code>:</li> </ul> <pre><code>ERROR: dependency \u2018DESeq\u2019 is not available for package \u2018APAlyzer\u2019\n* removing \u2018/cluster/home/user/R/x86_64-pc-linux-gnu-library/4.0/APAlyzer\u2019\n\nThe downloaded source packages are in\n    \u2018/tmp/Rtmpraf0Ix/downloaded_packages\u2019\nInstallation paths not writeable, unable to update packages\n  path: /opt/shared/R/4.0.0/lib64/R/library\n  packages:\n    boot, class, cluster, codetools, foreign, KernSmooth, lattice, MASS,\n    Matrix, mgcv, nlme, nnet, rpart, spatial, survival\nOld packages: 'openssl', 'Seurat'\nUpdate all/some/none? [a/s/n]: n\nWarning message:\nIn install.packages(...) :\n  installation of package \u2018APAlyzer\u2019 had non-zero exit status\n</code></pre> <ul> <li>We can try installing <code>APAlyzer</code> like so:</li> </ul> <pre><code>BiocManager::install('RJWANGbioinfo/APAlyzer')\n</code></pre> <p>Suggestion 5: Try updating the packages it asks to update</p> <ul> <li>R usually tries to tells you what it needs to proceed. Oftentimes when you install a package, you will be prompted to update the packages you have:</li> </ul> <pre><code>These packages have more recent versions available.\nIt is recommended to update all of them.\nWhich would you like to update?\n\n1: All                                           2: CRAN packages only                            3: None                                          4: sitmo        (2.0.1      -&gt; 2.0.2     ) [CRAN]\n5: BH           (1.75.0-0   -&gt; 1.81.0-1  ) [CRAN]\n6: dqrng        (0.2.1      -&gt; 0.3.0     ) [CRAN]\n7: irlba        (2.3.3      -&gt; 2.3.5.1   ) [CRAN]\n...\n...\n...\nEnter one or more numbers, or an empty line to skip updates: 1\n</code></pre> <ul> <li>Some packages require specific/updated versions of the packages you have. Package installation issues can be circumvented when you enter <code>1</code> to update all packkages. </li> </ul> <p>Suggestion 6: Restarting R</p> <ul> <li>The old IT addage of turning it on and off again is not just all talk. The way you set your libPath or the packages you already have loaded may interrupt your ability to install packages. You can restart R to wipe the proverbial slate clean by going to <code>Session &gt; Restart R</code>. Now try to install your package! </li> </ul>"},{"location":"hpc_tools/r-rstudio/","title":"RStudio OnDemand","text":""},{"location":"hpc_tools/r-rstudio/#rstudio-interactive-app-on-ondemand","title":"RStudio Interactive App on OnDemand","text":"<ul> <li>Login to the HPC cluster either by Command Line or the OnDemand Website. For information on how to log into the cluster check out:</li> </ul> <p>Navigate To The Cluster</p> <ul> <li>Go to \"Interactive Apps\" tab </li> <li>Select \"RStudio\"</li> <li> <p>Select the:</p> <ul> <li>the time needed on the app</li> <li>number of cores </li> <li>CPU memory you need</li> <li>the version of R you wish to run. </li> <li> <p>the modules needed for your pacakges to run </p> <ul> <li>typically the following are sufficient: </li> </ul> <pre><code>boost/1.63.0-python3 java/1.8.0_60 gsl/2.6\n</code></pre> </li> </ul> </li> </ul> <p>Note</p> <ul> <li>Each user can only start one OnDemand RStudio session on one compute node at a time. If you need to start multiple RStudio sessions, please make sure you select a different nodename from your current running session. </li> </ul> <p></p> <ul> <li>Click \"Launch\"</li> <li>Click on \"Connect to RStudio\"</li> <li> <p>When you are finished:</p> <ul> <li>exit RStudio properly <code>q()</code></li> <li>close the RStudio tab</li> <li>Go back to the main OnDemand page </li> <li>Click \"Delete\" to end the session</li> </ul> </li> </ul>"},{"location":"hpc_tools/rclone/","title":"Rclone","text":""},{"location":"hpc_tools/rclone/#rclone","title":"Rclone","text":"<p>Rclone is a tool to manage files stored on cloud storage. Some examples of this are Box, Google Drive, Amazon, etc.. To connect Rclone to the cluster,  you will need to install it on your local machine to set up your access token and then configure your Rclone on the cluster using this token.</p>"},{"location":"hpc_tools/rclone/#rclone-installation","title":"Rclone Installation","text":"<ul> <li>To set up Rclone on your local machine follow the instructions for your machine type:</li> </ul> <p>Rclone Installation</p>"},{"location":"hpc_tools/rclone/#rclone-configuration","title":"Rclone Configuration","text":"<ul> <li>On your local machine navigate to your terminal/command prompt and enter the following command to authorize the connection to the clound storage of your choice. Here we will demonstrate how to connect to Box:</li> </ul> <pre><code>rclone authorize \"box\"\n</code></pre> <ul> <li>Now the following text will pop up along with a web browser with instructions to enter your credentials. Follow the instructions and take the SECRET_TOKEN they provide and paste it into your terminal/command prompt whent prompeted:</li> </ul> <pre><code>If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth\nLog in and authorize rclone for access\nWaiting for code...\nGot code\nPaste the following into your remote machine ---&gt;\nSECRET_TOKEN\n&lt;---End paste\n</code></pre> <ul> <li>Now on navigate to the cluster (if you do not know how check out these instructions) and enter the following command:</li> </ul> <pre><code>module load rclone\n</code></pre> <ul> <li>Now we will start creating a remote file:</li> </ul> <pre><code>rclone config\n</code></pre> <ul> <li>You will be prompted to create a remote file:</li> </ul> <pre><code>No remotes found, make a new one?\nn) New remote\ns) Set configuration password\nq) Quit config\nn/s/q&gt; n\nname&gt; remote\nType of storage to configure.\nChoose a number from below, or type in your own value\n[snip]\nXX / Box\n   \\ \"box\"\n[snip]\nStorage&gt; box\nBox App Client Id - leave blank normally.\nclient_id&gt; \nBox App Client Secret - leave blank normally.\nclient_secret&gt;\nBox App config.json location\nLeave blank normally.\nEnter a string value. Press Enter for the default (\"\").\nbox_config_file&gt;\nBox App Primary Access Token\nLeave blank normally.\nEnter a string value. Press Enter for the default (\"\").\naccess_token&gt;\n\nEnter a string value. Press Enter for the default (\"user\").\nChoose a number from below, or type in your own value\n 1 / Rclone should act on behalf of a user\n   \\ \"user\"\n 2 / Rclone should act on behalf of a service account\n   \\ \"enterprise\"\nbox_sub_type&gt;\nRemote config\nUse web browser to automatically authenticate rclone with remote?\n * Say Y if the machine running rclone has a web browser you can use\n * Say N if running rclone on a (remote) machine without web browser access\nIf not sure try Y. If Y failed, try N.\ny) Yes\nn) No\ny/n&gt; n\nFor this to work, you will need rclone available on a machine that has\na web browser available.\n\nFor more help and alternate methods see: https://rclone.org/remote_setup/\n\nExecute the following on the machine with the web browser (same rclone\nversion recommended):\n\n    rclone authorize \"box\"\n\nThen paste the result below:\nresult&gt;\n</code></pre> <ul> <li>Now, in the result field enter the SECRET_TOKEN you got on your local machine:</li> </ul> <pre><code>result&gt; SECRET_TOKEN\n--------------------\n[acd12]\nclient_id = \nclient_secret = \ntoken = SECRET_TOKEN\n--------------------\ny) Yes this is OK\ne) Edit this remote\nd) Delete this remote\ny/e/d&gt;\n</code></pre> <p>Success</p> <p>Congrats! You have Rclone configured on the Tufts HPC Cluster!</p>"},{"location":"hpc_tools/rclone/#rclone-cluster-use","title":"Rclone Cluster Use","text":"<ul> <li>To examine files in your remote storage you can use the following command (assuming the name of your remote is <code>box</code>):</li> </ul> <pre><code>rclone ls box:/\n</code></pre> <ul> <li>To copy those files to your location on the cluster you can use (assuming the name of your remote is <code>box</code>):</li> </ul> <pre><code># to download a file to the cluster\nrclone copy box:/path/to/file .\n\n# to upload a file from the cluster to the cloud storage\nrclone copy filename box:/path/to/desitination/\n</code></pre> <ul> <li>To learn more about the different types of Rclone commands use the help option:</li> </ul> <pre><code>rclone help\n</code></pre>"},{"location":"hpc_tools/rclone/#references","title":"References","text":"<ol> <li>Rclone</li> </ol>"},{"location":"hpc_tools/vdi/","title":"Tufts VDI","text":""},{"location":"hpc_tools/vdi/#tufts-virtual-lab-vdi-desktop-client","title":"Tufts Virtual Lab (VDI) Desktop Client","text":"<p>Aside from the Tufts HPC, Tufts maintains a Virtual Lab (VDI) Desktop Client with a number of software, tools, and shared data already configured. </p>"},{"location":"hpc_tools/vdi/#vdi-installation","title":"VDI Installation","text":"WindowsMac <ol> <li>Navigate to https://vdi.it.tufts.edu </li> <li>Click Install VMWare Horizon Client.</li> <li>From the list of clients, locate the VMware Horizon Client for Windows, select the appropriate row for your system, then, click the Go to Downloads link on the far right to download the client.</li> <li>Start the installation.</li> <li>Once completed, restart your computer.</li> <li>Open the VMWare client.</li> <li>The only customization required for install is the selection of IPv4 as the IP protocol; leave everything else as the default.</li> <li>Click the plus icon.</li> <li>Input vdi.it.tufts.edu as the server to connect to.</li> <li>Input your Tufts Username and Tufts Password to sign in. Make sure the Domain says TUFTS.</li> <li>Open the available desktop or application.</li> </ol> <ol> <li>Navigate to https://vdi.it.tufts.edu</li> <li>Click \"Click Here to Download VMWare Horizon Client\"</li> <li>Click Here is at the bottom of the screen</li> <li>You will be taken to the VMWare Horizon website with a list of clients, locate the correct row for you (VMware Horizon Client for Mac or VMware Horizon Client for Linux); then, click the Go to Downloads link on the far right to download the client.<ul> <li>Tip: If you have a Linux machine, it is most likely 64-bit. If you'd like to check, follow these \"Check if your linux system is 32bit or 64bit\" instructions.</li> </ul> </li> <li>Start the installation.</li> <li>Once completed, locate your new application and open.</li> <li>Click the Add Server + icon.</li> <li>The plus is under new server</li> <li>Input vdi.it.tufts.edu as the Connection Server.</li> <li>Connection Server is the only field in the popup box</li> <li>Click Connect.</li> <li>A Tufts login screen may open in a browser window. Authenticate with your Tufts username and password.</li> <li>In the VDI panel, input your Tufts Username and Tufts Password to sign in. Make sure the Domain says TUFTS.</li> <li>Login is underneath the login fields</li> <li>Click Login.</li> </ol>"},{"location":"hpc_tools/vdi/#connecting-your-local-machine-to","title":"Connecting your local Machine to","text":"WindowsMac <ol> <li>Click on the settings icon at the top of the window:</li> <li>Click \u201cDrive and Network Sharing\u201d </li> <li>Select the local drive you\u2019d like to add</li> <li>Now click \u201cTTS Virtual Lab\u201c to start your session</li> <li>To find your local machine, go to \u201cThis PC\u201c &gt; \u201cNetwork Drive \u201c</li> </ol> <ol> <li>Click \u201cVMware Horizon Client\u201d at the top of your MAC, Then click \u201cPreferences\u201d</li> <li>Click \u201cDrive Sharing\u201d</li> <li>Select the local drive you\u2019d like to add</li> <li>Now click \u201cTTS Virtual Lab\u201c to start your session</li> <li>To find your local machine, go to \u201cThis PC\u201c &gt; \u201cNetwork Drive \u201c</li> </ol>"},{"location":"hpc_tools/vdi/#software-on-the-vdi","title":"Software on the VDI","text":"<p>The following software are on the Tufts VDI:</p> <ul> <li>7-zip</li> <li>Adobe Flash Player</li> <li>Adobe Acrobat Reader</li> <li>ArcGIS</li> <li>Audacity</li> <li>Box Edit and Box for Office</li> <li>Brainstorm</li> <li>CAST</li> <li>Cisco Jabber</li> <li>DNRGPS</li> <li>Endnote x8</li> <li>ENVI</li> <li>EPI Data and EPI Info</li> <li>FME Desktop</li> <li>GDAL</li> <li>GeoDa</li> <li>JASP</li> <li>JMP</li> <li>KNIME   </li> <li>Mathematica</li> <li>Matlab R2017a</li> <li>Mega</li> <li>Mendeley Desktop</li> <li>NotePad ++</li> <li>PyCharm Edu</li> <li>Python</li> <li>QGIS</li> <li>R and Rstudio</li> <li>SAS</li> <li>SNAP</li> <li>Skype</li> <li>SPSS</li> <li>STATA</li> <li>Tableau Desktop (Lab license)</li> <li>VLC Media Player</li> <li>VMware Tools</li> <li>WebEx</li> <li>WinSCP</li> <li>Wolfram Extras</li> <li>Write-N-Cite</li> <li>Zotero</li> </ul>"},{"location":"hpc_user_guide/batch-job/","title":"Submitting A Batch Job","text":""},{"location":"hpc_user_guide/batch-job/#batch-scripts","title":"Batch Scripts","text":"<ul> <li>When we want to run a script on the Tufts HPC cluster we need to submit it as a batch script.</li> <li>Here is an example of a batch script called sbatch.sh:</li> </ul> <p>sbatch.sh</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=job            # job name is \"job\"\n#SBATCH --nodes=1                 # 1 nodes #for many shared-memory programs,please leave -N as 1.\n#SBATCH -n 2                      # 2 tasks total and 1 cpu per task, that gives you 2 cpu cores for this job\n#SBATCH --partition=batch         # running on \"batch\" partition/queue\n#SBATCH --reservation=bioworkshop # running on a reservation, named \"bioworkshop\", if no access to reservation, omit this line\n#SBATCH --mem=8Gb                 # requesting 8GB of RAM total for the number of cpus you requested\n#SBATCH --time=0-24:00:00         # requested time (DD-HH:MM:SS) 24 hours\n#SBATCH --output=%j.out           # saving standard output to file, %j=JOBID\n#SBATCH --error=%j.err            # saving standard error to file, %j=JOBID\n#SBATCH --mail-type=ALL           # email optitions\n#SBATCH --mail-user=Your_Tufts_Email@tufts.edu  # use your own Tufts email address\n\n# [this is a comment]\n# The order of the \"#SBATCH\" options doesn't matter\n\n#[commands_you_would_like_to_exe_on_the_compute_nodes]\n# for example, running blast \n# load the module so the correct version of blast is available to you\n\nmodule load blast-plus/2.11.0\n\n# running blast\nblastp -query mm-second.faa -db zebrafish.1.protein.faa -out mm-second.x.zebrafish.tsv -outfmt 6\n</code></pre>"},{"location":"hpc_user_guide/batch-job/#submitting-a-batch-job","title":"Submitting a Batch Job","text":"<ul> <li>Submit the job using the following command from command line interface:</li> </ul> <pre><code>sbatch sbatch.sh\n</code></pre> <p>Looking for a sample batch script?</p> <p>Sample Scripts including R, conda, matlab, gaussian, etc. can be found here:</p> <pre><code>/cluster/tufts/hpc/tools/slurm_scripts\n</code></pre>"},{"location":"hpc_user_guide/cluster_storage_increase/","title":"Cluster Storage Increase Request","text":""},{"location":"hpc_user_guide/cluster_storage_increase/#cluster-storage-increase","title":"Cluster Storage Increase","text":"<p>If you need more storage, we are happy to give an increase. But please be mindful and clean up any data that is not needed once you are done with data analysis. To submit an official request for Cluster Storage Increase, please do the following and consider how much data you will generate or work with in the next 12 months.</p> <ol> <li>Use the following link to request an increase in cluster storage:</li> </ol> <p>Cluster Storage Increase Request</p> <ol> <li>Login with your Tufts credentials</li> <li>Fill out blank fields</li> <li>Select \"Research Storage Request\"</li> <li>Select \"Cluster (HPC)\"</li> <li>Select \"Increase\"</li> <li>For FULL PATH of STORAGE input: GroupName | /cluster/tufts/GroupName/</li> <li>Click next</li> <li>Fill out remaining information</li> <li>Click on Submit </li> </ol> <p>Once the request is submitted and the storage owner approves the request, we will apply the increase.</p>"},{"location":"hpc_user_guide/cluster_storage_increase/#questions","title":"Questions","text":"<p>If you have any questions about requesting a cluster storage account, please reach out to tts-research@tufts.edu</p>"},{"location":"hpc_user_guide/compute-resources/","title":"Compute Resources","text":""},{"location":"hpc_user_guide/compute-resources/#compute-resources","title":"Compute Resources","text":""},{"location":"hpc_user_guide/compute-resources/#cpus","title":"CPUs","text":"<ul> <li>Resources are orgnized into partitions on the cluster based on functionality and priority.</li> <li>After logging in on the HPC cluster, you can use command <code>sinfo</code> to check the <code>partition</code> you have access to (all partitions listed in the <code>sinfo</code> output).</li> </ul> <pre><code>sinfo\n</code></pre> <p>output</p> <pre><code>PARTITION    AVAIL  TIMELIMIT  NODES  STATE NODELIST \ninteractive     up    4:00:00      1    mix c1cmp064 \ninteractive     up    4:00:00      1   idle c1cmp063 \nbatch*          up 7-00:00:00      1  down* p1cmp005 \nbatch*          up 7-00:00:00      1  drain p1cmp056 \nbatch*          up 7-00:00:00     16   resv c1cmp[009,033,035-039,044-049],p1cmp[004,009,054] \nbatch*          up 7-00:00:00     34    mix c1cmp[003-008,010-020,023-024,034,040-043,051-052,054],i2cmp001,p1cmp[003,012,015,018,020-021] \nbatch*          up 7-00:00:00     17  alloc c1cmp[021-022,053],i2cmp003,p1cmp[001,006-008,010-011,013-014,019,022-024,055] \nbatch*          up 7-00:00:00      2   idle p1cmp[016-017] \nmpi             up 7-00:00:00      1  down* p1cmp005 \nmpi             up 7-00:00:00      1  drain p1cmp056 \nmpi             up 7-00:00:00     16   resv c1cmp[009,033,035-039,044-049],p1cmp[004,009,054] \nmpi             up 7-00:00:00     34    mix c1cmp[003-008,010-020,023-024,034,040-043,051-052,054],i2cmp001,p1cmp[003,012,015,018,020-021] \nmpi             up 7-00:00:00     16  alloc c1cmp[021-022,053],p1cmp[001,006-008,010-011,013-014,019,022-024,055] \nmpi             up 7-00:00:00      2   idle p1cmp[016-017] \ngpu             up 7-00:00:00      1    mix p1cmp073 \ngpu             up 7-00:00:00      2  alloc c1cmp[025-026] \nlargemem        up 7-00:00:00      7    mix c1cmp[027-028,030,057,061-062],i2cmp055 \nlargemem        up 7-00:00:00      2  alloc p1cmp[049-050] \nlargemem        up 7-00:00:00      3   idle c1cmp[032,058-059] \npreempt         up 7-00:00:00      2   mix$ p1cmp[094-095] \npreempt         up 7-00:00:00      4  maint p1cmp[090,092,103,109] \npreempt         up 7-00:00:00      1  down* p1cmp005 \npreempt         up 7-00:00:00      2  drain p1cmp[038,056] \npreempt         up 7-00:00:00      3   resv p1cmp[004,009,054] \npreempt         up 7-00:00:00     71    mix cc1gpu[001-005],i2cmp[010-032,038-043,045-051],p1cmp[003,012,015,018,020-021,070-077,079-080,091,093,096,098-102,104-108,110] \npreempt         up 7-00:00:00     25  alloc c1cmp[025-026],i2cmp[004-006,008-009,033-035,037,052-053],p1cmp[006-008,010-011,013-014,019,022-024,055] \npreempt         up 7-00:00:00     20   idle p1cmp[016-017,031-037,039-042,081-086,097] \n</code></pre> <ul> <li> <p>If you are curious about nodes and the details of those nodes you can go to:</p> <ul> <li>OnDemand <code>Misc</code> &gt; <code>Inventory</code> to dipslay more node details (core count &amp; memory)</li> </ul> </li> </ul> <p></p> <p></p>"},{"location":"hpc_user_guide/compute-resources/#gpus","title":"GPUs","text":"<ul> <li> <p>NVIDIA GPUs are available in <code>gpu</code> and <code>preempt</code> partitions</p> </li> <li> <p>Request GPU resources with <code>--gres</code>. See details below.</p> </li> <li>Please DO NOT manually set <code>CUDA_VISIBLE_DEVICES</code>. </li> <li>Users can ONLY see GPU devices that are assigned to them with <code>$ nvidia-smi</code>.</li> <li><code>gpu</code> partition<code>-p gpu</code>:</li> <li>NVIDIA P100s<ul> <li>In \"gpu\" partition</li> <li>Request with: <code>--gres=gpu:p100:1</code>(one P100 GPU, can request up to 6 on one node)</li> </ul> </li> <li>NVIDIA Tesla K20xm<ul> <li>In \"gpu\" partition</li> <li>Request with: <code>--gres=gpu:k20xm:1</code>(one Tesla K20xm GPU, can request up to 1 on one node)</li> </ul> </li> <li><code>preempt</code> partition <code>-p preempt</code>:</li> <li><code>a100</code>, <code>v100</code>, <code>p100</code>, <code>rtx_6000</code>, <code>t4</code></li> <li>NVIDIA T4<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:t4:1</code>(one T4 GPU, can request up to 4 on one node)</li> </ul> </li> <li>NVIDIA P100<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:p100:1</code>(one P100 GPU, can request up to 4 on one node)</li> </ul> </li> <li>NVIDIA rtx_6000<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:rtx_6000:1</code>(one RTX_6000 GPU, can request up to 8 on one node)</li> </ul> </li> <li>NVIDIA V100<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:v100:1</code>(one V100 GPU, can request up to 4 on one node)</li> </ul> </li> <li>NVIDIA A100<ul> <li>In \"preempt\" partition</li> <li>Request with: <code>--gres=gpu:a100:1</code>(one A100 GPU, can request up to 8 on one node)</li> </ul> </li> </ul>"},{"location":"hpc_user_guide/file-transfer/","title":"File Transfer","text":"<p>You can transfer files to and from the cluster using:</p> <ul> <li>OnDemand</li> <li>A File Transfer Client</li> <li>Command Line</li> </ul>"},{"location":"hpc_user_guide/file-transfer/#ondemand","title":"OnDemand","text":"<p>Note</p> <p>Only for transfering files size up to 976MB per file.</p> <ul> <li>Go to:</li> </ul> <p>OnDemand</p> <ul> <li>Under <code>Files</code></li> </ul> <p></p> <ul> <li>Using the <code>Upload</code> or <code>Download</code> buttons to transfer. </li> </ul> <p></p>"},{"location":"hpc_user_guide/file-transfer/#file-transfer-client","title":"File Transfer Client","text":"<ul> <li> <p>Download one of these free file transfer programs:</p> <ul> <li> <p>WinSCP </p> </li> <li> <p>FileZilla </p> </li> <li> <p>Cyberduck </p> </li> </ul> </li> <li> <p>Then use the following information to connect to the cluster:</p> <ul> <li>Hostname: xfer.cluster.tufts.edu</li> <li>Protocol: SCP or SFTP</li> <li>Use port 22 for SFTP</li> </ul> </li> </ul>"},{"location":"hpc_user_guide/file-transfer/#command-line","title":"Command Line","text":"<p>Terminology</p> <ul> <li>Local_Path: is the path to your files or directory on your local computer</li> <li>Cluster_Path: is the path to your files or directory on the cluster</li> <li>Cluster Home Directory: /cluster/home/your_utln/your_folder</li> <li>Cluster Home Directory: /cluster/home/your_utln/your_folder</li> <li>Cluster Research Project Storage Space Directory: /cluster/tufts/yourlabname/your_utln/your_folder</li> </ul> <ul> <li>Execute these commands from your local machine terminal using this general format to transfer files:</li> </ul> <pre><code>scp From_Path To_Path\n</code></pre> <pre><code>rsync From_Path To_Path\n</code></pre> <p>Note</p> <p>If you are transfering very large files that could take hours to finish, we would suggest using <code>rsync</code> as it has ability to restart from where it left if interrupted.</p>"},{"location":"hpc_user_guide/file-transfer/#download-from-cluster","title":"Download from cluster","text":"<pre><code>scp your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path\n</code></pre> <pre><code>rsync your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path\n</code></pre>"},{"location":"hpc_user_guide/file-transfer/#upload-to-cluster","title":"Upload to cluster","text":"<pre><code>scp Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path </code></pre> <pre><code>rsync Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path\n</code></pre> <p>Tip</p> <p>If you are transfering a directory use <code>scp -r</code> or <code>rsync -azP</code></p>"},{"location":"hpc_user_guide/file-transfer/#download-from-cluster_1","title":"Download from cluster","text":"<pre><code>scp -r your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path  </code></pre> <pre><code>rsync -azP your_utln@xfer.cluster.tufts.edu:Cluster_Path Local_Path\n</code></pre>"},{"location":"hpc_user_guide/file-transfer/#upload-to-cluster_1","title":"Upload to cluster","text":"<pre><code>scp -r Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path\n</code></pre> <pre><code>rsync -azP Local_Path your_utln@xfer.cluster.tufts.edu:Cluster_Path\n</code></pre>"},{"location":"hpc_user_guide/hpc-storage/","title":"Hpc storage","text":""},{"location":"hpc_user_guide/hpc-storage/#hpc-storage","title":"HPC Storage","text":""},{"location":"hpc_user_guide/hpc-storage/#home-directory","title":"Home Directory","text":"<ul> <li>Your Home Directory (10GB, fixed) should be <code>/cluster/home/your_utln</code></li> </ul>"},{"location":"hpc_user_guide/hpc-storage/#reserach-project-storage","title":"Reserach Project Storage","text":"<ul> <li>To Get Storage for a research project visit:</li> </ul> <p> Tufts Research Technology - High Performance Computing</p> <ul> <li>Where you will see the following options:</li> </ul> <p></p> <ul> <li>Your research projet storage (from 50GB and up) path should be <code>/cluster/tufts/yourlabname/</code></li> <li>Each member of the lab group has a dedicated directory:</li> </ul> <p><code>/cluster/tufts/yourlabname/your_utln</code></p> <ul> <li>To see your research project storage quota run the following command from any node on the new cluster Pax:</li> </ul> <p><code>df -h /cluster/tufts/yourlabname</code> </p> <p>Note</p> <p>Accessing your research project storage space for the first time, please make sure you type out the FULL PATH to the directory.</p> <ul> <li>If your group has existing HPC research project storage space set up, please use the same link to request access. </li> </ul>"},{"location":"hpc_user_guide/hpc_services/","title":"HPC Services","text":""},{"location":"hpc_user_guide/hpc_services/#hpc-for-classes","title":"HPC for Classes","text":"<ul> <li>Please use the following link to request resources for your class:</li> </ul> <p>Class Setup Request</p>"},{"location":"hpc_user_guide/hpc_services/#understanding-slurm","title":"Understanding SLURM","text":"<ul> <li>For more information on SLURM commands visit:</li> </ul> <p>SLURM Commands</p>"},{"location":"hpc_user_guide/hpc_services/#getting-help","title":"Getting Help","text":"<ul> <li>If you need any assistance with the Tufts HPC cluster reach out to tts-research@tufts.edu </li> </ul>"},{"location":"hpc_user_guide/interactive-session/","title":"Interactive Session","text":""},{"location":"hpc_user_guide/interactive-session/#interactive-session","title":"Interactive Session","text":"<ul> <li> <p>An interactive session is a way to temporarily grab resources on the Tufts HPC.</p> <ul> <li>Particularly good for debugging and working with software GUI. </li> </ul> </li> <li> <p>The following is the basic layout of the command to get an interactive session:</p> </li> </ul> <pre><code>srun [options] --pty [command]\n</code></pre> <p>What does this mean?</p> <ul> <li> <p>Command </p> <ul> <li>command to run an application, given the module is already loaded.</li> <li><code>bash</code> for a bash shell</li> </ul> </li> <li> <p>Options</p> <ul> <li>Pseudo terminal <code>--pty</code></li> <li>Partition <code>-p</code> <ul> <li>Default batch if not specified</li> </ul> </li> <li>You can start interactive sessions on any partition you have access to</li> <li>Time <code>-t</code> or <code>--time=</code><ul> <li>Default 15 minutes if not specified on non-\"interactive\" partition</li> </ul> </li> <li>Number of CPU cores <code>-n</code> <ul> <li>Default 1 if not specified</li> </ul> </li> <li>Memory <code>--mem=</code><ul> <li>Default 2GB if not specified</li> </ul> </li> <li>GPU <code>--gres=</code><ul> <li>Default none</li> </ul> </li> <li>X Window <code>--x11=first</code><ul> <li>Default none  </li> </ul> </li> </ul> </li> </ul>"},{"location":"hpc_user_guide/interactive-session/#example-of-getting-an-interactive-session","title":"Example of Getting an Interactive Session","text":"<pre><code>srun -p batch --time=1-2:10:00 -n 2 --mem=8g --pty bash\n</code></pre> <p>What does this mean?</p> <p>Starting an interactive session of bash shell on preempt partition with 2 CPU cores and 2GB of RAM, with X11 forwarding for 1 day, 2 hours, and 30 minutes (use <code>exit</code> to end session and release resources).</p> <p>You will have notice that your prompt changed from:</p> <pre><code>[your_utln@login-prod-01 ~]$\n</code></pre> <p>To:</p> <pre><code>[your_utln@c1cmp044 ~]$\n</code></pre> <ul> <li>This means you have been placed on a compute node!</li> </ul>"},{"location":"hpc_user_guide/job-status/","title":"Checking Job Status","text":""},{"location":"hpc_user_guide/job-status/#checking-job-status","title":"Checking Job Status","text":"<ul> <li>Checking your active jobs:</li> </ul> <pre><code>squeue -u $USER\n</code></pre> <p>output</p> <pre><code>   JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON) \n24063163     batch      job your_utln  R       0:17      1 c1cmp044 \n</code></pre> <pre><code>squeue -u your_utln\n</code></pre> <p>output</p> <pre><code>   JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON) \n24063163     batch      job your_utln  R       0:17      1 c1cmp044 \n</code></pre>"},{"location":"hpc_user_guide/job-status/#to-cancel-a-job","title":"To Cancel A Job","text":"<ul> <li>To cancel a specific job:</li> </ul> <pre><code>scancel JOBID\n</code></pre> <ul> <li>To cancel all of your jobs:</li> </ul> <pre><code>scancel -u $USER\n</code></pre> <pre><code>scancel -u your_utln\n</code></pre>"},{"location":"hpc_user_guide/job-status/#to-check-the-details-of-your-active-jobs","title":"To Check The Details Of Your Active Jobs","text":"<ul> <li>To check details of your active jobs (running \"R\" or pending \"PD\"):</li> </ul> <pre><code>scontrol show jobid -dd JOBID\n</code></pre> <pre><code>scontrol show jobid -dd 24063163\n</code></pre> <p>output</p> <pre><code>JobId=24063163 JobName=job\n   UserId=your_utln(31003) GroupId=your_utln(5343) MCS_label=N/A\n   Priority=12833 Nice=0 Account=normal QOS=normal\n   JobState=RUNNING Reason=None Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\n   DerivedExitCode=0:0\n   RunTime=00:01:31 TimeLimit=1-00:00:00 TimeMin=N/A\n   SubmitTime=2022-07-20T12:33:14 EligibleTime=2022-07-20T12:33:14\n   AccrueTime=2022-07-20T12:33:14\n   StartTime=2022-07-20T12:33:15 EndTime=2022-07-21T12:33:15 Deadline=N/A\n   PreemptEligibleTime=2022-07-20T12:33:15 PreemptTime=None\n   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2022-07-20T12:33:15\n   Partition=batch AllocNode:Sid=c1cmp044:27677\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=c1cmp044\n   BatchHost=c1cmp044\n   NumNodes=1 NumCPUs=2 NumTasks=2 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=2,mem=8G,node=1,billing=2\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   JOB_GRES=(null)\n     Nodes=c1cmp044 CPU_IDs=2-3 Mem=8192 GRES=\n   MinCPUsNode=1 MinMemoryNode=8G MinTmpDiskNode=0\n   Features=(null) DelayBoot=00:00:00\n   Reservation=bioworkshop\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/cluster/home/ymalon01/workshop/LS/sbatch.sh\n   WorkDir=/cluster/home/ymalon01/workshop/LS\n   StdErr=/cluster/home/ymalon01/workshop/LS/24063163.err\n   StdIn=/dev/null\n   StdOut=/cluster/home/ymalon01/workshop/LS/24063163.out\n   Power=\n   MailUser=ymalon01 MailType=NONE\n</code></pre>"},{"location":"hpc_user_guide/job-status/#to-check-the-details-of-your-finished-jobs","title":"To Check The Details Of Your Finished Jobs","text":"<ul> <li>Checking your finished jobs:</li> </ul> <p>You can no longer see these jobs in <code>squeue</code> command output.</p> <p>Tip</p> <p>Querying finished jobs helps users make better decisions on requesting resources for future jobs.</p> <p>Display job CPU and memory usage:</p> <pre><code>seff JOBID\n</code></pre> <p>output</p> <pre><code>seff JOBID\nJob ID: JOBID\nCluster: pax\nUser/Group: your_utln/your_utln\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 2\nCPU Utilized: 00:03:00\nCPU Efficiency: 50.00% of 00:06:00 core-walltime\nJob Wall-clock time: 00:03:00\nMemory Utilized: 54.16 MB\nMemory Efficiency: 0.66% of 8.00 GB\n</code></pre>"},{"location":"hpc_user_guide/job-status/#check-accounting-data-for-a-job","title":"Check Accounting Data for a Job","text":"<ul> <li>Display detailed job accounting data:</li> </ul> <pre><code>sacct --format=partition,state,time,start,end,elapsed,MaxRss,ReqMem,MaxVMSize,nnodes,ncpus,nodelist -j JOBID\n</code></pre> <p>output</p> <pre><code> Partition      State  Timelimit               Start                 End    Elapsed     MaxRSS     ReqMem  MaxVMSize   NNodes      NCPUS        NodeList \n---------- ---------- ---------- ------------------- ------------------- ---------- ---------- ---------- ---------- -------- ---------- --------------- batch  COMPLETED 1-00:00:00 2022-07-20T12:33:15 2022-07-20T12:36:15   00:03:00                   8Gn                   1          2        c1cmp044 COMPLETED            2022-07-20T12:33:15 2022-07-20T12:36:15   00:03:00     55464K        8Gn    198364K        1          2        c1cmp044 OUT_OF_ME+            2022-07-20T12:33:15 2022-07-20T12:36:15   00:03:00          0        8Gn    108052K        1          2        c1cmp044 </code></pre>"},{"location":"hpc_user_guide/job-status/#going-further","title":"Going Further","text":"<ul> <li>For more SLURM options check out:</li> </ul> <p>SLURM Workload Manager</p>"},{"location":"hpc_user_guide/navigate-to-cluster/","title":"Navigate To The Cluster","text":""},{"location":"hpc_user_guide/navigate-to-cluster/#prerequisites","title":"Prerequisites","text":"<ul> <li>Request an Research Computing Cluster account</li> <li>Connect to the VPN if off campus</li> </ul>"},{"location":"hpc_user_guide/navigate-to-cluster/#navigate-to-the-cluster","title":"Navigate to the Cluster","text":"<ul> <li>You can access the Tufts HPC in two ways, either the OnDemand website, or command line.</li> </ul>"},{"location":"hpc_user_guide/navigate-to-cluster/#the-ondemand-website","title":"The OnDemand Website:","text":"<p>OnDemand Website</p> <ul> <li>Log in with your tufts credentials</li> <li>Once you are logged in you'll notice a few navigation options:</li> </ul> <p></p> <p>To Access A Shell Terminal</p> <ul> <li>Click on Clusters &gt; Tufts HPC Shell Access</li> </ul> <p>To Access An Interactive App</p> <ul> <li>Click on Interactive Apps</li> <li>Select the App you are interested in using</li> <li>For more information on how to access an interactive app take a look at our tutorials on RStudio and JupyterLab:</li> </ul> <p>RStudio Interactive Session</p> <p>JupyterLab Interactive Session</p>"},{"location":"hpc_user_guide/navigate-to-cluster/#command-line","title":"Command Line:","text":"<ul> <li> <p>You can access the Tufts HPC Cluster via command line with:</p> <ul> <li>The Terminal app on a Mac or Linux machine</li> <li>PuTTy or Cygwin SSH or SecureCRT or other SSH clients on a Windows machine</li> </ul> </li> <li> <p>To Log in open one of the aformentioned apps and enter:</p> </li> </ul> <pre><code>ssh your_utln@login.pax.tufts.edu\n</code></pre> <ul> <li>Next log in with your Tufts Credentials</li> <li>At this point you are on a login node and should see something like the following:</li> </ul> <p>output</p> <pre><code>[your_utln@login-prod-02 ~]\n</code></pre> <ul> <li>While on a login node please do not run any programs. For this you will need to get compute resources. See the following to get compute resources: </li> </ul> <p>Compute Resources</p>"},{"location":"hpc_user_guide/new_cluster_storage/","title":"Cluster Storage Request","text":""},{"location":"hpc_user_guide/new_cluster_storage/#new-cluster-storage-request","title":"New Cluster Storage Request","text":"<p>Please do the following to submit an official request for new Cluster Storage for your research group.</p> <ol> <li>Use the following the link to request a cluster storage account:</li> </ol> <p>Cluster Storage Account Request</p> <ol> <li>Login with your Tufts credentials</li> <li>Fill out blank fields</li> <li>Select \"Research Storage Request\"</li> <li>Select \"Cluster (HPC)\"</li> <li>Select \"New\"</li> <li>Click next</li> <li>Fill out remaining information</li> <li>Click on Submit</li> </ol>"},{"location":"hpc_user_guide/new_cluster_storage/#questions","title":"Questions","text":"<p>If you have any questions about requesting a cluster storage account, please reach out to tts-research@tufts.edu</p>"},{"location":"hpc_user_guide/request-account/","title":"Request an Account","text":"<p>Requirements</p> <ul> <li>You must complete an online account request form and be approved to use the Tufts Research Cluster.</li> <li>Account requests require a valid Tufts Username and Tufts Password</li> <li>Guest and student accounts require faculty or researcher sponsorship</li> </ul>"},{"location":"hpc_user_guide/request-account/#get-started","title":"Get Started","text":"<ul> <li>To request a Research Computing Cluster account use the following link:</li> </ul> <p>Cluster Account Request</p> <ul> <li>Log in with your Tufts Username and Tufts Password.</li> <li>The form will auto-fill as much of your user information as possible. Double-check to make sure it\u2019s correct, selecting the down arrow to adjust the affiliation information if it\u2019s incorrect.</li> <li>Remove the example text in the Usage Information box and briefly describe your planned use of the Cluster.</li> <li>In the Type of Account field, select the Cluster. </li> <li>When finished, click, Submit Request.</li> </ul>"},{"location":"hpc_user_guide/request-account/#help-and-use-cases","title":"Help and Use Cases","text":"<ul> <li>Research Cluster FAQ</li> <li>See how some of your peers are making use of the Research Cluster and its resources.</li> </ul>"},{"location":"hpc_user_guide/software-cluster/","title":"Software On The Cluster","text":""},{"location":"hpc_user_guide/software-cluster/#modules","title":"Modules","text":"<ul> <li>A tool that simplifies shell initialization and lets users easily modify their environment during the session with modulefiles</li> <li>Each modulefile contains the information needed to configure the shell for an application. (PATH, LD_LIBRARY_PATH, CPATH, etc.)</li> <li>Modules are useful in managing different versions of applications. </li> <li>Modules can also be bundled into metamodules that will load an entire set of different applications (dependencies). </li> </ul> <p>Module  Commands</p> <ul> <li><code>module av</code> : to check all available modules</li> <li><code>module load</code> : to load a particular module</li> <li><code>module list</code> : to list modules that are loaded</li> <li><code>module purge</code> : purge any loaded modules</li> </ul>"},{"location":"hpc_user_guide/software-cluster/#check-all-available-modules","title":"Check All Available Modules","text":"<ul> <li>To check ALL available modules installed on the cluster:</li> </ul> <pre><code>module av\n</code></pre> <ul> <li>Upon login, environment <code>PATH</code> is set for the system to search executables:</li> </ul> <pre><code>echo $PATH\n</code></pre> <p>output</p> <pre><code>/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/your_utln/bin:/cluster/home/your_utln/.local/bin\n</code></pre> <ul> <li>For example, I would like to use <code>blast</code>, to check what versions of blast are available:</li> </ul> <pre><code>module av blast\n</code></pre> <p>output</p> <pre><code>---------------------- /opt/shared/Modules/modulefiles-rhel6 ----------------------\nblast/2.2.24 blast/2.2.31 blast/2.3.0  blast/2.8.1\n\n---------------------- /cluster/tufts/hpc/tools/module ----------------------------\nblast-plus/2.11.0\n</code></pre>"},{"location":"hpc_user_guide/software-cluster/#load-desired-module","title":"Load Desired Module","text":"<ul> <li>To load the version I would like to use, and use it:</li> </ul> <pre><code>module load blast-plus/2.11.0\n</code></pre>"},{"location":"hpc_user_guide/software-cluster/#check-which-modules-are-loaded","title":"Check Which Modules Are Loaded","text":"<ul> <li>To check which modules are loaded:</li> </ul> <pre><code>module list\n</code></pre> <p>output</p> <pre><code>Currently Loaded Modulefiles:\n    1) use.own     2) blast-plus/2.11.0\n</code></pre>"},{"location":"hpc_user_guide/software-cluster/#check-the-tool-version","title":"Check The Tool Version","text":"<ul> <li>To determine the tool version:</li> </ul> <pre><code>blastp -version\n</code></pre> <p>output</p> <pre><code>blastp: 2.11.0+\n Package: blast 2.11.0, build Aug 17 2021 06:29:22 \n</code></pre>"},{"location":"hpc_user_guide/software-cluster/#check-module-paths","title":"Check Module Paths","text":"<ul> <li>To determine the module paths:</li> </ul> <pre><code>which blastp\n</code></pre> <p>output</p> <pre><code>/cluster/tufts/hpc/tools/spack/linux-rhel7-ivybridge/gcc-9.3.0/blast-plus-2.11.0-ip4jcqabi3a2jscgusnkipvib6goy5mv/bin/blastp\n</code></pre> <pre><code>echo $PATH\n</code></pre> <p>output</p> <pre><code>/cluster/tufts/bio/tools/edirect:/cluster/tufts/hpc/tools/spack/linux-rhel7-ivybridge/gcc-9.3.0/blast-plus-2.11.0-ip4jcqabi3a2jscgusnkipvib6goy5mv/bin:/cluster/home/your_utln/.iraf/bin:/cluster/home/your_utln/.iraf/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/your_utln/bin:/cluster/home/your_utln/.local/bin \n</code></pre>"},{"location":"hpc_user_guide/software-cluster/#to-unload-modules","title":"To Unload Modules","text":"<ul> <li>To unload a loaded module:</li> </ul> <pre><code>module unload blast-plus/2.11.0\n</code></pre> <pre><code>echo $PATH\n</code></pre> <p>output</p> <pre><code>/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/cluster/home/your_utln/bin:/cluster/home/your_utln/.local/bin\n</code></pre> <ul> <li>To unload ALL of the loaded modules:</li> </ul> <pre><code>module purge\n</code></pre> <pre><code>module list\n</code></pre> <p>output</p> <pre><code>No Modulefiles Currently Loaded.\n</code></pre>"},{"location":"hpc_user_guide/what-is-the-cluster/","title":"What is the Cluster?","text":"<p>Before getting to the cluster it is worth discussing what a cluster is and some of the terminology. First, let's discuss the difference between a CPU and a GPU.</p>"},{"location":"hpc_user_guide/what-is-the-cluster/#cpu-central-processing-unit","title":"CPU -- Central Processing Unit","text":"<ul> <li>A CPU can never be fully replaced by a GPU</li> <li>Can be thought of as the taskmaster of the entire system, coordinating a wide range of general-purpose computing tasks</li> </ul>"},{"location":"hpc_user_guide/what-is-the-cluster/#gpu-graphics-processing-unit","title":"GPU -- Graphics Processing Unit","text":"<ul> <li>GPUs were originally designed to create images for computer graphics and video game consoles</li> <li>Performing a narrower range of more specialized tasks</li> </ul> <p>You'll notice that in the picture above the CPU is composed of a smaller unit, a core. A core is the computing unit in a CPU. You'll also note that the whole system (including CPUs, GPUs and Storage) is a single computer in the system called a node.</p> <p></p> <p>When a CPU performs some computation they use a storage hierarchy. This hierarchy places small/fast storage options close to the CPU and slower/larger options away from the CPU. These small/fast options are called memory/RAM while the slower/larger options are simply called storage.</p> <p></p> <p>Now that we now the components we can put together an image of what a computer cluster is. A computer cluster is a group of loosely or tightly connected computers that work together as a single system. A HPC (High Performance Compute) cluster is a computer cluster capable of performing computations at high speeds.</p> <p></p>"},{"location":"news/2022_news/","title":"2022","text":""},{"location":"news/2022_news/#december-2022","title":"December 2022","text":"<p>Welcome to the beta release of this tutorials page! Content is still under development, but we look forward to flushing this out more in 2023!</p> <p>2023 2022 </p>"},{"location":"news/2023_news/","title":"2023","text":""},{"location":"news/2023_news/#january-2023","title":"January 2023","text":"<p>TBD</p> <p>2023 2022 </p>"},{"location":"programming_languages/unix_r_python/","title":"Introduction","text":""},{"location":"programming_languages/unix_r_python/#unix-tutorials","title":"Unix Tutorials","text":"<ul> <li>Intro To Unix</li> </ul>"},{"location":"programming_languages/unix_r_python/#r-tutorials","title":"R Tutorials","text":"<ul> <li>Intro To R</li> </ul>"},{"location":"programming_languages/unix_r_python/#python-tutorials","title":"Python Tutorials","text":"<ul> <li>Intro To Python</li> </ul>"},{"location":"programming_languages/intro_r_studio/01_r-intro/","title":"Introduction To R OnDemand","text":""},{"location":"programming_languages/intro_r_studio/01_r-intro/#intro-to-rstudio-for-life-sciences","title":"Intro to RStudio For Life Sciences","text":"<p>October 26, 2022</p>"},{"location":"programming_languages/intro_r_studio/01_r-intro/#tts-research-technology-instructors","title":"TTS Research Technology Instructors","text":"<ul> <li>Jason Laird, Bioinformatics Scientist</li> <li>Christina Divoll, Research Instrumentation and HPC Specialist</li> </ul> <p>TTS Help</p> <p>If you'd like to contact Research Technology with questions regarding cluster and storage accounts at Tufts, feel free to reach out to us at</p> <p>tts-research@tufts.edu</p>"},{"location":"programming_languages/intro_r_studio/01_r-intro/#recording","title":"Recording","text":"<p>We will be recording this workshop and distributing among Tufts HPC users as a reference so please contact us if you have any questions about this. </p>"},{"location":"programming_languages/intro_r_studio/01_r-intro/#best-elist","title":"BEST Elist","text":"<p>We are also happy to mention that the Bioinformatics team within TTS Research Technology has an elist, sign up with this link best@elist.tufts.edu to find out about Bioinformatics Education, Software and Tools</p> <p>Find out about other Data Lab and Bioinformatics Workshops being offered this semester from this link.</p> <p>Bioinformatics Workshops</p> <p>If you have a question regarding bioinformatics workshops specifically, please reach out to </p> <p>bioinformatics-workshop-questions@elist.tufts.edu</p> <p>Acknowledgement</p> <p>We would like to thank Kyle Monahan, Christina Divoll, Adelaide Rhodes, Kayla Sansevere, and Uku Uustalu for their review of this content</p>"},{"location":"programming_languages/intro_r_studio/02_r-ondemand/","title":"Introduction To RStudio For Life Sciences","text":"<p>Prerequisites</p> <ul> <li>Request an account on the Tufts HPC Cluster</li> <li>Connect to the VPN or the Tufts Secure Network</li> </ul>"},{"location":"programming_languages/intro_r_studio/02_r-ondemand/#learning-objectives","title":"Learning objectives","text":"<p>Today we are going to learn about</p> <ul> <li>project organization</li> <li>R packages and how to access them on the tufts HPC</li> <li>working with variables and data frames</li> <li>visualizing data</li> <li>and finally writing a markdown report of our findings</li> </ul>"},{"location":"programming_languages/intro_r_studio/02_r-ondemand/#navigate-to-the-cluster","title":"Navigate To The Cluster","text":"<p>Once you have an account and are connected to the VPN/Tufts Network, navigate to the OnDemand Website and log in with your tufts credentials. Once you are logged in you'll notice a few navigation options:</p> <p></p> <p>Click on <code>Interactive Apps &gt; RStudio Pax</code> and you will see a form to fill out to request compute resources to use RStudio on the Tufts HPC cluster. We will fill out the form with the following entries:</p> <ul> <li><code>Number of hours</code> : <code>3</code></li> <li><code>Number of cores</code> : <code>1</code></li> <li><code>Amount of memory</code> : <code>4GB</code></li> <li><code>R version</code> : <code>4.0.0</code></li> <li><code>Reservation for class, training, workshop</code> : <code>Bioinformatics Workshops</code>---&gt; NOTE: This reservation closed on Nov 9, 2022, use Default if running through the materials after that date.</li> <li><code>Load Supporting Modules</code>: <code>boost/1.63.0-python3 java/1.8.0_60 gsl/2.6</code></li> </ul> <p>Click <code>Launch</code> and wait until your session is ready. Click <code>Connect To RStudio Server</code>, and you will notice a new window will pop up with RStudio. </p> Are you connected to RStudio? <ul> <li>Yes (put up a green check mark in zoom)</li> <li>No (raise hand in zoom)</li> </ul>"},{"location":"programming_languages/intro_r_studio/02_r-ondemand/#introduction-to-rstudio","title":"Introduction To RStudio","text":"<p>RStudio is what is known as an Integrated Development Environment or IDE. Here you can write scripts, run R code, use R packages, view plots, and manage projects. This pane is broken up into three panels:</p> <ul> <li>The Interactive R console/Terminal (left)</li> <li>Environment/History/Connections (upper right)</li> <li>Files/Plots/Packages/Help/Viewer (lower right)</li> </ul> <p></p>"},{"location":"programming_languages/intro_r_studio/02_r-ondemand/#project-management","title":"Project Management","text":"<p>Before we dive into R it is worth taking a moment to talk about project management. Often times data analysis is incremental and files build up over time resulting in messy directories:</p> <p></p> <p>Sifting through a non-organized file system can make it difficult to find files, share data/scripts, and identify different versions of scripts. To remedy this, It is reccomended to work within an R Project. Before we make this project, we should make sure you are in your home directory. To do this click on the three dots in the files tab:</p> <p></p> <p>Then enter in a ~ symbol to go home!</p> <p></p>"},{"location":"programming_languages/intro_r_studio/02_r-ondemand/#r-project","title":"R Project","text":"<p>To Create a new R project:</p> <ol> <li>Go to <code>File</code> &gt; <code>New Project</code></li> <li><code>New Directory</code></li> <li><code>New Project</code></li> <li>Create a name for your project (e.g. <code>R-Practice</code>)</li> <li><code>Create Project</code></li> </ol> <p>You will notice that your RStudio console switches to this project directory. When you log out of RStudio you can open this project again by clicking the <code>.Rproj</code> file in the project directory. </p> <p>Note</p> <p>The paths will be relative to this project directory as a safe guard against referencing data from outside sources. </p> Have you created the project? <ul> <li>Yes (put up a green check mark in zoom)</li> <li>No (raise hand in zoom)</li> </ul>"},{"location":"programming_languages/intro_r_studio/02_r-ondemand/#file-organization","title":"File Organization","text":"<ul> <li>You noticed now that you are inside your project folder</li> <li>Let's start by creating some folders to you organize our files</li> <li>In the files window click new folder and enter scripts</li> <li>Let's do this again to create a data folder and a results folder</li> </ul>"},{"location":"programming_languages/intro_r_studio/02_r-ondemand/#data-principles","title":"Data Principles","text":"<ul> <li>Treat data as read-only</li> <li>Store raw data separately from cleaned data if you do need to manipulate it</li> <li>Ensure scripts to clean data are kept in a separate <code>scripts</code> folder</li> <li>Treat reproducible results as disposable</li> </ul> <p>Tip</p> <p>Result files are good candidate files to cut if you are getting low on storage.</p>"},{"location":"programming_languages/intro_r_studio/02_r-ondemand/#getting-data","title":"Getting Data","text":"<ul> <li>Today we will be using a fake dataset assessing the taxa count on the mouse microbiome before and after antibiotic usage.</li> <li>To copy over this data we will use an R function called file.copy. </li> <li>A function takes some input and delivers an output. </li> <li>In this case we specify two inputs the location of our file and where we want to copy it to. </li> <li>The function's output is copying over this file. So let's try it copy over using the following commands:</li> </ul> <pre><code>file.copy(from=\"/cluster/tufts/bio/tools/training/intro-to-r/data/meta.tsv\", to=\"./data/\")\nfile.copy(from=\"/cluster/tufts/bio/tools/training/intro-to-r/data/meta2.tsv\", to=\"./data/\")\n</code></pre> <p>So here you'll note we copied over the file metadata.tsv to the data folder. Let's copy over our script:</p> <pre><code>file.copy(from=\"/cluster/tufts/bio/tools/training/intro-to-r/scripts/intro-to-r.Rmd\", to=\"./scripts\")\n</code></pre> <p>Here we copy over our script intro-to-r to the scripts folder.</p>"},{"location":"programming_languages/intro_r_studio/02_r-ondemand/#opening-the-script","title":"Opening the Script","text":"<p>Now let's start by opening our script. Go to scripts and then double click on intro-to-r.Rmd!</p>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/","title":"Data Structures","text":""},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#markdown-language","title":"Markdown Language","text":"<ul> <li>Way of writing HTML content without having to deal with HTML code</li> <li>At the top of the page you'll notice a header section</li> <li>this header section is defined by two sets of three dashes </li> <li>contains <ul> <li>the title of our markdown report</li> <li>the output format of our markdown report</li> </ul> </li> <li>In the body of the document headers can be specified by adding hastags before the text</li> <li>Lists can be specified by adding a dash or asterisk before the text</li> <li>for more information on markdown formatting visit:   https://www.markdownguide.org/basic-syntax/</li> <li>While we will be working with an R markdown document today you can also run code in an R script. To open an R script you can go to File &gt; New File &gt; R Script. </li> </ul> <p>NOTE: R scripts will end in \".R\", while R markdowns will end in \".Rmd\"</p>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#code-chunks","title":"Code chunks","text":"<ul> <li>code chunks can be included in our markdown document with two sets of three tick marks</li> <li>you'll notice in the brackets we add in, <code>r</code>, which indicates we are running R code</li> </ul> <p>Let's start with R by defining what is called a variable. We can run this chunk of code by clicking the play button in the corner of the code chunk:</p> <pre><code>num &lt;- 18\nnum\n</code></pre> <p>output</p> <pre><code>[1] 18\n</code></pre> <p>What did we do:</p> <ul> <li>assigned the value 18 to the word \"num\" </li> <li>assign value with  the \"&lt;-\" operator</li> <li>call the value of this variable with the word \"num\" </li> <li>NOTE: our variable appears in the environment window to the right.</li> <li>NOTE: when we ran the code chunk our console window shrank! This is because our output is appearing below the code chunk. We can always reopen it by clicking on it!</li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#variable-names","title":"Variable Names","text":"<ul> <li>variable names are case sensitive</li> <li>they can include any combination of:</li> <li>lower-case letters</li> <li>upper-case letters</li> <li>underscores/periods/numbers (however, these cannot be the first character)</li> </ul> <pre><code>second.number.2 &lt;- 2\nthird_number_3 &lt;- 3\nfourthNumber4 &lt;- 4\n</code></pre> <p>Whate did we do:</p> <ul> <li>we assigned three numbers 2,3,4 to the variables second.number.2, third_number_3,fourthNumber4</li> <li>all are valid variable names</li> <li>keep you variable names as short as possible to still convey what they represent</li> <li>just be consistent with your naming convention</li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#variable-properties","title":"Variable Properties","text":"<ul> <li>When we define variables we can treat that variable name as the value itself</li> <li>We can also add variable names together</li> <li>We can assign more than one value to a variable name</li> </ul> <p>Let's try this out in code!</p> <pre><code># add 5 to num\nnum &lt;- num + 5\nnum\n\n# assign 20 to new num and add it to num\nnew_num &lt;- 20\nnew_num + num\n\n#create a variable with multiple values\ncombined &lt;- c(3,4,6)\ncombined\n</code></pre> <p>output</p> <pre><code>[1] 23\n\n[1] 43\n\n[1] 3 4 6\n</code></pre> <p>What did we do:</p> <ul> <li>First we added 5 to the variable <code>num</code></li> <li>We assigned that num + 5 back to the variable <code>num</code> which overwrote the original value of 18! (now it's 23)</li> <li>we assigned a new variable <code>new_num</code> to the value 20 and then showed we can add the values of <code>new_num</code> and <code>num</code> together with just their names</li> <li>we then assigned multiple values to the variable <code>combined</code> by separating values by commas and enclosing them in <code>c()</code></li> <li> <p>this variable with multiple values is called a vector!</p> </li> <li> <p>You'll also note we add text inside our code block by putting a hashtag in front of it. This is called a comment and they are very useful in giving your code context. </p> </li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#accessingmanipulating-values-in-a-vector","title":"Accessing/Manipulating Values in a Vector","text":"<ul> <li>Suppose we want to access one value in our vector <code>combined</code></li> <li>We can do this by specifying the value number in that vector.</li> <li>Let's try grabbing the second value in <code>combined</code></li> </ul> <pre><code># call the second value in combined\ncombined[2]\n\n# replace second value in combined\ncombined[2] &lt;- 10\ncombined[2]\n</code></pre> <p>output</p> <pre><code>[1] 4\n\n[1] 10\n</code></pre> <p>What did we do:</p> <ul> <li>grabbed the second value in <code>combined</code> by specifying the vector and then the number value we want in brackets</li> <li>vectors in R are one-indexed meaning that when you want the first value in a vector you use <code>[1]</code>, second value you would use <code>[2]</code> and so on</li> <li>we also replaced the second value of <code>combined</code> by calling <code>combined[2]</code> and reassigning it to <code>10</code></li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#libraries","title":"Libraries","text":"<ul> <li>R has a collection of base functions (we just used the file.copy() function!)</li> <li>However, there are thousands of other functions we can use by importing different libraries</li> <li>Tufts HPC has a collection of different libraries pre-installed we can use!</li> </ul> <p>Let's access that collection and import a library:</p> <pre><code>.libPaths(\"/cluster/tufts/hpc/tools/R/4.0.0/\")\nlibrary(tidyverse)\n</code></pre> <p>output</p> <pre><code>Registered S3 methods overwritten by 'dbplyr':\nmethod         from\nprint.tbl_lazy     \nprint.tbl_sql      \n\u2500\u2500 Attaching packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 1.3.0 \u2500\u2500\n\u2713 ggplot2 3.3.5     \u2713 purrr   0.3.4\n\u2713 tibble  3.1.6     \u2713 dplyr   1.0.8\n\u2713 tidyr   1.2.0     \u2713 stringr 1.4.0\n\u2713 readr   1.4.0     \u2713 forcats 0.5.1\n\u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts() \u2500\u2500\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n</code></pre> <p>What did we do:</p> <ul> <li>we used the .libPaths() function to point to where Tufts is keeping this collection of R packages</li> <li>after pointing to this location we can import a package!</li> <li>here we imported the tidyverse package using the library() function</li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#importing-data","title":"Importing Data","text":"<p>So what does this package do?  - the tidyverse package contains all sorts of functions to load, manipulate, and visualize data!</p> <p>Let's try use the read_delim() function to import some data:</p> <pre><code># load our data\nmeta &lt;- read_delim(file=\"../data/meta.tsv\",\n                   delim = \"\\t\")\n</code></pre> <p>What did we do:</p> <ul> <li>specified where our data  is</li> <li>it is one folder up (a.k.a. \"../\") and in the data folder (\"data/\")</li> <li>we specified the delimiter or the separator between our data</li> <li>here we say \"\\t\" to indicate our file is separated by tabs</li> <li>assign our data to the variable \"meta\"</li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#inspecting-data","title":"Inspecting Data","text":"<ul> <li>It is good practice to inspect your data before using it</li> <li>we can use the str() function to get a high level summary of our data :</li> </ul> <pre><code>str(object=meta)\n</code></pre> <p>output</p> <pre><code>spec_tbl_df [9 \u00d7 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ SampleID       : chr [1:9] \"sample 1\" \"sample 2\" \"sample 3\" \"sample 4\" ...\n $ AntibioticUsage: chr [1:9] \"None\" \"None\" \"None\" \"None\" ...     \n $ Day            : chr [1:9] \"Day0\" \"Day0\" \"Day0\" \"Day0\" ...     \n $ Organism       : chr [1:9] \"mouse\" \"mouse\" \"mouse\" \"mouse\" ...    \n $ TaxaCount      : num [1:9] 1174 1474 1492 1451 314 ...    \n - attr(*, \"spec\")=     \n  .. cols(     \n  ..   SampleID = col_character(),  \n  ..   AntibioticUsage = col_character(),\n  ..   Day = col_character(),\n  ..   Organism = col_character(),\n  ..   TaxaCount = col_double()\n  .. )\n</code></pre> <p>What did we do:</p> <ul> <li>we input our variable \"meta\" into the str() function which takes some <code>object</code>, here specify that <code>object</code> is our variable <code>meta</code></li> <li>Our output indicates a few things:</li> <li>the dimensions of our data (9 rows by 5 columns)</li> <li>our data is a table/data.frame</li> <li>the names of our columns (SampleID, AntibioticUsage, etc.)</li> <li>the data type of our columns (chr = character data, num = numeric data)</li> <li>how many values per column</li> <li>a preview of the first few values</li> </ul> <p>NOTE: If you want more information on R data types and how to convert between data types, visit: https://swcarpentry.github.io/r-novice-inflammation/13-supp-data-structures/</p> <p>To view the entire data frame, click on the variable in the environment window:   - here we can see the entire data frame and even search for values   - You'll note that our rows are different samples   - and our columns are different attributes about those samples</p>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#accessing-values-by-number","title":"Accessing Values By Number","text":"<ul> <li>We can access values in our data frame by specifying their row and column</li> <li>Let's try finding the value in the second row and the third column:</li> </ul> <pre><code>meta[[2,3]] # [[row,column]]\nmeta[[3]][2] # [[column]][row]\n</code></pre> <p>output</p> <pre><code>[1] \"Day0\"\n\n[1] \"Day0\"\n</code></pre> <p>What did we do:</p> <ul> <li>accessed our value using double brackets</li> <li>single brackets would subset our data frame instead of accessing our value</li> <li>we can either specify the row then column  inside the double brackets</li> <li>or specify our column in double brackets and then the second element in single brackets</li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#accessing-values-by-name","title":"Accessing Values By Name","text":"<ul> <li>But what if we don't have our index number? What if we wanted to determine the antibiotic usage of \"sample 5\"?</li> <li>Let's see how we can do this:</li> </ul> <pre><code>#data[[column name]]\nmeta[[\"AntibioticUsage\"]]\n</code></pre> <p>output</p> <pre><code>[1] \"None\"         \"None\"         \"None\"         \"None\"         \"Streptomycin\" \"Streptomycin\" \"Streptomycin\"\n\n[8] \"Streptomycin\" \"Streptomycin\"\n</code></pre> <pre><code># data[[column name]][data[[column name]]==pattern]\nmeta[[\"AntibioticUsage\"]][meta[[\"SampleID\"]]==\"sample 5\"] \n\n#data$ColumnName[data$ColumnName == pattern]\nmeta$AntibioticUsage[meta$SampleID==\"sample 5\"] \n</code></pre> <p>output</p> <pre><code>[1] \"Streptomycin\"\n\n[1] \"Streptomycin\"\n</code></pre> <p>What did we do:</p> <ul> <li>first we accessed our AntibioticUse column by calling our data frame, then in double brackets we reference our column name.</li> <li>we accessed our value by:</li> <li>specifying the column in double brackets</li> <li>we then use single brackets to select some value in that column</li> <li>we then specify a condition:<ul> <li>where the column \"SampleID\" is equal to \"sample 5\"</li> </ul> </li> <li>second we accessed our value by using the \"$\" operator</li> <li>when we are dealing with a data frame we can use the \"$\" operator to avoid having to write double brackets!</li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#comparison-operators","title":"Comparison Operators","text":"<ul> <li>You'll have noticed above we used a comparison operator</li> <li>We asked which value in the \"SampleID\" column was equal to \"sample 5\"</li> <li>Let's look at some other comparison operators:</li> <li><code>==</code> equals</li> <li><code>!=</code> does not equal</li> <li><code>&lt;</code> less than</li> <li><code>&gt;</code> greater than</li> <li><code>=&lt;</code> less than or equal to</li> <li><code>&gt;=</code> greater than or equal to</li> <li><code>%in%</code> is a value in another set of values</li> <li><code>&amp;</code> and</li> <li><code>|</code> or</li> <li>Let's try to use these operators to ask a few questions about our data:</li> <li>Do we have any samples with over 1000 different taxa?</li> <li>Is \"sample 8\" in our SampleID column?</li> </ul> <pre><code># first let's see if there are any samples with over 1000 different taxa\n# df$column_name1[df$column_name2&gt;threshold]\nmeta$SampleID[meta$TaxaCount&gt;1000]\n\n# now let's see if there is a \"sample 8\" in our SampleID column\n# pattern %in% df$column_name\n\"sample 8\" %in% meta$SampleID\n</code></pre> <p>output</p> <pre><code>[1] \"sample 1\" \"sample 2\" \"sample 3\" \"sample 4\"\n\n[1] TRUE\n</code></pre> <p>What did we do:</p> <ul> <li>To identify samples with over 1000 different taxa we:</li> <li>specified our SampleID column</li> <li>specified our condition column (here it is TaxaCount)</li> <li> <p>used the greater than operator and threshold to specify we only want to identify samples with a TaxaCount greater than 1000</p> </li> <li> <p>To identify if \"sample 8\" was in our Sample ID column we:</p> </li> <li>specifed our pattern (here it is \"sample 8\")</li> <li>specified our column of interest (SampleID)</li> <li>used the %in% operator to see if our pattern was in our column of interest</li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#applying-subsetting-to-data-frames","title":"Applying Subsetting To Data Frames","text":"<ul> <li>So far we have accessed individual values in a data frame. But what about filtering our data frame?</li> <li>Let's filter or subset our data frame into two data frames:</li> <li>one with just samples and their antibiotic usage</li> <li>another with samples on Day 5 of treatment</li> </ul> <pre><code># filter data frame for just samples and their antibiotic usage\n# df[c(\"column_name1\",\"column_name2\")]\nsamples_antibiotics &lt;- meta[,c(\"SampleID\",\"AntibioticUsage\")]\nhead(samples_antibiotics)\n</code></pre> <p>output</p> <pre><code>  SampleID AntibioticUsage\n  &lt;chr&gt;    &lt;chr&gt;          \n1 sample 1 None           \n2 sample 2 None           \n3 sample 3 None           \n4 sample 4 None           \n5 sample 5 Streptomycin   \n6 sample 6 Streptomycin \n</code></pre> <pre><code># filter data frame for just samples on Day 5 of treatment\n#df[df$column_name == pattern,]\nday_5 &lt;- meta[meta$Day == \"Day5\",]\nhead(day_5)\n</code></pre> <p>output</p> <pre><code>SampleID AntibioticUsage Day   Organism TaxaCount\n&lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;\n1 sample 5 Streptomycin    Day5  mouse          314\n2 sample 6 Streptomycin    Day5  mouse          189\n3 sample 7 Streptomycin    Day5  mouse          279\n4 sample 8 Streptomycin    Day5  mouse          175\n5 sample 9 Streptomycin    Day5  mouse          452\n</code></pre> <p>What did we do:</p> <ul> <li>To filter the data frame for just samples and their antibiotic usage:</li> <li>specified our data frame (meta)</li> <li>identified which columns we wanted to keep within <code>c()</code></li> <li>specified we are grabbing columns by placing our column names behind the comma</li> <li>saved this filtered data frame to <code>samples_antibiotics</code></li> <li>used the <code>head()</code> function to view the first 6 rows of our new data frame</li> <li>To filter the data frame to just samples on Day 5 of treatment:</li> <li>specified our data frame (<code>meta</code>)</li> <li>specified the column we intend to filter (<code>Day</code>)</li> <li>used <code>==</code> to filter for only values that are equal to \"Day5\"</li> <li>specified we are filtering rows by placing the comma after our pattern</li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#merging-data-frames","title":"Merging Data Frames","text":"<ul> <li>Often times you may want to merge in data from another data frame</li> <li>Let's see how to do this!</li> </ul> <pre><code># read in second meta data file\nmeta2 &lt;- read_delim(\"../data/meta2.tsv\",delim = \"\\t\")\nhead(meta2)\n</code></pre> <p>output</p> <pre><code>  SampleID   RBC\n  &lt;chr&gt;    &lt;dbl&gt;\n1 sample 1    12\n2 sample 2    17\n3 sample 3    14\n4 sample 4    16\n5 sample 5     3\n6 sample 6     7\n</code></pre> <pre><code># merge with existing meta data file\nmerged &lt;- inner_join(\n  x = meta,\n  y = meta2,\n  by = c(\"SampleID\")\n)\nhead(merged)\n</code></pre> <p>output</p> <pre><code>  SampleID AntibioticUsage Day   Organism TaxaCount   RBC  \n  &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;   \n1 sample 1 None            Day0  mouse         1174    12\n2 sample 2 None            Day0  mouse         1474    17 \n3 sample 3 None            Day0  mouse         1492    14 \n4 sample 4 None            Day0  mouse         1451    16\n5 sample 5 Streptomycin    Day5  mouse          314     3\n6 sample 6 Streptomycin    Day5  mouse          189     7 \n</code></pre> <p>What did we do:</p> <ul> <li>we read in another data frame from our <code>data</code> folder and named this data frame <code>meta2</code></li> <li>we then previewed this data frame to see that we have our SampleID column and a new column <code>RBC</code></li> <li>we then use the inner_join function to merge the two data frames, which takes:</li> <li><code>x</code>  data frame 1</li> <li><code>y</code> data frame 2,</li> <li><code>by</code> the column to merge on in both data frames</li> <li>we then use the <code>head()</code> command to preview our merged data frame</li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#adding-columns","title":"Adding Columns","text":"<ul> <li>Sometimes you may want to create columns in your data frame based on data in your existing data frame:</li> </ul> <pre><code># add column based on data on data\nmerged$RBC_Status &lt;- ifelse(\n  test = merged$RBC &gt; 13,\n  yes = \"High RBC Count\",\n  no = \"Low RBC Count\"\n)\nhead(merged)\n</code></pre> <p>output</p> <pre><code>  SampleID AntibioticUsage Day   Organism TaxaCount   RBC RBC_Status    \n  &lt;chr&gt;    &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         \n1 sample 1 None            Day0  mouse         1174    12 Low RBC Count \n2 sample 2 None            Day0  mouse         1474    17 High RBC Count\n3 sample 3 None            Day0  mouse         1492    14 High RBC Count\n4 sample 4 None            Day0  mouse         1451    16 High RBC Count\n5 sample 5 Streptomycin    Day5  mouse          314     3 Low RBC Count \n6 sample 6 Streptomycin    Day5  mouse          189     7 Low RBC Count   \n</code></pre> <p>What did we do:</p> <ul> <li>we added a new column by specifying the name of our data frame, <code>merged</code> and then the new column name after the <code>$</code> symbol</li> <li>used the <code>ifelse()</code> function to add different values based on some <code>test</code></li> <li>here our test was to see if the value in the <code>RBC</code> column was over 13</li> <li>if the answer was <code>yes</code>, it was over 13, then we input the value \"High RBC Count\"</li> <li>if the answer was <code>no</code>, it was under 13, then we input the value \"Low RBC Count\"</li> <li>we again use the <code>head()</code> function to preview our updated data frame</li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#creating-a-factor","title":"Creating a Factor","text":"<ul> <li>We have two data types in our data frame, character values, and numeric values</li> <li>Sometimes a character value will have an order to it (i.e. low, medium, high) </li> <li>In R when you provide an order to a character variable it is a factor data type</li> <li>Let's make our RBC Status column a factor specifying the order should be Low then High RBC count</li> </ul> <pre><code># make the day column a factor\nmerged$RBC_Status &lt;- factor(\n  merged$RBC_Status,\n  levels = c(\n    \"Low RBC Count\",\n    \"High RBC Count\"\n  )\n)\nmerged$RBC_Status\n</code></pre> <p>output</p> <pre><code>[1] Low RBC Count  High RBC Count High RBC Count High RBC Count Low RBC Count  Low RBC Count  Low RBC Count \n[8] Low RBC Count  Low RBC Count \nLevels: Low RBC Count High RBC Count\n</code></pre>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#visualizing-data","title":"Visualizing Data","text":"<ul> <li>Now for the fun part of R: data visualization!</li> <li>There are a few different ways to plot in R, but today we will show you how to plot using the <code>ggplot2</code> package as it is widely popular among R users.</li> <li>NOTE: <code>ggplot2</code> is a part of the <code>tidyverse</code> package that we already loaded so we don't need to load it again.</li> <li>Here we will plot:</li> <li>RBC counts versus Taxa Counts</li> <li>Antibiotic Usage versus Taxa Counts</li> </ul> <pre><code>rbc_v_taxa &lt;- ggplot(merged,                        # data to use\n                     aes(x=RBC,                     # x axis data\n                         y = TaxaCount,             # y axis data\n                         color=AntibioticUsage))+   # column to color data by\n  geom_point()+                                     # this plot is a scatterplot\n  theme_bw()+                                       # the theme is theme_bw()\n  labs(                                             \n    x=\"RBC Counts\",                                 # x axis title\n    y=\"Taxa Counts\",                                # y axis title\n    color=\"Antibiotic Usage\",                       # legend title\n    title=\"RBC Counts v. Taxa Counts\"               # figure title\n  )\nrbc_v_taxa\n</code></pre> <p>What did we we do:</p> <ul> <li>Created a scatter plot where:</li> <li>we used the <code>ggplot()</code> function to specify our data, and inside this function we used the <code>aes()</code> function to specify which columns we wanted to plot (x axis being the <code>RBC</code> column and the y axis being the <code>TaxaCount</code> column)</li> <li>inside the <code>aes()</code> function we specified the <code>color</code> argument to indicate we want to color by the column <code>Antibiotic Usage</code></li> <li>we used the <code>geom_point()</code> function to specify this is a scatter plot</li> <li>we used the <code>theme_bw()</code> function to style this plot using the <code>theme_bw()</code> style</li> <li>we used the <code>labs()</code> function to specify our X axis title, y axis title, legend title and figure title</li> <li>we then saved this figure to the variable <code>rbc_v_taxa</code></li> <li>For more information on plotting with ggplot visit:</li> </ul> <p>http://www.sthda.com/english/wiki/be-awesome-in-ggplot2-a-practical-guide-to-be-highly-effective-r-software-and-data-visualization</p>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#saving-plotsdata","title":"Saving Plots/Data","text":"<ul> <li>Now that we have created all this wonderful data and plots we should learn how to save them!</li> </ul> <pre><code># to save new data frame\nwrite_delim(x = merged,\n            file = \"../results/merged.tsv\",\n            delim = \"\\t\")\n\n# to save our plot\nggsave(filename = \"../results/rbc_v_taxa.png\",\n       plot = rbc_v_taxa)\n</code></pre> <p>What did we do:</p> <ul> <li>To save our new merged data frame:</li> <li>we used the <code>write_delim</code> function</li> <li>specified our data frame, or <code>x</code> argument, to be the variable <code>merged</code></li> <li>we said we wanted to save our <code>file</code> one folder up \"../\" in the results folder, \"results/\" as \"merged.tsv\"</li> <li> <p>we also noted that our file should be separated or delimited, <code>delim</code>, by tabs <code>\\t</code></p> </li> <li> <p>To save our plot:</p> </li> <li>we used the <code>ggsave</code> function</li> <li>we said we wanted to save our file (<code>filename</code>) one folder up \"../\" in the results folder, \"results/\" as \"rbc_v_taxa.png\"</li> <li>specified our plot, <code>plot</code>, to be the variable <code>rbc_v_taxa</code></li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#getting-help","title":"Getting Help","text":"<ul> <li>Sometimes we won't know what every function does. </li> <li>Let's investigate the <code>aes()</code> function we just used to create our plot!</li> </ul> <pre><code>?aes\n</code></pre> <p>What did we do:</p> <ul> <li>To investigate the <code>aes()</code> function we:</li> <li>put a <code>?</code> in front of the function of interest. </li> <li>then in the help window we see a description of the function and examples on how to use it!</li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#creating-the-markdown-report","title":"Creating the Markdown Report","text":"<ul> <li>Now this combination of text and code can be \"knitted\" into a report of our choice. </li> <li>Today we will be creating an HTML page of our results. </li> <li>For a full list of R markdown output options visit:</li> </ul> <p>https://rmarkdown.rstudio.com/lesson-9.html</p> <ul> <li>To create our output file go to the top of the script window and click \"Knit\"!</li> </ul> <p>Thanks for taking part in the Intro To R for the Life Sciences Tutorial!</p> <p>So as a summary we learned about:</p> <ul> <li>project organization</li> <li>R packages and how to access them on the tufts HPC</li> <li>working with variables and data frames</li> <li>visualizing data</li> <li>and finally writing a markdown report of our findings</li> </ul>"},{"location":"programming_languages/intro_r_studio/03_r-workshop-markdown/#references","title":"References","text":"<ol> <li>A Gentle Introduction to R</li> <li>R for Reproducible Scientific Analysis</li> <li>Markdown Syntax</li> <li>Programming With R</li> <li>Be Awesome in ggplot2: A Practical Guide to be Highly Effective - R software and data visualization</li> <li>Output Formats</li> </ol>"},{"location":"programming_languages/intro_to_python/01_python-ondemand/","title":"Introduction To Python OnDemand","text":""},{"location":"programming_languages/intro_to_python/01_python-ondemand/#setup","title":"Setup","text":"<p>Before getting started you will need:</p> <ul> <li>Account on Tufts HPC</li> <li>VPN if accessing the HPC from off campus</li> </ul>"},{"location":"programming_languages/intro_to_python/01_python-ondemand/#navigate-to-the-cluster","title":"Navigate To The Cluster","text":"<p>Once you have an account and are connected to the VPN/Tufts Network, navigate to the OnDemand Website and log in with your tufts credentials. Once you are logged in you'll notice a few navigation options:</p> <p></p> <p>Click on <code>Interactive Apps &gt; JupyterLab</code> and you will see a form to fill out to request compute resources to use JupyterLab on the Tufts HPC cluster. We will fill out the form with the following entries:</p> <ul> <li><code>Number of hours</code> : <code>3</code></li> <li><code>Number of cores</code> : <code>1</code></li> <li><code>Amount of memory</code> : <code>32GB</code></li> <li><code>R version</code> : <code>4.0.0</code></li> <li><code>Reservation for class, training, workshop</code> : <code>Default</code></li> <li><code>Load Supporting Modules</code>: <code>curl/7.47.1 gcc/7.3.0 hdf5/1.10.4 boost/1.63.0-python3 libpng/1.6.37 java/1.8.0_60 libxml2/2.9.10 libiconv/1.16 fftw/3.3.2 gsl/2.6</code></li> </ul> <p>Click <code>Lauch</code> and wait until your session is ready. Click <code>Connect To JupyterLab Server</code>, and you will notice a new window will pop up with JupyterLab. </p>"},{"location":"programming_languages/intro_to_python/01_python-ondemand/#introduction-to-jupyterlab","title":"Introduction to JupyterLab","text":"<p>Jupyterlab is a web-based user interface to run Python code and is not a traditional Integrated Development Environment (IDE) where you create scripts via some text editor and then submit directly to command line. JupyterLab has several advantages, including being able to run code in chunks, annotating code with links, and displaying figures right next to code! For this reason, JupyterLab is a robust tool for script development/data analysis. When you open JupyterLab you will notice:</p> <ul> <li>Left Sidebar: containing your file browser, list of running kernels/terminals, table of contents, extension manager</li> <li>Main Work Area: containing options for file/window types to open (ipykernels, terminal environments, text files, markdown files, and python files)</li> </ul> <p></p> <p>We are going to start by opening up a <code>.ipynb</code> file by clicking <code>Notebook Python 3 (ipykernel)</code>. These are not python scripts, but notebook files that contain code but also text, links and images. These files can easily be converted to a python script (file ending in <code>.py</code>) by going to:</p> <ul> <li><code>File</code></li> <li><code>Download as</code></li> <li><code>Python (.py)</code></li> </ul> <p>For now let's work in the Jupyter notebook (<code>.ipynb</code> file)!</p>"},{"location":"programming_languages/intro_to_python/01_python-ondemand/#code-vs-markdown","title":"Code Vs. Markdown","text":"<p>You will notice when you open up your notebook that you are working in blocks:</p> <p></p> <p>These blocks can either be:</p> <ul> <li>raw blocks: raw data that can be converted into HTML/Latex formats</li> <li>code blocks: python code that can be run in chunks</li> <li>markdown blocks: a plain text format that can render links, lists, and images like what you might find on a website</li> </ul> <p>Here we will focus on code blocks to run chunks of python code, and markdown blocks which can add in images, links, etc. to annotate our code.</p>"},{"location":"programming_languages/intro_to_python/01_python-ondemand/#markdown-basics","title":"Markdown Basics","text":"<p>markdown code:</p> <p><pre><code>- list item 1\n- list item 2\n</code></pre> output: - list item 1 - list item 2</p> <p>markdown code:</p> <p><pre><code>1. numbered list item 1\n2. numbered list item 2\n</code></pre> output: 1. numbered list item 1 2. numbered list item 2</p> <p>markdown code:</p> <p><pre><code># Level 1 Heading\n## Level 2 Heading\n</code></pre> output:</p>"},{"location":"programming_languages/intro_to_python/01_python-ondemand/#level-1-heading","title":"Level 1 Heading","text":""},{"location":"programming_languages/intro_to_python/01_python-ondemand/#level-2-heading","title":"Level 2 Heading","text":"<p>markdown code:</p> <p><pre><code>[google link](https://www.google.com/)\n</code></pre> output: google link</p> <p>Now that we have a basic understanding of markdown, let's create some annotations. In your first code block change the type to markdown and enter:</p> <pre><code># Introduction to Python \n\nHere are a few helpful links to get started:\n\n- [Python Cheatsheet](https://www.pythoncheatsheet.org/cheatsheet/basics)\n- [JupyterLab Documentation](https://jupyterlab.readthedocs.io/en/stable/)\n</code></pre> <p>Now hit either the play button at the top of the screen or hit <code>Shift + Enter</code> to run the block:</p> <p></p>"},{"location":"programming_languages/intro_to_python/02_variables-data-types/","title":"Variables & Data Types","text":""},{"location":"programming_languages/intro_to_python/02_variables-data-types/#variables","title":"Variables","text":"<p>In Python, we store values using names called variables. We can assign a variable with an <code>=</code> sign:</p> <p><pre><code>max_coverage = 6000\nminCoverage = 35\nantibiotic = 'Streptomycin'\nantibiotic2 = 'Penicillin'\n</code></pre> You will note a few things about variables:</p> <ul> <li>can incorporate letters, digits and underscores</li> <li>cannot start with a digit</li> <li>these are case sensitive</li> </ul> <p>Variables, once we assign them to some value, can be passed into functions to accomplish certain tasks. Functions, generally speaking, take in some input and spit out some output. Let's use the simplist use case, the <code>print()</code> function:</p> <p><pre><code>print('The maximum coverage is ', max_coverage)\n</code></pre> <pre><code>The maximum coverage is  6000\n</code></pre></p> <p>Here the function <code>print()</code> took in two character values and printed a combined string of words.</p> <p>Note</p> <p>variables are available to use between blocks. However, the order in which you run blocks matters so make sure to run your code blocks in order!</p>"},{"location":"programming_languages/intro_to_python/02_variables-data-types/#data-types","title":"Data Types","text":"<ul> <li><code>integer</code>: a positive/negative whole number (34, -675)</li> <li><code>float</code>: a floating point number (4.67, -2034.67)</li> <li><code>string</code>: a character string written with either single or double quotes ('Streptomycin', \"antibiotic\")</li> <li><code>bool</code>: a TRUE/FALSE value</li> </ul> <p>So you have a variable, how do you determine the type? Well we can use the <code>type()</code> function:</p> <pre><code>type(max_coverage)\n</code></pre> <pre><code>int\n</code></pre> <p>If you want to convert between data types you can specify with the following functions:</p> <ul> <li><code>int()</code>: to convert to an integer</li> <li><code>float()</code>: to convert to a floating point number</li> <li><code>str()</code>: to convert to a string</li> </ul>"},{"location":"programming_languages/intro_to_python/02_variables-data-types/#calculations","title":"Calculations","text":"<p>You can use Python like a calculator using the following symbols:</p> Operator Name Example + Addition x + y - Subtraction x - y * Multiplication x * y / Division x / y % Modulus x % y ** Exponentiation x ** y // Floor division x // y <p>Let's try an few example:</p> <pre><code>35 / 7 - 5 + 4 * 4 + 2**2\n</code></pre> <pre><code>20.0\n</code></pre> <p>We note that Python calculations follow the order of operations when performing a calculation. We should also bring up two non-standard operations that you may or may not be familiar with: Modulus and Floor division. Modulus is the remainder after division so:</p> <pre><code>7 % 2\n</code></pre> <pre><code>1\n</code></pre> <p>Floor division is a division operation for which you round the result down to a whole number:</p> <pre><code>7 // 2\n</code></pre> <pre><code>3\n</code></pre>"},{"location":"programming_languages/intro_to_python/02_variables-data-types/#strings-operators","title":"Strings &amp; Operators","text":"<p>You can use <code>+</code> and <code>*</code> with string data as well to add and multiply, take for instance:</p> <pre><code>antibiotic + antibiotic\n</code></pre> <pre><code>'StreptomycinStreptomycin'\n</code></pre> <pre><code>antibiotic * 4\n</code></pre> <pre><code>'StreptomycinStreptomycinStreptomycinStreptomycin'\n</code></pre>"},{"location":"programming_languages/intro_to_python/02_variables-data-types/#indexing","title":"Indexing","text":"<p>Unlike the other data types, strings have lengths. We can use the <code>len()</code> function to  check how long  a string is:</p> <pre><code>print(antibiotic)\nlen(antibiotic)\n</code></pre> <pre><code>'Streptomycin'\n12\n</code></pre> <p>We can slice strings if needed to! However, the letters you are grabbing are zero-indexed meaning that the first letter is letter 0, the second letter is letter 1, and so on:</p> <pre><code>antibiotic[0]\n</code></pre> <pre><code>'S'\n</code></pre> <pre><code>antibiotic[1]\n</code></pre> <pre><code>'t'\n</code></pre> <p>We can grab more letters using the format <code>[start:stop]</code>:</p> <pre><code>antibiotic[1:5]\n</code></pre> <pre><code>'trep'\n</code></pre>"},{"location":"programming_languages/intro_to_python/02_variables-data-types/#comments","title":"Comments","text":"<p>When assigning variables we can add descriptions to our code to give our code context. We do this by writing our description after a <code>#</code> symbol:</p> <pre><code># creating a variable for time of day\ntime_of_day = 'Morning'\n</code></pre> <p>Everything after the <code>#</code> is not processed as Python code even within a code block in a Jupyter notebook.</p>"},{"location":"programming_languages/intro_to_python/03_libraries-data-frames/","title":"Libraries","text":"<p>Libraries are collections of functions called modules that can be imported and used in your script. Let's use the <code>math</code> library to grab constants:</p> <pre><code>import math\nmath.e\n</code></pre> <pre><code>2.718281828459045\n</code></pre> <p>Now how about functions:</p> <pre><code>math.log2(25)\n</code></pre> <p><pre><code>4.643856189774724\n</code></pre> We want to point out that here we call the value <code>e</code> after <code>math</code>. This is called a member value. On the contrary we see that <code>.log2()</code> comes after <code>math</code>. The parentheses make this a method or function in a given package. </p> <p>Tip</p> <p>If you ever need assistance with a library, try using the <code>help()</code> function to grab more information (e.g. <code>help(math)</code>).</p>"},{"location":"programming_languages/intro_to_python/03_libraries-data-frames/#importing-parts-of-libraries-using-aliases","title":"Importing Parts of Libraries &amp; Using Aliases","text":"<p>Sometimes you'll only need a few things from a library. To grab just those few things use the following approach:</p> <pre><code>from math import log2, e\nmath.log2(25)\n</code></pre> <pre><code>4.643856189774724\n</code></pre> <p>Now sometimes the name of a library is just too long to continuously type out. For this we can use an alias</p> <pre><code>from math import log2 as l2\nmath.l2(25)\n</code></pre> <pre><code>4.643856189774724\n</code></pre> <p>Here we abbreviate <code>log2</code> from the <code>math</code> package to <code>l2</code>.</p>"},{"location":"programming_languages/intro_to_python/03_libraries-data-frames/#importing-and-inspecting-data-frames","title":"Importing and Inspecting Data Frames","text":"<p>In data analysis we often work with tabular data, or two dimensional data with columns and rows. Columns will typically contain the same type of data and rows will be one sample with different observations. We commonly read in tabular data using the <code>pandas</code> module:</p> <pre><code>import pandas as pd\nimport csv\ndf = pd.read_csv('/cluster/tufts/bio/tools/training/intro-to-r/metadata.tsv' , sep = '/t', engine = 'python')\nprint(df)\n</code></pre> <pre><code>    SampleID AntibioticUsage DaySinceExperimentStart Genotype                 Description OtuCount\n1   WT.unt.1            None                    DAY0       WT   16S_WT_unt_1_SRR2627457_1     1174\n2   WT.unt.2            None                    DAY0       WT   16S_WT_unt_2_SRR2627461_1     1474\n3   WT.unt.3            None                    DAY0       WT   16S_WT_unt_3_SRR2627463_1     1492\n4   WT.unt.7            None                    DAY0       WT   16S_WT_unt_7_SRR2627465_1     1451\n5 WT.day3.11    Streptomycin                    DAY3       WT 16S_WT_day3_11_SRR2628505_1      314\n6 WT.day3.13    Streptomycin                    DAY3       WT 16S_WT_day3_13_SRR2628506_1      189\n7 WT.day3.15    Streptomycin                    DAY3       WT 16S_WT_day3_15_SRR2628507_1      279\n8 WT.day3.14    Streptomycin                    DAY3       WT 16S_WT_day3_14_SRR2627471_1      175\n9  WT.day3.9    Streptomycin                    DAY3       WT  16S_WT_day3_9_SRR2628504_1      452\n</code></pre> <p>If we want to inspect this data frame we can use a few useful commands. To get a quick summary of the data frame we can use:</p> <pre><code>df.info() # reveals that we have 6 columns, 9 rows, uses 504.0+ bytes of memory, and has one integer column\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 9 entries, 1 to 9\nData columns (total 6 columns):\n #   Column                   Non-Null Count  Dtype \n---  ------                   --------------  ----- \n 0   SampleID                 9 non-null      object\n 1   AntibioticUsage          9 non-null      object\n 2   DaySinceExperimentStart  9 non-null      object\n 3   Genotype                 9 non-null      object\n 4   Description              9 non-null      object\n 5   OtuCount                 9 non-null      int64 \ndtypes: int64(1), object(5)\nmemory usage: 504.0+ bytes\n</code></pre> <p>To get column names:</p> <pre><code>df.columns\n</code></pre> <pre><code>Index(['SampleID', 'AntibioticUsage', 'DaySinceExperimentStart', 'Genotype',\n       'Description', 'OtuCount'],\n      dtype='object')\n</code></pre> <p>To get row names:</p> <pre><code>df.index\n</code></pre> <pre><code>Int64Index([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype='int64')\n</code></pre> <p>To transpose (flip the columns and rows) the data frame:</p> <pre><code>df.T\n</code></pre> <pre><code>                                                 1                          2  \nSampleID                                  WT.unt.1                   WT.unt.2   \nAntibioticUsage                               None                       None   \nDaySinceExperimentStart                       DAY0                       DAY0   \nGenotype                                        WT                         WT   \nDescription              16S_WT_unt_1_SRR2627457_1  16S_WT_unt_2_SRR2627461_1   \nOtuCount                                      1174                       1474   \n</code></pre> <p>If we wanted a numeric summary we can use:</p> <pre><code>df.describe()\n</code></pre> <pre><code>          OtuCount\ncount     9.000000\nmean    777.777778\nstd     600.526806\nmin     175.000000\n25%     279.000000\n50%     452.000000\n75%    1451.000000\nmax    1492.000000\n</code></pre> <p>Note</p> <p>the <code>.describe()</code> method will only summarizes numeric data. So here we only have one column of numeric data that get's summarized.</p>"},{"location":"programming_languages/intro_to_python/03_libraries-data-frames/#data-manipulation","title":"Data Manipulation","text":"<p>Say you want to grab certain values in a data frame using the number location. So the value in the second row and third column:</p> <pre><code>df.iloc[2,3]\n</code></pre> <p><pre><code>'WT'\n</code></pre> Here we see that the formula to grab values is <code>[row, column]</code>. If we wanted to use row/column names to specify the value:</p> <pre><code>df.loc[1,\"OtuCount\"]\n</code></pre> <pre><code>1174\n</code></pre> <p>To grab all values in a row or column we use <code>:</code> to specify every value:</p> <pre><code>df.loc[:,\"OtuCount\"]\n</code></pre> <pre><code>1    1174\n2    1474\n3    1492\n4    1451\n5     314\n6     189\n7     279\n8     175\n9     452\nName: OtuCount, dtype: int64\n</code></pre> <p>We can also subset our data with a few operators:</p> Operator Description &gt; greater than &gt;= greater than or equal &lt; less than &lt;= less than or equal == equals != not equal &amp; and | or <p>Let's go through a few of these:</p> <pre><code>df[df[\"AntibioticUsage\"] == \"None\"]    # select samples with no antibiotic useage\n</code></pre> <pre><code>SampleID    AntibioticUsage DaySinceExperimentStart Genotype    Description OtuCount\n1   WT.unt.1    None    DAY0    WT  16S_WT_unt_1_SRR2627457_1   1174\n2   WT.unt.2    None    DAY0    WT  16S_WT_unt_2_SRR2627461_1   1474\n3   WT.unt.3    None    DAY0    WT  16S_WT_unt_3_SRR2627463_1   1492\n4   WT.unt.7    None    DAY0    WT  16S_WT_unt_7_SRR2627465_1   1451\n</code></pre> <pre><code>df[df[\"OtuCount\"] &gt; 400]   # select samples with an otu count over 400\n</code></pre> <pre><code>SampleID    AntibioticUsage DaySinceExperimentStart Genotype    Description OtuCount\n1   WT.unt.1    None    DAY0    WT  16S_WT_unt_1_SRR2627457_1   1174\n2   WT.unt.2    None    DAY0    WT  16S_WT_unt_2_SRR2627461_1   1474\n3   WT.unt.3    None    DAY0    WT  16S_WT_unt_3_SRR2627463_1   1492\n4   WT.unt.7    None    DAY0    WT  16S_WT_unt_7_SRR2627465_1   1451\n9   WT.day3.9   Streptomycin    DAY3    WT  16S_WT_day3_9_SRR2628504_1  452\n</code></pre>"},{"location":"programming_languages/intro_to_python/04_plotting-plotly/","title":"Plotting with Plotly","text":""},{"location":"programming_languages/intro_to_python/04_plotting-plotly/#plotting-with-plotly","title":"Plotting with Plotly","text":"<p>While there are other plotting libraries, we will focus on <code>plotly</code> for the following reasons:</p> <ul> <li>has the ability to zoom </li> <li>images can be downloaded as <code>png</code> files</li> <li>select features can highlight features of the plot</li> </ul>"},{"location":"programming_languages/intro_to_python/04_plotting-plotly/#basic-plot","title":"Basic Plot","text":"<p>Let's make a scatterplot:</p> <pre><code>import plotly.express as px\nfig = px.scatter(df,                      # the data we are using\n                 x=\"Day\",                 # x axis data\n                 y=\"OtuCount\",            # y axis data\n                 color='Day',             # how to color our data\n                 template=\"simple_white\") # what theme we would like\nfig.show()\n</code></pre> <p></p>"},{"location":"programming_languages/intro_to_python/04_plotting-plotly/#adding-a-trendline","title":"Adding A TrendLine","text":"<p>We can add a trend line as well:</p> <pre><code>import plotly.express as px\nfig = px.scatter(df,\n                 x=\"Day\",\n                 y=\"OtuCount\",\n                 color='Day',\n                 template=\"simple_white\",\n                 trendline=\"ols\")         # add in a trend line\nfig.show()\n</code></pre> <p></p>"},{"location":"programming_languages/intro_to_python/04_plotting-plotly/#scaling","title":"Scaling","text":"<p>Now if one of your axes spans multiple magnitudes you can scale your data using the <code>log_x</code> or <code>log_y</code> arguements:</p> <pre><code>fig = px.scatter(df,                                   \n                 x=\"Day\",                              \n                 y=\"OtuCount\",                          \n                 color='Day',                           \n                 template=\"simple_white\",\n                 trendline=\"ols\",\n                 log_y = True)             # scale y axis\nfig.show()\n</code></pre> <p></p>"},{"location":"programming_languages/intro_to_python/04_plotting-plotly/#panels","title":"Panels","text":"<p>Sometimes it is useful to separate data by some variable and create panels. We can easily do this by specifying the <code>facet_row</code> or <code>facet_col</code> arguements - where plots are stacked one on top of the other or side-by-side, respectively:</p> <pre><code>fig = px.scatter(df,                                   \n                 x=\"Day\",                              \n                 y=\"OtuCount\",                          \n                 color='Day',                           \n                 template=\"simple_white\",\n                 facet_col = \"DaySinceExperimentStart\") # split plots by variable\nfig.show()\n</code></pre> <p></p>"},{"location":"programming_languages/intro_to_python/04_plotting-plotly/#modifying-text","title":"Modifying Text","text":"<p>To modify text we can use the <code>labels</code> and <code>title</code> option:</p> <pre><code>fig = px.scatter(df,                                   \n                 x=\"Day\",                              \n                 y=\"OtuCount\",                          \n                 color='Day',                           \n                 template=\"simple_white\",\n                 labels={                        \n                     \"OtuCount\": \"OTU count\"     # add in a space and capitalize\n                 },\n                 title = \"Figure 1\")             # add in figure title\nfig.show()\n</code></pre> <p></p> <p>Tip</p> <p>For more plots and plot customization options, checkout the Plotly Graphing Library Page for more information</p>"},{"location":"programming_languages/intro_to_python/05_lists/","title":"Lists","text":"<p>A data frame is not the only way to store data, we can also create lists of values which can be the same data type or different data types. Here is an example:</p> <pre><code>coverage = [200, 34, 900, 423, 98, 789]\n</code></pre>"},{"location":"programming_languages/intro_to_python/05_lists/#grabbing-list-values","title":"Grabbing List Values","text":"<p>We can also grab these values by their index, which again are zero-indexed (meaning they start at zero). Here is an example of grabbing the 3rd item in the list:</p> <pre><code>coverage[2]\n</code></pre> <pre><code>900\n</code></pre>"},{"location":"programming_languages/intro_to_python/05_lists/#addingdeleting-values","title":"Adding/Deleting Values","text":"<p>To add values we can use the <code>.append()</code> method to add items to the end of a list:</p> <pre><code>coverage.append(542)\ncoverage\n</code></pre> <pre><code>[200, 34, 300, 423, 98, 789, 542]\n</code></pre> <p>Additionally, we can also remove items from a list as well with the <code>del</code> statement:</p> <pre><code>del coverage[3]\ncoverage\n</code></pre> <pre><code>[200, 34, 300, 98, 789, 542]\n</code></pre>"},{"location":"programming_languages/intro_to_python/06_loops-conditionals/","title":"Loops & Conditionals","text":""},{"location":"programming_languages/intro_to_python/06_loops-conditionals/#loops","title":"Loops","text":"<p>Loops perform some operation on a value in a set of values. Let's go through an example using our <code>coverage</code> list from the previous note:</p> <pre><code>for i in coverage:\n    print(i)\n</code></pre> <pre><code>200\n34\n300\n98\n789\n542\n</code></pre> <p>Here we see that <code>i</code> is a substitute for some value in the sequence provided - in this case 200, <code>34, 300, 98, 789, 542</code>. </p>"},{"location":"programming_languages/intro_to_python/06_loops-conditionals/#nested-loops","title":"Nested Loops","text":"<p>Loops can also nested where a loop is placed inside a loop:</p> <pre><code>for i in [1,2]:\n    for j in coverage:\n        print(j*2)\n</code></pre> <pre><code>200\n34\n300\n98\n789\n542\n400\n68\n600\n196\n1578\n1084\n</code></pre> <p>Here we move through the loop and for every value in the first list (<code>[1,2]</code>), Then for each pass of the first loop we move through values the second list (<code>[200, 34, 300, 98, 789, 542]</code>). Finally for each value <code>i</code> we then multiply by each value <code>j</code>. </p>"},{"location":"programming_languages/intro_to_python/06_loops-conditionals/#pass-statement","title":"Pass Statement","text":"<p>If you want a placeholder for your loop, meaning no operation is performed, use the <code>pass</code> statement:</p> <pre><code>for i in coverage:\n    pass\n</code></pre>"},{"location":"programming_languages/intro_to_python/06_loops-conditionals/#conditionals","title":"Conditionals","text":"<p>If we were interested in performing some operation on a value only if a condition is met, we can use an <code>if</code> statement:</p> <pre><code>for i in coverage:\n    if i &gt; 500:\n        print(i)\n    else:\n        pass\n</code></pre> <pre><code>789\n542\n</code></pre> <p>Here we use the comparison operators we mentioned in the Libraries &amp; Data Frames Topic Note to only print values in <code>coverage</code> if they are larger than <code>500</code>.</p>"},{"location":"programming_languages/intro_to_python/06_loops-conditionals/#multiple-conditionals","title":"Multiple Conditionals","text":"<p>To perform operations based on multiple conditions you can add in <code>elif</code> statements:</p> <pre><code>for i in coverage:\n    if i &gt; 500:\n        print(i)\n    elif i &lt; 500:\n        print('This value is less than 500')\n</code></pre> <pre><code>This value is less than 500\nThis value is less than 500\nThis value is less than 500\nThis value is less than 500\n789\n542\n</code></pre>"},{"location":"programming_languages/intro_to_python/07_functions-scope/","title":"Functions & Scope","text":""},{"location":"programming_languages/intro_to_python/07_functions-scope/#functions","title":"Functions","text":"<p>So far we have used functions in base python or python modules. But what if we want to create our own? Well here is the general formula to do so:</p> <pre><code># load the module we need\nimport numpy as np \n\n# define a function to return geometric mean\ndef geometric_mean(values):\n    return np.exp(np.mean(np.log(values)))\n\n# call our function\ngeometric_mean(coverage)                 \n</code></pre> <pre><code>209.88855396892262\n</code></pre> <p>Here we see that we use <code>def</code> to define the function and <code>return</code> to specify what value you'd like to return. We then call our function and use our <code>coverage</code> variable as our set of values. The geometric mean of the set of values in <code>coverage</code> are then returned.</p>"},{"location":"programming_languages/intro_to_python/07_functions-scope/#function-documentation","title":"Function Documentation","text":"<p>To clarify the purpose of your function you can add a multiline string to your function using three quotes <code>'''</code>:</p> <pre><code># define a function to return geometric mean\ndef geometric_mean(values):\n''' This function takes a list of\n    values and returns the geometric mean \n    of those values'''\n    return np.exp(np.mean(np.log(values)))\n</code></pre> <p>This multiline string is also accessible when we run our function through the <code>help</code> function:</p> <pre><code>help(geometric_mean)\n</code></pre> <pre><code>Help on function geometric_mean in module __main__:\n\ngeometric_mean(values)\n    This function takes a list of\n    values and returns the geometric mean \n    of those values\n</code></pre>"},{"location":"programming_languages/intro_to_python/07_functions-scope/#variable-scope","title":"Variable Scope","text":"<p>Variable naming can be difficult and sometimes variable names might need to be reused. Normally, when we use a variable name over again, we change the value of that variable. However, if we assign the same variable in and outside a function the values do not get overwritten:</p> <pre><code>x = 45\n\ndef print_x():\n    x = 30\n    return x\n</code></pre> <pre><code>x\n</code></pre> <pre><code>45\n</code></pre> <pre><code>print_x()\n</code></pre> <pre><code>30\n</code></pre>"},{"location":"programming_languages/intro_to_unix/01_intro-to-unix/","title":"Intro To Linux","text":""},{"location":"programming_languages/intro_to_unix/01_intro-to-unix/#intro-to-command-line","title":"Intro to Command Line","text":"<p>Content Developed By Adelaide Rhodes, PhD</p> <p>This short workshop provides some basic training on bash and shell scripting on the command line on the Linux-based Tufts HPC cluster.</p> <p>This course is not meant to be comprehensive, but provides some insights into how the command line works as well as some strategic resources for studying and understanding command line on the HPC cluster.</p> <p>Helpful Vocabulary</p> <ul> <li>Command line is a more general term to indicate that you are using text commands on a terminal (linux bash shell or similar). Command line differs from \"Graphical User Interface (GUI)\" because all commands are texts instead of drag-and-drop or interactive formats such as the Windows or Mac Operating Sytems provide.</li> <li>HPC stands for High Performance Computing, \"cluster\" refers to a shared computer resource to enable more powerful computation than regularly available on an individual machine.</li> <li>Linux can refer to any of the free open source version of \"Unix\" from AT&amp;T Bell labs who pioneered the language in 1965. There are a number of Linux operating systems installed on HPC clusters (Ubuntu, Debian, RedHat Enterprise License (RHEL), CentOs, Fedora, etc.) Each of these systems have slight differences that may impact the commands demoed here. Tufts University Research Cluster is currently using RHEL7.</li> <li>Bash is one type of languages used in a \"shell\", the text interface on the Linux system. This lesson introduces a few objectives to help users understand how to use bash commands on the Linux RHEL shell of our HPC. Other shell languages have slight differences that affect how commands are run (e.g. new MacOSX ship with \"zsh\" as the default shell language on their installed terminal programs).</li> </ul>"},{"location":"programming_languages/intro_to_unix/01_intro-to-unix/#learning-objectives","title":"Learning Objectives","text":"<ul> <li>What is the shell?</li> <li>How do you access it?</li> <li> <p>How do you use it and what is it good for?</p> </li> <li> <p>Running commands</p> </li> <li>File Directory Structure</li> <li>Manipulating files</li> <li>Simple Bash Scripts</li> </ul>"},{"location":"programming_languages/intro_to_unix/01_intro-to-unix/#what-is-the-shell","title":"What is the shell?","text":"<p>The shell is a program that presents a command line interface which allows you to control your computer using commands entered with a keyboard instead of controlling graphical user interfaces (GUIs) with a mouse/keyboard combination.</p> <p>There are many reasons to learn about the shell.  A few specific ones:</p> <ul> <li> <p>For most bioinformatics tools, you have to use the shell. There is no   graphical interface. If you want to work in metagenomics or genomics you're   going to need to use the shell.</p> </li> <li> <p>The shell gives you power. The command line gives you the power to   do your work more efficiently and more quickly. Shell allows users to automate repetitive tasks.</p> </li> <li> <p>To use remote computers or cloud computing, you need to use the shell.</p> </li> </ul>"},{"location":"programming_languages/intro_to_unix/01_intro-to-unix/#knowing-shell-increases-speed-and-efficiency-through-automation","title":"Knowing Shell Increases Speed and Efficiency Through Automation","text":"<p>The most important reason to learn the shell is to learn about automation.  Any time you find yourself doing roughly the same computational task more than few times, it may be worth automating it; the shell is often the best way to automate anything to do with files.</p> <p>In this lesson, we're going to go through how to access Unix/Linux and some of the basic shell commands. We will finish with a demonstration of how to run programs interactively as well by submitting a job to SLURM. Slurm is a scalable cluster management and job scheduling system for Linux clusters. Other job scheduling systems you may be familiar with from other universities are \"PBS\" and \"SGE_Batch\".</p>"},{"location":"programming_languages/intro_to_unix/01_intro-to-unix/#where-to-learn-shell-commands","title":"Where to learn shell commands","text":"<p>The challenge with bash for the command line is that it's not particularly simple - it's a power tool, with its own deep internal logic with lots of details.</p> <p>Practice is the best way to learn, but here are some helpful shell command resources:</p> <ul> <li>Fun With Unix Cheat Sheet</li> <li>Shell Cheatsheet - Software Carpentry</li> <li>Explain shell - a web site where you can see what the different components of a shell command are doing.</li> </ul>"},{"location":"programming_languages/intro_to_unix/01_intro-to-unix/#how-to-access-the-shell-at-tufts","title":"How to Access the Shell at Tufts","text":"<p>Log In through OnDemand</p> <p></p>"},{"location":"programming_languages/intro_to_unix/01_intro-to-unix/#mac","title":"Mac","text":"<p>On Mac the shell is available through Terminal Applications -&gt; Utilities -&gt; Terminal Go ahead and drag the Terminal application to your Dock for easy access. Note: newer versions of MacOSX ship with \"zsh\" as the default shell language in their terminal. It is possible to change the preference to \"bash\". However, if you are only using the terminal to log into the Tufts cluster, you don't necessarily need to do this, because you will be using \"bash\" once you are on the cluster. \"zsh\" would only impact scripts and commands run locally on your own machine.</p>"},{"location":"programming_languages/intro_to_unix/01_intro-to-unix/#windows","title":"Windows","text":"<p>For Windows, an easy one to install and use right away is  gitbash. Download and install gitbash Open up the program.</p>"},{"location":"programming_languages/intro_to_unix/01_intro-to-unix/#other-options","title":"Other options:","text":"<ul> <li>Microsoft Window Terminal</li> <li>Conemu</li> </ul>"},{"location":"programming_languages/intro_to_unix/01_intro-to-unix/#linux","title":"Linux","text":"<p>You probably already know how to find the shell prompt.</p>"},{"location":"programming_languages/intro_to_unix/02_start-w-shell/","title":"Starting with the Shell","text":"<p>We will spend most of our time learning about the basics of the shell by manipulating some experimental data that we download from the internet.</p> For Attendees Using Terminal Programs to Access the Cluster (instead of the Web Browser \"OnDemand <p>If you are using a terminal on your home machine to connect to the tufts cluster, you will first need to log in by sending a simple command. Ignore this if you are using the web browser login tool.</p> <p>Replace \"username01\" with your tufts username.</p> <pre><code>ssh username01@login.pax.tufts.edu\n</code></pre> <p>Your username will have been created when your account was set up. If you do not have a cluster account, you can still follow this tutorial from your laptop or personal computer, except that the file structure will be different from what is described.</p> <p>The login will ask you for your Tufts password.</p> Connection Issues? <p>If you are not on the Tufts network, you will need to set up the Tufts VPN (Virtual Private Network) before logging in:</p> <p>VPN Instructions</p> <p>Best Practices for Logging In</p> <p>If you are logged in to OnDemand, and on a machine called \"login\". If you are not on the login machine, type <code>exit</code> to get there.</p> <p>First, run the <code>tmux</code> commands (remember which login machine you are on, <code>login-prod-#</code></p> <p><pre><code>module load tmux\ntmux new -s OctoberWorkshop\n</code></pre> You will be able to recover this session if you are on the same login node and run <code>tmux a -t OctoberWorkshop</code></p> <p>Second, run the <code>srun</code> command to go to a working machine, remember we have a reservation for this workshop on October 19 so if you are reading this at a different time, just drop the <code>--reservation=bioworkshop</code> parameter from the command.</p> <pre><code>srun -p batch --time=1-2:10:00 -n 2 --mem=4g --reservation=bioworkshop --pty bash\n</code></pre> <p>The beginning of the line is called the 'command line prompt.'</p> <p><pre><code>[username01@login-prod-02 ~]$\n</code></pre> It tells you who you are and what machine you have been assigned.</p> <p>Once you run the <code>srun</code> command, the machine name should change</p> <pre><code>[username01@i2cmp003 ~]$\n</code></pre> <p>Question</p> <p>What machine are you on? Type your answer into the chat box.</p> <p>Tip</p> <p>The <code>$</code> at the end of the line is where you start typing your commands. The <code>$</code> (on some Mac terminals it is a <code>%</code>) is not part of the command. The outputs from commands will not have that piece of information or <code>$</code> at the beginning of the line.</p> <p>Tip</p> <p>The name of the computer you are on is important informatiom when troubleshooting the cluster.  <code>login</code> machines will reject large commands and output an error.  Make sure to switch machines with <code>srun</code> before running programs. </p>"},{"location":"programming_languages/intro_to_unix/02_start-w-shell/#using-basic-commands","title":"Using Basic Commands","text":"<p>Open up the shell through a terminal (OnDemand or on your laptop) and type the command::</p> <pre><code>whoami\n</code></pre> <p>and then hit ENTER </p> <p>(This is a good question for Mondays ....)</p> <p>When you are on the Tufts cluster, this will return your username according to the cluster. This username is attached to you wherever you are in the cluster and creates a home where your files can be kept, regardless of which machine you are on in the cluster. [If you are on your laptop or personal computer, the answer to this may be different before you log in.]</p>"},{"location":"programming_languages/intro_to_unix/02_start-w-shell/#running-commands","title":"Running Commands","text":"<p>Let's try some simple commands.</p> <p>Much like text shortcuts, shell commands often use abbreviations to get their point across.</p> <p>For example, the command pwd is short for \"print working directory.\" The word \"print\" here means it will output it into the visible screen.</p> <p>Now type the command</p> <pre><code>pwd\n</code></pre> <p>You should see something similar to this:</p> <pre><code>/cluster/home/username01/\n</code></pre> <p>Try this command</p> <pre><code>ls\n</code></pre> <p>It may be empty for the moment, or it may not if this is not your first time using the shell. We will be creating content in the next part of this lesson.</p> <p>Takeaways</p> <p><code>pwd</code> and <code>ls</code> are examples of commands - programs you run at the shell prompt that do stuff. </p> <ul> <li><code>pwd</code> stands for 'print working directory', while</li> <li><code>ls</code> stands for 'list files'. </li> </ul> <p>It is similar to the abbreviations used in texting, it takes less time to get the point across (lol, tbh, imho, afaik, ftw -- you're saying them outloud in your head, right now, correct?)</p>"},{"location":"programming_languages/intro_to_unix/03_bash-parameters/","title":"Bash Parameters","text":""},{"location":"programming_languages/intro_to_unix/03_bash-parameters/#making-files-and-directories","title":"Making files and directories","text":"<p>Many bash commands have special parameters, sometimes referred to as flags that open up a lot more possibilities.</p> <p>Let's start by going to your home directory (you choose the command)</p> <p>For new users, this may not return any content (besides <code>privatemodules</code> if you loaded <code>tmux</code> at the beginning.)</p> <p>Let's make a workshop directory and put a file into it.</p> <p>1.) Go to your home directory </p> <pre><code>cd\n</code></pre> <p><code>cd</code> not only changes directory, it allows you to go home by typing the command all by itself without a directory name.</p> <p>2.) Create a new directory for the workshop</p> <pre><code>mkdir Oct22Workshop\n</code></pre> <p>Note</p> <p><code>mkdir</code> is a specific command that allows you to make a directory. <code>rmdir</code> is a command that allows you to remove a directory (but only if it is empty)  When nameing files and directories, avoid spaces and special characters except underscores (\"_\") and hyphens (\"-\").</p> <p>Important</p> <p>Spelling and Capitalization are literal in unix. Be careful when making and using files to be consistent in your process.  This will make it easier to find files later.</p> <p>3.) Let's go into the directory using a very common command <code>cd</code> --&gt; <code>change directory</code></p> <pre><code>cd Oct22Workshop\n</code></pre> <p>4.) Make a new file that is empty</p> <pre><code>touch emptyfile.txt\n</code></pre> <p><code>touch</code> is a bash command that creates an empty file.</p> Why would you want an empty file? <p>Some programs require some pre-existing file names to be created.</p> <p>5.) Make a new file that contains \"Hello World\"</p> <pre><code>echo \"Hello World\" &gt; helloworld.txt\n</code></pre> <p><code>echo</code> is a command that prints the content to the terminal window (sometimes refered to as <code>print to screen</code></p> <p>The <code>&gt;</code> in this command tells the command to place the output into the place it is pointing. </p> <p>In this case, it creates the file <code>helloworld.txt</code> and puts the phrase <code>Hello World</code> into the file. </p> <p>6.) Print out the contents of the file to the terminal</p> <pre><code>cat helloworld.txt\n</code></pre> <p>You should see the output</p> <pre><code>Hello World\n</code></pre> <p>7.) Return to your home directory and run <code>ls</code></p> <pre><code>cd\n</code></pre> <pre><code>ls\n</code></pre> <p>Tip</p> <p>If you want to speed up the execution of commands, you can copy and paste multiple commands at the same time.</p> <pre><code>cd\nls\n</code></pre> <p>Question</p> <p>Please put a green checkmark in your box if you see the new directory when you type <code>ls</code> from your home directory).</p> <p>Be Careful with Redirect</p> <p>Be careful with redirect.</p> <p>When using <code>&gt;</code> to redirect content into a file, if the filename already exists, it will overwrite the file. This means that the original file is gone, and there is no undo in shell.</p> <p>If you want to add to a file (for example if you are running the same command on several files and extracting a piece of information that you want to put together at the end) you can use another form of redirect <code>&gt;&gt;</code>. Using the double redirect will add to the file instead of overwriting it.</p> <p>Which one is used depends on your process. If you are only running a command once, or have an intermediate file in a process that does not need to be retained at the end, then <code>&gt;</code> is okay to use.</p>"},{"location":"programming_languages/intro_to_unix/03_bash-parameters/#setting-parameters-for-bash-commands","title":"Setting Parameters for Bash Commands","text":"<p>As you start using bash more and more, you will find a mix of files and directories/folders.  If we want to know which is which, we can add a <code>parameter</code> (sometimes referred to as a <code>flag</code>)</p> <p>This is an example of adding a <code>parameter</code> without an <code>argument</code>.</p> <pre><code>ls -F\n</code></pre>"},{"location":"programming_languages/intro_to_unix/03_bash-parameters/#adding-arguments-to-bash-commands","title":"Adding Arguments to Bash Commands","text":"<p>An <code>argument</code> is a file name or other data that is provided to a command.</p> <pre><code>ls -F Oct22Workshop\n</code></pre> <p>It is possible to list the files and see their types inside a specific directory by adding the <code>argument</code> of the directory name to the <code>ls</code> command.</p> What do you see when you run the two commands above? <p>Anything with a \"/\" after it is a directory. Anything with a <code>*</code> after it are programs. (we will make a program later) If there's nothing there it's an otherwise unremarkable file (e.g. a data file or picture).</p> <p>Depending on which terminal you are using, some of the file types may have different colors. </p> <p>In our ondemand shell:</p> <p>Files are white Directories are blue Programs (also called <code>executables</code>) are green Compressed files are red (e.g. files that end in .zip or .gzip or .tar)</p>"},{"location":"programming_languages/intro_to_unix/03_bash-parameters/#other-useful-parameters-for-ls","title":"Other Useful Parameters for <code>ls</code>","text":"<p>Show hidden files</p> <pre><code>ls -a\n</code></pre> <p>You should see a file called <code>.bashrc</code> here. This may be a file we need for troubleshooting your work or where you can make shortcuts or add paths to your login.</p> <p>Show the <code>long form</code> of the list command</p> <pre><code>ls -l\n</code></pre> <p>To see whether items in a directory are files or directories. <code>ls -l</code> gives a lot more information too, such as the size of the file.</p> <p>It also shows the permissions of who can read, write or execute a file.</p> <pre><code>drwxrwx--- 2 username05 username05     4096 Jul 18 09:57 JulyWorkshop\n</code></pre> <p>The first 10 letters in this line indicates the permission settings.</p> <p></p>"},{"location":"programming_languages/intro_to_unix/03_bash-parameters/#getting-help-on-the-command-line","title":"Getting Help on the Command Line","text":"<p>There are an overwhelming number of possibilities with some of these shell commands, so knowing how to find help on demand is important.</p> <p>For example, <code>ls</code> has a lot of flags that can be used.</p> <pre><code>ls --help\n</code></pre> <p>This outputs a list of all the ways that <code>ls</code> can be altered to find information about your files.</p> <p>Parameters can be added together in some cases.</p> <pre><code>ls -ltr\n</code></pre> <p>This can replace <code>ls -l -t -r</code> <code>l</code> is for long form of the list (outputs the permission settings -- something we need to troubleshoot occasionally) <code>t</code> is to order the files chronologically <code>r</code> means to reverse the order of the files to put the newest file at the bottom</p> <p>This command strings together three flags.</p> <p><code>ls -l</code> is list with details <code>ls -t</code> is sort the list by creation time <code>ls -r</code> is sort the list in reverse</p> <p>For very full directories, this is helpful because it outputs the most recent set of files as the last in the list.</p> <p>Another way to get help is to use the <code>man</code> command. Not every unix installation has this installed, but the Tufts cluster does.</p> <p><code>man</code> is short for \"manual\"</p> <p>Navigating a <code>man</code> page</p> <p>Use the <code>spacebar</code> to scroll through the document. Use <code>q</code> to leave the manual and go back to the command line prompt.</p> <pre><code>man ls\n</code></pre> <p>This opens up the manual on the <code>ls</code> command. It spells out the meaning of all the parameters in detail.</p> <p>Most common bash commands have a <code>man</code> page that explains it (I wish they had this for emojis....).</p> <p>Many programs have a help function built in, try adding <code>--help</code> or <code>-h</code> to see if some helpful information pops up. Sometimes just running the command without any arguments or parameters leads to some usage information or describes the correct command to get help.</p> <p>For example, if I want to understand the command <code>tr</code> - which is used to change a word or character to a new value.</p> <p>Most programs recognize when you ask for an incorrect parameter, and will tell you how to get more information, as in this example.  To get help, type the command with the correct parameter.</p> <p>Tip</p> <p>For some programs, the <code>help</code> function may be <code>-h</code>, <code>--help</code> </p> <pre><code>tr -h\n</code></pre> <p>The shell outputs:</p> <pre><code>tr: invalid option -- 'h'\nTry 'tr --help' for more information\n</code></pre> <pre><code>tr --help\n</code></pre> <p>In this case, a <code>man</code> page does exist, so you can get even more direction by typing:</p> <pre><code>man tr\n</code></pre>"},{"location":"programming_languages/intro_to_unix/04_shell-navigation/","title":"Shell Navigation","text":""},{"location":"programming_languages/intro_to_unix/04_shell-navigation/#navigating-in-the-shell","title":"Navigating in the Shell","text":"<p>Best Practices for Naming Files and Directories</p> <p>A directory is like a desk drawer. We create them to store files that relate to each other mostly.</p> <p>When creating directories and filenames it is helpful to put some information about the project and the date of activity.</p> <p></p>"},{"location":"programming_languages/intro_to_unix/04_shell-navigation/#absolute-and-relative-paths","title":"Absolute and Relative Paths","text":"<p>Let's go into our directory and look around using relative and absolute paths.</p> <p>Go home</p> <p><pre><code>cd\n</code></pre> Go into our workshop directory</p> <pre><code>cd Oct22Workshop\n</code></pre> <p>and then</p> <pre><code>pwd\n</code></pre> <p>You should now see something like this:</p> <pre><code>/cluster/home/username01/Oct22Workshop\n</code></pre> <p>This is an example of an Absolute Path.</p> <p>It gives an address for where you are located on the cluster, much like a postal address that defines where you are in several layers (e.g. /country/state/city/street/specific_house.</p> <p></p> <p>You can have many files and folders that share the same name in your directories (e.g. scripts, data). An absolute path ensures that you go to the correct file, as it will be unique.</p> <p>If you want to go back to the directory that is in the level above our current file (in this case \"home\"), another common shortcut used in bash is <code>..</code></p> <pre><code>cd ..\n</code></pre> <p><code>..</code> is a reference to a RELATIVE PATH</p> <pre><code>pwd\n</code></pre> <p>You should be back in your home directory.</p> <pre><code>/cluster/home/username01/\n</code></pre> <p>If you want to go back to the directory that you just left, type this command.</p> <p><pre><code>cd -\n</code></pre> Then find your location.</p> <pre><code>pwd\n</code></pre> <p>You should be back in the directory you came from.</p> <pre><code>/cluster/home/username01/Oct22Workshop\n</code></pre> <p>A *RELATIVE PATH means that the command only works from the relative location that you are in.</p> <p><code>cd ..</code> and <code>cd -</code> are examples of relative path commands.</p> <p>Note</p> <p>Your home directory is not all the way back at the root ('/'), it is set within the cluster as <code>/cluster/home/username01/</code>.</p> <p>You can make sure that you are in the right directory by using the command <code>cd</code> with the absolute path.</p> <pre><code>cd /cluster/home/username01/Oct22Workshop\n</code></pre> <p>This command will make sense inside a script, because the exact path is specified.</p>"},{"location":"programming_languages/intro_to_unix/04_shell-navigation/#using-bash-commands-with-absolute-paths","title":"Using Bash Commands with Absolute Paths","text":"<p>Many commands in bash can be used with the ABSOLUTE PATH.</p> <pre><code>ls /cluster/home/username01/Oct22Workshop\n</code></pre> <pre><code>helloworld.txt\nemptyfile.txt\n</code></pre> <p>Absolute Paths are better for SLURM</p> <p>This can get confusing if you are moving around a lot in your directories or sending commands to SLURM, so the alternative method to navigating around the cluster is using an ABSOLUTE PATH.</p>"},{"location":"programming_languages/intro_to_unix/05_create-manipulate-files/","title":"Creating & Manipulating Files","text":""},{"location":"programming_languages/intro_to_unix/05_create-manipulate-files/#reading-file-contents","title":"Reading File Contents","text":"<p>There are a few different ways to see the contents of a file.</p> <p>We already used this first example.</p> <pre><code>cd ~/Oct22Workshops\n</code></pre> <p>Let's look inside the file. We have several methods of viewing the content of files that we have created.</p> <p>A helpful command is <code>cat</code>.</p> <pre><code>cat helloworld.txt\n</code></pre> <p>\"cat\" will open the entire file, so this is not the best command for long files.</p> <p>In that case \"head\" is a good option. Head pulls the top ten lines of the file and prints them to the screen.</p> <pre><code>head helloworld.txt\n</code></pre> <p>It does not look any different from cat in this case because there is only one line in the file.</p> <p>A third way to check file contents is by using a program called \"less\" (or \"more\").</p> <p>\"less\" will open the file interactively, then you can scroll through it and when you are done, push \"q\" on your keyboard to close the file.</p> <pre><code>less helloworld.txt\n</code></pre> <p>Press q to close the file opened by <code>less</code></p> <p>There are many versions of these tools on command line, but \"cat\", \"head\" and \"less\" are very common.</p>"},{"location":"programming_languages/intro_to_unix/05_create-manipulate-files/#copying-files","title":"Copying Files","text":"<p>Sometimes we have a file that we want to reuse.</p> <p>When copying within the same directory, make sure to change the name of the file, or the original will be overwritten.</p> <p>When copying to a new directory, the name can stay the same.</p> <p>This command copies the file within the same directory with a new name. Both files are kept.</p> <p><pre><code>cp helloworld.txt helloworld1.txt\n</code></pre> Check this with <code>ls</code></p> <p>These commands make a new directory, and then copies the file into the new directory with the same name.</p> <pre><code>mkdir helloworld\ncp helloworld.txt helloworld\n</code></pre> <p>Check this with <code>ls helloworld</code> (lists the contents of the directory).</p>"},{"location":"programming_languages/intro_to_unix/05_create-manipulate-files/#moving-files","title":"Moving Files","text":"<p><code>mv</code> is an option for renaming files, but also has the potential to overwrite existing files.</p> <p>For example, this command changes the name of the file and removes the original file. If <code>helloworld2.txt</code> already existed, it would be replaced.</p> <pre><code>mv helloworld1.txt helloworld2.txt\n</code></pre> <p>Check this with <code>ls</code></p>"},{"location":"programming_languages/intro_to_unix/05_create-manipulate-files/#removing-files","title":"Removing Files","text":"<p><code>rm</code> and <code>rmdir</code> are permanent in shell, so make sure you are ready to delete files.</p> <pre><code>rm helloworld/helloworld.txt\n</code></pre> <p>Once the directory is empty, we can remove the directory.</p> <pre><code>rmdir helloworld\n</code></pre> <p>It will throw an error if the directory is not empty.</p> <p>If you are positive that you want to remove a directory and all the files within it, then add two flags, <code>-r</code> for recursive and <code>-f</code> for force.</p> <p>Both commands above could have been replaced with one remove command: <code>rm -rf helloworld</code></p> <p>Tip</p> <p>Until you are confident with file structure and bash commands, it is a good idea to copy instead of move and to    * <code>cp -u</code> will copy files only if they do not already exist.   * <code>cp -r</code> is a good command for copying directories, it means <code>copy recursively</code> which will copy the entire directory.   * <code>cp -rf</code> BE CAREFUL with this, it copies the entire directory AND forces the overwrite of any files that already exist.   * Adding the interactive flag <code>-i</code> on the commands <code>rm</code> and <code>mv</code> to set up a question that you answer <code>y</code> or <code>n</code> to before removing.</p> <pre><code>rm -i helloworld/helloworld.txt\n</code></pre> <p>Generates this question <pre><code>rm: remove regular file \u2018helloworld/helloworld.txt\u2019?\n</code></pre></p> <p><code>mv -i</code> only generates a question if you are in danger of overwriting an existing file.</p> <p>For example:</p> <p>1.) Make a new file from the original file we created</p> <p><pre><code>cp -u helloworld.txt helloworld1.txt\n</code></pre> <code>-u</code> for the copy command will not copy the file if it already exists.</p> <p>2.) Try to rename the file with <code>mv</code>, with the <code>i</code> parameter set to prevent overwriting an existing file.</p> <p><pre><code>mv -i helloworld.txt helloworld1.txt\n</code></pre> Generates the question:</p> <pre><code>mv: overwrite \u2018helloworld1.txt\u2019?\n</code></pre> <p>A great website to look at to understand the nuances of shell commands is:</p> <p>ComputerHope</p>"},{"location":"programming_languages/intro_to_unix/06_going-home/","title":"Going Home","text":""},{"location":"programming_languages/intro_to_unix/06_going-home/#going-home","title":"Going Home","text":"<p>Sometimes we get lost, so it is useful to know a few ways to get back to where you started.</p> <pre><code>cd\n</code></pre> <p>This command returns you to your home directory. Check by typing this command.</p> <pre><code>pwd\n</code></pre> <p>Other options for going back to your home directory:</p> <pre><code>cd ~\n</code></pre> <pre><code>cd $HOME\n</code></pre> <p>When lost in the file structure, going home is a good place to start.</p>"},{"location":"programming_languages/intro_to_unix/07_running-interactive/","title":"Running an Interactive Session","text":""},{"location":"programming_languages/intro_to_unix/07_running-interactive/#running-programs-interactively","title":"Running Programs Interactively","text":""},{"location":"programming_languages/intro_to_unix/07_running-interactive/#hpc-etiquette","title":"HPC Etiquette","text":"<p>Try not to use the login computers for programs or large file management jobs. Looking things up and small commands such as <code>cat</code> or <code>head</code> are fine, but running programs may block others from logging in to the cluster.</p>"},{"location":"programming_languages/intro_to_unix/07_running-interactive/#switch-to-an-interactive-session","title":"Switch to an Interactive Session","text":"<p>Do this first before running programs or testing your code.</p> <pre><code>srun -p batch --reservation=bioworkshop -n 2 --mem=8g -t 1-0 --pty bash\n</code></pre> <p>This command only works during the October-November 2022 workshops. To use this command after the workshop is over or if you are working on your own, just remove the reservation flag.</p> <ul> <li> <p><code>-p</code> which partition to use, outside of class it is okay to use <code>interactive</code> or <code>batch</code>, your group may have it's own partition. You can read more about what is available by going to the OnDemand dropdown menu for \"Misc\" and look at \"Scheduler Info\" to find all the partition names.</p> </li> <li> <p><code>--reservation</code> only applies during a specific class or workshop</p> </li> <li><code>-n</code> is the number of cpus to request, 2 or 4 is sufficient for most tests.</li> <li><code>--mem</code> specifies the memory requested, 8g is usually sufficient for small jobs, consult the documentation for a program to find out if a minimum memory requirement is needed.</li> <li><code>-t</code> indicates the time <code>1-0</code> means one day, so for tomorrow's session you will need to rerun this command.</li> <li><code>--pty bash</code> just indicates that the shell opens in <code>bash</code>, meaning that all the commands that we learned today will work.</li> </ul> <p>Question</p> <p>What compute node are you on? Type it into the chat box.</p> <p></p>"},{"location":"programming_languages/intro_to_unix/07_running-interactive/#finding-your-files-interactively","title":"Finding your files interactively","text":"<p>When you request a computer using an <code>srun</code> command, the beginning of your command line should change to indicate that you are no longer on a <code>login</code> node and instead are on a <code>compute</code> node. It will tell you which node you are on.</p> <p>Your files will <code>mount</code> to the new node. This means that you can be on any computer in the Tufts HPC and it will recognize your home directory structure.</p> <p>Let's go back into the Oct22Workshop directory, but this time use your ABSOLUTE path by changing <code>username01</code> to your username. If you forget your username, try <code>whoami</code>.</p> <pre><code>cd /cluster/home/username01/Oct22Workshop\n</code></pre>"},{"location":"programming_languages/intro_to_unix/07_running-interactive/#find-and-tree","title":"Find and Tree","text":"<p>File structures can get complicated quickly.</p> <p>Two tools to understand where your files are that can help are <code>find</code> and <code>tree</code>.</p> <p>From your home directory, you can find your file named <code>helloworld.txt</code> by typing the following:</p> <pre><code>cd\nfind . -name helloworld.txt\n</code></pre> <p><code>find</code> is a bash command <code>.</code> means look from this location and into any subdirectories to this location <code>name</code> is the file that you are looking for</p> <p>From the home directory, the answer is given using the RELATIVE path:</p> <pre><code>./Oct22Workshop/helloworld.txt\n</code></pre> <p><code>.</code> in this case is another RELATIVE path direction that indicates \"from this directoy that I am in currently\". Note that the answer is given in the RELATIVE path format, starting with <code>.</code> = here.</p> <p>It is also possible to provide an ABSOLUTE path to this command.</p> <p><pre><code>find /cluster/home/username01 -name helloworld.txt\n</code></pre> This command will work from anywhere in the cluster. Note that the answer is given in the ABSOLUTE path format.</p> <pre><code>/cluster/home/username01/Oct22Workshop/helloworld.txt\n</code></pre> <p>Tip</p> <ul> <li><code>find</code> using the parameter <code>iname</code> allows the search to be insensitive to case <code>find . -iname \"helloworld.txt\"</code> finds <code>Helloworld.txt</code> AND <code>helloworld.txt</code></li> <li>wildcards allow for partial searches of many filenames that may match.</li> <li>The easiest wildcard is <code>*</code> which means any number of characters can match, such as <code>find . -iname \"hello*.txt\"</code> finds any file that begins with <code>hello</code> and has any number of characters before <code>.txt</code> finds <code>helloworld1.txt</code> AND <code>helloworld2.txt</code>. </li> <li><code>*</code> can be used anywhere in the pattern: <code>hello*.txt</code>, <code>hello*.txt</code>, <code>*.txt</code></li> </ul> <p>Another helpful bash command for finding files is <code>tree</code>.</p> <pre><code>tree\n</code></pre> <p>This outputs your directory structure with lines that indicate the tree-like branches of your file structure.</p> <p>This could be very messy if you already have a lot of files in a directory, so limit the level by adding a flag.</p> <pre><code>tree -L 2\n</code></pre> <p>This just shows the top two levels of the file structure.</p> <p>Tip</p> <p>There are some keyboard shortcuts that can help when writing complex commands and running programs interactively.</p> <ul> <li>Control-C will terminate a running process</li> <li>Control-A will put your cursor at the beginning of the line</li> <li>Control-E will put your cursor at the end of the line</li> <li>Up and down arrows will scroll through recent commands - If you make a mistake, just hit up to reveal the command and work on the part that was a mistake instead of retyping the whole thing.</li> </ul> <p>Note</p> <p>When trouble shooting a command using tickets, screen shots of error messages are a good option. (On Macs, Command-Shift-4)</p>"},{"location":"programming_languages/intro_to_unix/07_running-interactive/#what-is-blast","title":"What is BLAST?","text":"<p>BLAST is the Basic Local Alignment Search Tool. It uses an index to rapdily search large sequence databases; it starts by finding small matches between the two sequences and extending those matches.</p> <p></p> <p>For more information on how BLAST works and the different BLAST functionality, check out the summary on Wikipedia or the NCBI's list of BLAST resources.</p> <p>BLAST can be helpful for identifying the source of a sequence, or finding a similar sequence in another organism. In this lesson, we will use BLAST to find zebrafish proteins that are similar to a small set of mouse proteins.</p>"},{"location":"programming_languages/intro_to_unix/07_running-interactive/#why-use-the-command-line","title":"Why use the command line?","text":"<p>BLAST has a very nice graphical interface for searching sequences in NCBI's database. However, running BLAST through the commmand line has many benefits:   * It's much easier to run many BLAST queries using the command line than the GUI   * Running BLAST with the command line is reproducible and can be documented in a script   * The results can be saved in a machine-readable format that can be analyzed later on   * You can create your own databases to search rather than using NCBI's pre-built databases   * It allows the queries to be automated   * It allows you to use a remote computer to run the BLAST queries</p> <p>We are next going to write a script that we will send to SLURM which will demonstrate these advantages.</p>"},{"location":"programming_languages/intro_to_unix/08_blast-example/","title":"Example with BLAST","text":""},{"location":"programming_languages/intro_to_unix/08_blast-example/#return-to-the-workshop-directory","title":"Return to the workshop directory","text":"<pre><code>cd ~/Oct22Workshop\n</code></pre>"},{"location":"programming_languages/intro_to_unix/08_blast-example/#loading-modules","title":"Loading Modules","text":"<p>Many common programs are pre-loaded into the Tufts HPC using a system called \"modules\".</p> <p>To see what versions of blast are available as a module, try running this command. </p> <p>Tip</p> <p>You can use the first part of the program name to check if there is a module.</p> <pre><code>module av blast\n</code></pre> <p>As of October 2022, these are the modules you might see displayed.</p> <p></p> <p>Choose the latest blast-plus version of the module and load it. </p> <pre><code>module load blast-plus/2.11.0\n</code></pre> <p>When there is only one version of a module, the full version does not need to be provided, but it is always best to inclue the version as we are loading and updating versions of programs all of the time.</p> <p>Confirm that the module is loaded.</p> <pre><code>module list\n</code></pre> <p>tmux and blast should be listed.</p> <p>If other programs are loaded with the module, they may also show up with this command.</p>"},{"location":"programming_languages/intro_to_unix/08_blast-example/#bringing-in-files-from-the-internet","title":"Bringing in Files from the Internet","text":"<p>We need some data!  Let's grab the mouse and zebrafish RefSeq protein data sets from NCBI, and put them in our home directory. (this example is adapted from a lesson from Titus Brown's summer institute. These lessons contain a lot of command line examples.</p> <p>Note</p> <p><code>curl</code> and <code>wget</code> are the two most common tools used to bring in files that are available from a url. We are going to use <code>curl</code> because that command works well for files coming from an <code>ftp://</code> url.</p> Copying files over from NCBI <p>For genomics projects, the files are often stored in pubic repositories and we must go and get those files before proceeding. These files originally came from the   NCBI FTP site, a copy has been placed in our github directory for future reference.</p> <p>Now, we'll use <code>curl</code> to download the files from a Web site onto our computer. You will need to be connected to the internet for these commands to work.</p> <ul> <li><code>-o</code> indicates this is the name we are assigning to our files in our own directory</li> <li><code>-L</code> provides the full path for the download</li> </ul> <p>It is possible to copy and paste both conmands to your terminal, they will run in sequence if there is not an error.</p> <pre><code>curl -o mouse.1.protein.faa.gz -L https://tuftsdatalab.github.io/Research_Technology_Bioinformatics/workshops/hpcForLifeSciences_July2022/IntroToLinux/mouse.1.protein.faa.gz\n\ncurl -o zebrafish.1.protein.faa.gz -L https://tuftsdatalab.github.io/Research_Technology_Bioinformatics/workshops/hpcForLifeSciences_July2022/IntroToLinux/zebrafish.1.protein.faa.gz\n</code></pre> <p>Another method for pulling files from the internet is <code>wget</code>, which will be demoed tomorrow. <code>curl</code> can pull more file types than <code>wget</code>, but in this simple case, either can be used.</p> <p>If you look at the files in the current directory:</p> <pre><code>ls -l\n</code></pre> <p>You should now see these 3 files with details on who has permissions and when the files were created (notice that the dates are not today).</p> <pre><code>total 29908\n-rw-rw-r-- 1 username01 username01 12553742 Jun 29 08:41 mouse.1.protein.faa.gz\n-rw-rw-r-- 1 username01 username01 13963093 Jun 29 08:42 zebrafish.1.protein.faa.gz\n</code></pre> <p>The three files you just downloaded are the last three on the list - the <code>.faa.gz</code> files.</p> <p>All three of the files are FASTA protein files (that's what the .faa suggests) that are compressed with <code>gzip</code> (that's what the .gz means). Compressed files may have a different color when you use the <code>ls</code> command.</p> <p>Uncompress the files.</p> <pre><code>gunzip *.faa.gz\n</code></pre> <p>Because both files follow a very similar pattern, and we want to decompress all our .gz files, we can use the <code>*</code> wildcard (filenames that have a pattern that matches and number of missing letters before the part of the file name that is the same</p> <p>Regular Expressions</p> <p><code>*</code> and other wildcards are useful to save on typing scripts, because many actions can be combined in one request.</p> <p>Regular Expressions are a set of special characters combined with unix commands.</p> <p>Here is a link that explains the basic syntax){:target=\"_blank\" rel=\"noopener\"}. </p>"},{"location":"programming_languages/intro_to_unix/08_blast-example/#checking-the-contents-of-a-file","title":"Checking the contents of a File","text":"<p>We've already used <code>cat</code> and <code>less</code> to look at the content of our helloworld.txt files. Some files are very large and we may only want to check the first few lines to reassure ourselves that the download worked correctly.</p> <p>Let's look at the first few sequences in the file:</p> <pre><code>head mouse.1.protein.faa \n</code></pre> <p>!!! note \"FASTA format</p> <pre><code>These are protein sequences in FASTA format.  FASTA format is something many of you have probably seen in one form or another -- it's pretty ubiquitous.  It's a text file, containing records; each record starts with a line beginning with a '&gt;', and then contains one or more lines of sequence text.\n</code></pre> <p>Let's take those first two sequences and save them to a file.  We'll do this using output redirection with '&gt;', which says \"take all the output and put it into this file here.\"</p> <pre><code>head -n 11 mouse.1.protein.faa &gt; mm-first.faa\n</code></pre> <p><code>-n</code> flag for <code>head</code> specifies a number of lines to pull.</p> <p>The first 11 lines contain two protein sequences. Let's extract those for blasting to test that our process is working.</p> <pre><code>cat mm-first.faa\n</code></pre> <p>Should produce:</p> <pre><code>&gt;YP_220550.1 NADH dehydrogenase subunit 1 (mitochondrion) [Mus musculus domesticus]\nMFFINILTLLVPILIAMAFLTLVERKILGYMQLRKGPNIVGPYGILQPFADAMKLFMKEPMRPLTTSMSLFIIAPTLSLT\nLALSLWVPLPMPHPLINLNLGILFILATSSLSVYSILWSGWASNSKYSLFGALRAVAQTISYEVTMAIILLSVLLMNGSY\nSLQTLITTQEHMWLLLPAWPMAMMWFISTLAETNRAPFDLTEGESELVSGFNVEYAAGPFALFFMAEYTNIILMNALTTI\nIFLGPLYYINLPELYSTNFMMEALLLSSTFLWIRASYPRFRYDQLMHLLWKNFLPLTLALCMWHISLPIFTAGVPPYM\n&gt;YP_220551.1 NADH dehydrogenase subunit 2 (mitochondrion) [Mus musculus domesticus]\nMNPITLAIIYFTIFLGPVITMSSTNLMLMWVGLEFSLLAIIPMLINKKNPRSTEAATKYFVTQATASMIILLAIVLNYKQ\nLGTWMFQQQTNGLILNMTLMALSMKLGLAPFHFWLPEVTQGIPLHMGLILLTWQKIAPLSILIQIYPLLNSTIILMLAIT\nSIFMGAWGGLNQTQMRKIMAYSSIAHMGWMLAILPYNPSLTLLNLMIYIILTAPMFMALMLNNSMTINSISLLWNKTPAM\nLTMISLMLLSLGGLPPLTGFLPKWIIITELMKNNCLIMATLMAMMALLNLFFYTRLIYSTSLTMFPTNNNSKMMTHQTKT\nKPNLMFSTLAIMSTMTLPLAPQLIT\n</code></pre> <p>Now let's BLAST these two sequences against the entire zebrafish protein data set. First, we need to tell BLAST that the zebrafish sequences are (a) a database, and (b) a protein database.  That's done by calling <code>makeblastdb</code></p> <pre><code>makeblastdb -in zebrafish.1.protein.faa -dbtype prot\n</code></pre> <p><code>makeblastdb</code> is a program that was loaded using the <code>module</code> command. If you unload the module, this command may not work.</p> <p>Next, we call BLAST to do the search:</p> <pre><code>blastp -query mm-first.faa -db zebrafish.1.protein.faa\n</code></pre> <p>This should run pretty quickly, but you're going to get a lot of output!! To save it to a file instead of watching it go past on the screen, ask BLAST to save the output to a file that we'll name <code>mm-first.x.zebrafish.txt</code>:</p> <pre><code>blastp -query mm-first.faa -db zebrafish.1.protein.faa -out mm-first.x.zebrafish.txt\n</code></pre> <p>and then you can 'page' through this file at your leisure by typing:</p> <pre><code>less mm-first.x.zebrafish.txt\n</code></pre> <p>(Type spacebar to move down, and 'q' to get out of paging mode.)</p> <p>What are your questions?</p> <p>Note</p> <p>This command was an example of <code>interactive</code> shell scripting because we are typing in the commands manually and waiting for the results. If we walk away from our machine and the session times out, then the program may be interrupted. <code>tmux</code> allows us to keep running the program even if we take a break.</p> <p>The next session demonstrates how to combine all of these commands into a script that runs on SLURM. </p> <p>SLURM differs from <code>interactive</code> computing because you activate the script instead of manually writing the commands one at a time.</p>"},{"location":"programming_languages/intro_to_unix/09_blast-batch/","title":"BLAST Batch Script","text":""},{"location":"programming_languages/intro_to_unix/09_blast-batch/#writing-a-bash-script-and-running-it-as-batch","title":"Writing a BASH Script and Running it as \"Batch\"","text":"<p>In this example, we'll repeat the blast command above but refine it by outputting a table which summarizes each blast hit on one line. </p>"},{"location":"programming_languages/intro_to_unix/09_blast-batch/#return-to-the-workshop-directory","title":"Return to the Workshop Directory","text":"<p>cd ~/Oct22Workshop</p> <p>First, let's add more sequences to our query file. This will extract the first 186 sequences.  </p> <pre><code>head -n 999 mouse.1.protein.faa &gt; mm-second.faa\n</code></pre> <p>See this link for a description of the possible BLAST output formats.</p> <p>In order to do this, we need to open a text editor.</p>"},{"location":"programming_languages/intro_to_unix/09_blast-batch/#opening-a-text-editor","title":"Opening a Text Editor","text":"<p>The easiest text editor to use on command line for beginners is <code>nano</code>, but there are many other types of command line text editors (<code>vi</code>,<code>emacs</code>,<code>vim</code>, etc.)</p> <p>Nano is nice because it puts the instructions at the bottom of the editor in case you forget.</p> <p>Open nano</p> <pre><code>nano\n</code></pre> <p>Control-X to exit, say no and no. Nothing is saved, because we did not type into the file.</p> <p>Let's reopen and copy and paste our script into the file.</p> <p>Sometimes it is good to give a file name, so let's nano with a filename for our script.</p> <pre><code>nano blast_sbatch.sh\n</code></pre> <p>Before closing, let's put some text into the file. </p> <p>Make sure to change the email address to your own email.</p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=blast\n#SBATCH --nodes=1\n#SBATCH -n 2\n#SBATCH --partition=batch\n#SBATCH --reservation=bioworkshop\n#SBATCH --mem=8Gb\n#SBATCH --time=0-24:00:00\n#SBATCH --output=%j.out\n#SBATCH --error=%j.err\n#SBATCH --mail-user=youremail@tufts.edu\n\nmodule load blast-plus/2.11.0\nblastp -query mm-second.faa -db zebrafish.1.protein.faa -out mm-second.x.zebrafish.tsv -outfmt 6\n</code></pre> <p>Control -X to close and save and use the same file name (blast_sbatch.sh)</p> <p>Because it is going to one or several virtual locations in the cluster, we need to reload the module as part of the script before running the script. This will make the command recognizable to the machine where the job is running.</p> <pre><code>cat blast_sbatch.sh\n</code></pre> <p>Does it have all the elements?</p> <p>If it does, a simple way to run it is by telling shell that it is a program to run on SLURM.</p> <pre><code>sbatch blast_sbatch.sh\n</code></pre> <p>Check that the job is running</p> <pre><code>squeue -u $USER\n</code></pre> <p>Let's go ahead and run it from the workshop directory where you copied your data to.</p> <p>Because we did not add any ABSOLUTE paths, then the sbatch command will look for the files where the program is running.</p> <p>The results will also show up in that directory.</p> <p>You can look at the output file with <code>less -S</code>, the flag allows scrolling from left to right instead of wrapping text or cutting it off:</p> <pre><code>less -S mm-second.x.zebrafish.tsv\n</code></pre> <p>(and again, type 'q' to get out of paging mode)</p> <p>The command line may move stuff around slightly, but it is a tab delimited file that can be downloaded to your computer and loaded into your spreadsheet program of choice.</p> <p><code>blastp</code> is a versatile tool for finding similar sequences, to see all the options, type <code>blastp -help</code></p> <p>Tip</p> <p>If writing the script on your laptop before copying and pasting, make sure to use a compatible text editor.</p> <p>Even though you can't see it, popular word processors will add hidden symbols and change punctuation to your code.</p> <p>There are several free tools available to avoid these errors.</p> <ul> <li>Notepad+ is free to download and use.</li> <li>BBEdit has a free version.</li> </ul> <p>Other options are Sublime and PyCharm, which have some features to help edit files.</p>"},{"location":"programming_languages/intro_to_unix/09_blast-batch/#resources-for-further-training-in-command-line","title":"Resources for Further Training in Command Line","text":"<ul> <li>Udemy (free to the Tufts community)</li> <li>Coursera</li> <li>LinkedIn Learning</li> </ul> <p>What are your favorites?</p>"},{"location":"programming_languages/intro_to_unix/unix/","title":"Introduction To Unix","text":""}]}